%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai19}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{color,soul}
\usepackage{listings}
\usepackage{caption,subcaption}
\usepackage{natbib}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{mathrsfs}  
\usepackage{amsfonts}  



\newtheorem{mydef}{Definition}
\newtheorem{define}{Definition}


\newcounter{tmp}

% \usepackage{fontspec}
% \usepackage{unicode-math}
% \newcommand{\stripsp}{{\sc strips+}}
\newcommand{\stripsp}{{\sc STRIPS+}}
% \newcommand{\pddlthree}{{\sc PDDL3}}
\newcommand{\pddlthree}{{PDDL3}}
\newcommand{{\lama}}{{{LAMA}}}
\newcommand{\mercury}{{Mercury}}
\newcommand{{\fdss}}{{Fast Downward Stone Soup 2018}}
\newcommand{{\fdssshort}}{{FDSS 2018}}
\newcommand{\lamapruning}{$\text{LAMA}_{\text{P}}(h_\text{R})$}

\newcommand{{\miplan}}{{MIPlan}}
\newcommand{{\ibacop}}{{IBaCoP2}}
\newcommand{{\fdremixcomplete}}{{Fast Downward Remix}}
\newcommand{\lprpgp}{{LPRPG-P}}
\newcommand{{\fdremix}}{{FDRemix}}
% \newcommand{{\fifthIPC}}{{{IPC5}}}

\newcommand{\strips}{{\sc strips}}
\newcommand{\adl}{{\sc adl}}
\newcommand{\pddlIII}{{\sc{pddl3}}}
\newcommand{\stripspap}{{\sc strips+ap}}
\newcommand{\stripspp}{{\sc strips+p}}
\newcommand{\pddl}{{\sc pddl}}
% new command
\newcommand{\hplanp}{{HPlan-P}}
\newcommand{\mipsxxl}{{MIPS-XXL}}
\newcommand{\lpg}{{LPG}}
\newcommand{{\GBLcoles}}{{{GBL15-NB-B15}}}
% \newcommand{{\fifthIPC}}{{ {IPC5}}}
% \newcommand{{\fifthIPC}}{{\footnotesize {IPC5}}}
\newcommand{{\fifthIPC}}{{{IPC5}}}
\newcommand{\threatA}{T_{\mathcal{A}}(o)}
\newcommand{\violA}{V_{\mathcal{A}}(o)}
\newcommand{\threatSB}{T_{\mathcal{SB}}(o)}
\newcommand{\supportSB}{S_{\mathcal{SB}}(o)}
\newcommand{\threatAMO}{T_{\mathcal{AO}}(o)}
\newcommand{\supportST}{S_{\mathcal{ST}}(o)}
\newcommand{\affected}{\mathcal{I}(o)}
\newcommand{\truettt}{\small \texttt{TRUE}}
\newcommand{\pviol}{P\textit{-violated}}
\newcommand{\affectedd}{P_{\textit{affected}}(o)}

\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
  \pdfinfo{
/Title (Compiling PDDL3 Qualitative Preferences without Using Automata)
/Author (Anonymous)}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Compiling PDDL3 Qualitative Preferences without Using Automata}
\author{...
}
\maketitle
\begin{abstract}
We address the problem of planning with preferences in propositional 
domains extended with the class of (preferred) temporally extended goals 
supported in  {\pddlIII}, that is part of the standard planning language 
{\pddl} since the 5th International Planning Competition (\fifthIPC). Such 
preferences are useful to characterise plan quality by allowing the user
to express certain soft constraints on the state trajectory of the desired
solution plans.
Starting from the work of Keyder and Geffner on compiling (reachability)
soft goals, we propose a compilation scheme
for translating a {\strips} problem enriched with qualitative  {\pddlIII}
preferences  (and possibly also with soft goals) into an equivalent {\strips}
problem with action costs. 
The proposed compilation, which supports all types of preferences in the benchmarks
from \fifthIPC, allows many existing {\strips} planners to immediately address
planning with preferences. 
An experimental analysis presented in the paper evaluates the performance of
state-of-the-art {\strips} planners supporting action costs using our compilation
approach to deal with qualitative  {\pddlIII} preferences.  The results indicate
that our approach is highly competitive with respect to current planners that
natively support the considered class of preferences.
\end{abstract}



%%%%%%%%%%%%%%%%
% INTRODUZIONE %
%%%%%%%%%%%%%%%%
\section{Introduction}
Planning with preferences, also called ``over-subscription planning" in \cite{Briel-et-al:AAAI04,do-kambhampati04,smith04}, concerns the generation of plans for problems involving soft goals or soft state-trajectory constraints (called preferences in \pddlIII), that it is desired a plan satisfies, but that do not have to be satisfied. The quality of a solution plan for these problems depends on the soft goals and preferences that are satisfied.

For instance, a useful class of preferences than can be expressed in {\pddlIII} \cite{GHLS+09} consists of  {\em always preferences}, requiring that a certain condition should hold in {\em every} state reached by a plan. As discussed in \cite{Weld:94AA,bacKab,GHLS+09,Hplanp:aij09}, adding always preferences to the problem model can be very useful to express safety or maintenance conditions, and other desired plan properties. An simple example of such conditions is ``whenever a building surveillance robot is outside a room, all the room doors should be closed". 

{\pddlIII} supports other useful types of preferences, and in particular the qualitative preferences of types {\em at-end}, which are which are equivalent to soft goals, {\em sometime}, {\em sometime-before} and  {\em at-most-once}, which are all the types used in the available benchmarks for planning with qualitative {\pddlIII} preferences \cite{GHLS+09}. Examples of preferences that can be expressed  through these constructs in a logistics domain are: ``sometime during the plan the fuel in the tank of every vehicle should be full'', ``a certain depots should be visited before another once'',  ``every store should be visited at most once'' (the reader can find additional examples in \cite{GHLS+09}).
 
In this paper, we study propositional planning with these types of preferences through a compilation approach. 




%%%%%%%%%%%%%%%%
% RELATED WORK %
%%%%%%%%%%%%%%%%
\section{Related Work}
Our compilative approach is inspired by the work of Keyder and Geffner \cite{kn:geffner09}
on compiling soft goals into {\strips} with action costs (here denoted with \stripsp). In this work
the compilation scheme introduces, for each soft goal $p$ of the problem, a dummy goal $p'$ that
can be achieved using two actions in mutual exclusion. The first one, which is called \textit{collect(p)},
has cost equal to 0 and requires that $p$ be true when it is applied; the second one, whic is called \textit{forgo(p)},
has cost equal to the utlity of $p$ and requires that $p$ be false when it is applied.
\\\indent Both of these action can be performed at the end of the plan and for each soft goal $p$ but just one of
$\{$\textit{collect(p), forgo(p)}$\}$ can appear in the plan depending on whether the soft goal
has been achived or not. This scheme has achieved good perfomance which can be improved with the use of
an ad hoc admissible heuristic based on the reachability of soft goals \cite{percassi2017improving}.

The most prominent existing planners supporting {\pddlIII} preferences are {\hplanp} \cite{Hplanp:aij09,BM:AAAI06},
which won the ``qualitative preference" track of IPC-5, {\mipsxxl} \cite{Edelk06,Edelk} and the more recent {\lprpgp} \cite{LPRPGP-P:icaps11}
and its extension in \cite{coles2013searching}.
%
These (forward) planners represent preferences through automata whose states are synchronised with the states generated by the action plans, so that an accepting automaton state corresponds to preference satisfaction. For the synchronisation, {\hplanp} and {\lprpgp} use planner-specific techniques, while {\mipsxxl} compiles the automata by modifying the domain operators and adding new ones modelling the automata transitions of the grounded preferences. 

Our computation method is very different from the one of {\mipsxxl} since, rather than translating automata into new operators, the problem preferences are compiled by only modifying the domain operators, possibly creating multiple variants of them. Moreover, our compiled files only use {\stripsp}, while {\mipsxxl} also uses numerical fluents.\footnote{Another compilation scheme using numerical fluents is considered in \cite{GHLS+09} to study the expressiveness of \pddlIII (without an implementation).}

The works on compiling LTL goal formulas by Cresswell and Coddington \cite{CC:ECAI04} and Rintanen \cite{Rintanen:ECAI00}
are also somewhat related to ours, but with important differences. Their methods handle {\em hard} temporally extended
goals instead of preferences, i.e., every temporally extended goal must be satisfied in a valid plan, and hence there
is no notion of plan quality referred to the amount of satisfied preferences. Rintanen's compilation considers only
single literals in the always formulae (while we deal with arbitrary CNF formulas), and it appears that extending
it to handle more general formulas requires substantial new techniques \cite{Rintanen:personal2015}. 
An implementation of Crosswell and Coddington's approach is unavailable, but Bayer and McIlaraith \cite{BM:AAAI06} observed
that their approach suffers exponential blow up problems and performs less efficiently than the approach of {\hplanp}.

\hl{IMPORTANTE, CONFRONTO LAVORO NEBEL:} An important work about soft-trajectory constraints compilation,
which is closely related to ours, has been recently proposed by Wright, Matt\"uller and
Nebel in \cite{nebelCompiling} (cerca riferimento articolo Nebel). This approach is a based
on the compilation of the soft trajectory constraints into conditional effects and state dependent action costs using
$\sf{LTL}_{\text{f}}$ and B{\"u}chi automata. There are some similarities between  
between our and their approach but at the same time there are also
differences. 

\textbf{FP: differenze approccio Nebel e nostro: 1) numero di fluenti introdotto nella compilazione diverso; 2) il nostro
e' un approccio piu' specifico PDDL3-oriented, il loro piu' generale; 3) diverso meccanismo di aggiornamento dei costi, nel loro caso
ricompensa e penalita', nel nosto solo penalità 4) il loro schema prevede quindi costi negativi e postivi, il nostro positivi; nel loro caso per avere solo costi positivi e' necessario perdere l'ottimalita' nel nostro caso no ed inoltre volendo potremmo introdurre dei costi
negativi per quelle preferenze la cui violazione puo' essere testata solo alla fine del piano mantenendo l'ottimalita'; ho fatto inoltre menzione del fatto che i costi possono essere incrementanti il prima possibile (es. always)}

\textbf{1)} In our approach, given a preference $P$,
we introduce at most a pair of boolean fluents, typically one additional fluent to 
represent if a preference is violated or not ($P\textit{-violated}$) and
in some cases an additional fluent to correctly represent the status of a preference during
the planning,
while in their approach it is introduced a boolean variable
for each state of the corresponding automaton of $P$. \textbf{2)} Their approach is more general
while ours is focused on PDDL3 constraints and therefore it is more specific.

\textbf{3)} In their work the cost of the plan during the planning is updated by using rewards
and penalties (negative and positive costs respectively). The cost of the plan is increased whenever
a violation of a preference (also reversible) occurs. On the contrary,
if an operator causes the satisfaction of a preference then the cost of the plan is decreased.
In both cases the negative or positive cost is equal to the utility of the interested preference\footnote{
\hl{Messo in footnote altrimenti era troppo lungo:}
Note that 
both the violation and the satisfaction of a preference may be temporary conditions depending on the type 
of interested preference. For example an always preference can be irreversibly violated and its satisfaction can only be evaluated at the end of the plan considering the whole trajectory of the states; on the contrary a sometime-after preference can be satisfied and violated several times during the execution of the plan.
}. \textbf{4)} This type of cost update requires the use of negative costs while our compilation scheme produces a problem
whose costs are monotonically increasing because, similarly to what was proposed in Keyder and Geffner,
costs are realized only at the end of the planning. In our scheme some costs can be anticipated for those preferences whose violation is irreversible (e.g. always, sometime-before) and negative costs could be used for those preferences that may be ``temporarily" violated (e.g. sometime, at-end). In both cases our scheme would maintain the optimality but in this work we have provided a compilation based only on positive costs in order to take advantage of a wider spectrum of classical planners (few planners still support negative costs).








we propose a compilation scheme for translating a {\strips} problem with {\pddlIII} qualitative preferences into an equivalent {\stripsp} problem. Handling action costs is a practically important, basic functionality that is supported by many powerful planners; the proposed compilation method allows them to immediately support (through the compiled problems)  the considered class of preferences with no change to their algorithms and code.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SEZIONE DEFINIZIONE PROBLEMA %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Propositional Planning with Qualitative PDDL3 Preferences}
A {\strips}  problem is a tuple $\langle F, I, O, G\rangle$ where $F$ is a set of fluents, $I \subseteq F$ and $G \subseteq F$ are the initial state and goal set, respectively, and $O$ is a set of actions or operators defined over $F$ as follows.

A {\strips} operator $o \in O$ is a pair $\langle \textit{Pre}(o), {\textit{Eff}}(o)\rangle$, where 
$\textit{Pre}(o)$ is a sets of atomic formulae over $F$ and ${\textit{Eff}}(o)$ is a set of literals over $F$.  ${\textit{Eff}}(o)^+$ denotes the set of positive literals
in ${\textit{Eff}}(o)$, ${\textit{Eff}}(o)^-$ the set of negative literals in ${\textit{Eff}}(o)$.
An action sequence $\pi=\langle a_0, \ldots, a_m\rangle$ is applicable in a planning problem $\Pi$ if all actions $a_i$ are in $O$ and there exists a sequence of states $\langle s_0, \ldots, s_{m+1}\rangle$ such that $s_0=I$, $prec(a_i) \subseteq s_i$ and $s_{i+1}=s_i  \setminus 
\{p \mid \neg{p} \in {\textit{Eff}}(a_i)^-$\}$\cup{\textit{Eff}}(a_i)^+$, for $i = 0 \dots m$. Applicable action sequence $\pi$ achieves a fluent $g$ if $g \in s_{m+1}$, and is a valid plan for $\Pi$ if it achieves each goal $g \in G$ (denoted with $\pi \models G$).

A {\stripsp} problem is a tuple $\langle F,I,O,G,c \rangle $, where $\langle F,I,O,G \rangle $ is a {\strips} problem and $c$ is a function 
mapping each $o\in O$ to a non-negative real number.
The cost $c(\pi)$ of a plan $\pi$ is $\sum_{i=0}^{|\pi|-1} c(a_i)$, where $c(a_i)$ denotes the cost of the $i$th action $a_i$ in $\pi$ and $|\pi|$ is the length of $\pi$.

Starting from a {\stripsp} problem we define the 
notion of state trajectory generatered by a plan. Given a 
{\stripsp} $ \Pi = \langle F,I,O,G,c \rangle $ problem, a plan $\pi$ generates the trajectory
$\langle (S_0,0),(S_1,t_1),...,(S_n,t_n)\rangle$ iif
$S_0 = I$ and for each happening $h$ generated by $\pi$, with $h$
at time $t$, there is some $i$ such that $t_i = t$ and $S_i$ is the
result of applying the happening $h$ to $S_{i-1}$, and for every
$j \in \{1 \ldots n\}$ there is a happening $\pi$ at $t_j$.

The following is a fragment of the grammar of {\pddlIII},
describing the new modalities of {\pddlIII} for expressing these soft state-trajectory constraints
(indicated with {\small \texttt{con-GD}})
(the full BNF grammar is given in \cite{techReportGerevini1,techReportGerevini2})
where \texttt{<GD>} is a goal description (a first order logic formula):

{\small
\begin{verbatim}
  <con-GD> ::= (at end <GD>) | (always <GD>) |
               (sometime <GD>) |
               (at-most-once <GD>) | 
               (sometime-after <GD> <GD>) | 
               (sometime-before <GD> <GD>)
\end{verbatim}
}
Let $\phi$ and $\psi$ be atomic formulae over the predicates of a planning problem,
then interpretation of the modal operators considered in this work is specified in Figure \ref{fig:semantics} [COMMENTATA].
We write $\pi \models P$ to indicate that the state-trajectory generated by $\pi$ satisfies a preference $P$ and we indicate with $\mathcal{A}$, $\mathcal{SB}$, $\mathcal{ST}$, $\mathcal{AO}$, $\mathcal{SG}$ the classes of all preferecense of always, sometime-before, sometime, at-most-once and soft goal respectively for a given {\stripsp} problem.


% FIGURA CONTENENTE LA SEMANTICA DA SCOMMENTARE

% \begin{figure}[t]
% \vspace{-7mm}
% {\small
% \[\begin{array}{ll}
% \multicolumn{2}{l}{\langle (S_0,0),(S_1,t_1),...,(S_n,t_n)\rangle \models (\mbox{\textit{at\,end}}~\phi)} \\
%  & \hspace{1cm} \mbox{iff} \hspace{5mm} S_n \models \phi\\
% \multicolumn{2}{l}{\langle (S_0,0),(S_1,t_1),...,(S_n,t_n)\rangle \models \phi} \\
%  &\hspace{1cm}  \mbox{iff} \hspace{5mm} S_n \models \phi\\
% \multicolumn{2}{l}{\langle (S_0,0),(S_1,t_1),...,(S_n,t_n)\rangle \models (\mbox{\textit{always}}~\phi)} \\
%   &\hspace{1cm} \mbox{iff} \hspace{5mm} \forall i: 0 \leq i\leq n \cdot S_i \models \phi\\
% \multicolumn{2}{l}{\langle (S_0,0),(S_1,t_1),...,(S_n,t_n)\rangle \models (\mbox{\textit{sometime}}~\phi)} \\
%  &\hspace{1cm}  \mbox{iff} \hspace{5mm} \exists i: 0 \leq i \leq n \cdot S_j \models \phi\\

% \multicolumn{2}{l}{\langle (S_0,0),(S_1,t_1),...,(S_n,t_n)\rangle \models (\mbox{\textit{at-most-once}}~\phi)}\\
% &\hspace{1cm}  \mbox{iff} \hspace{5mm} \forall i : 0 \leq i \leq n \cdot~\mbox{if}~S_i \models \phi~\mbox{then}~\\
% &\hspace{3.8cm} \exists j : j \geq i \cdot \forall k : i \leq k \leq j \cdot S_k \models \phi \mbox{ and}~\\
% &\hspace{3.8cm} \forall k : k > j \cdot S_k \models \neg\phi\\

% \multicolumn{2}{l}{\langle (S_0,0),(S_1,t_1),...,(S_n,t_n)\rangle \models
% (\mbox{\textit{sometime-after}}~\phi~\psi)}\\
%  & \hspace{1cm} \mbox{iff} \hspace{5mm} \forall i \cdot 0 \leq i \leq n \cdot \mbox{if}~S_i \models \phi~\mbox{then}~\exists j : i \leq j \leq n \cdot S_j \models \psi \\
% \multicolumn{2}{l}{\langle (S_0,0),(S_1,t_1),...,(S_n,t_n)\rangle \models
% (\mbox{\textit{sometime-before}}~\phi~\psi)}\\
%  & \hspace{1cm} \mbox{iff} \hspace{5mm} \forall i \cdot 0 \leq i \leq n \cdot \mbox{if}~S_i \models \phi~\mbox{then}~\exists j : 0 \leq j < i \cdot S_j \models \psi \\

% \end{array}\]
% }

% \vspace{-0.5cm}
% \caption{\label{fig:semantics}
% Semantics of the basic modal operators in {\pddlIII}. $\phi$ and $\psi$ stand for arbitrary (syntactically valid) goal formulae of {\pddlIII}.}
% \end{figure}

But in the following, without loss of generality, we will assume in the discussion that the formula $\phi$ and $\psi$ involved in a preference $P$ is expressed in conjuctive normal form $\phi = \phi_{1} \land \phi_{2} \land ... \land \phi_{n}$ where each $\phi_{i}$ ($i \in \{1 \ldots n\}$) is a clause of $P$ formed by literals over the problem fluents. 

\begin{mydef}
A {\strips+} problem with preferences is a tuple $\langle F, I, O, G, \mathscr{P}, c, u \rangle$ where:
\begin{itemize}
    \item $\langle F,I,O,G,c \rangle$ is a \strips+ problem;
    \item $\mathscr{P} = \{ \mathscr{P}_{\mathcal{A}}  \cup  \mathscr{P}_{\mathcal{SB}} \cup  \mathscr{P}_{\mathcal{ST}} \cup  \mathscr{P}_{\mathcal{AO}} \cup  \mathscr{P}_{\mathcal{SG}} \}$ is the set of the preferences of\ $\Pi$ where $\mathscr{P}_{\mathcal{A}} \subseteq \mathcal{A}$, $\mathscr{P}_{\mathcal{SB}} \subseteq \mathcal{SB}$, $\mathscr{P}_{\mathcal{ST}} \subseteq \mathcal{ST}$, $\mathscr{P}_{\mathcal{AO}} \subseteq \mathcal{AO}$ and $\mathscr{P}_{\mathcal{SG}} \subseteq \mathcal{SG}$;
    \item $u$ is an utility function mapping each $P \in \mathscr{P}$ to a value in $\mathbb{R}_0^{+}$.
\end{itemize}
\end{mydef}

In the following the class of {\stripsp} problems with a set of preferences is indicated with \stripspp.

\begin{mydef}
Let $\Pi$ be a {\stripspp} problem. The utility $u(\pi)$ of a plan $\pi$ solving $\Pi$ is the difference between the total amount of utility of the preferences by the plan and its cost:
$$u(\pi) = \sum_{P \in \mathscr{P}: \pi \models_{} P}{u(P)} - c(\pi).$$
\end{mydef}

The definition of plan utility for {\stripspp} is similar to the one given for {\stripsp} with soft goals by Keyder and Geffner \cite{kn:geffner09}. 
A plan $\pi$ with utility $u(\pi)$ for a {\stripspp} problem is optimal when there is no plan $\pi'$ such that $u(\pi') > u(\pi) $. The definitions below are introduced to simplify the notation in the discussion.

\section{Operator-Preference Interactions}

The definitions below are introduced to simplify the notation in the discussion.

\begin{mydef}
Given a preference clause $\phi_{i} = l_1 \lor l_2 \lor ... \lor l_m$, the set $L(\phi_{i}) = \{l_1, l_2, ..., l_m\}$ is the equivalent set-based definition of $\phi_{i}$ and $\overline{L}(\phi_{i}) = \{\lnot l_1, \lnot l_2, ..., \lnot l_m\}$ is the literal complement set of $L(\phi_{i})$.
\end{mydef}

\begin{mydef}
Given an operator $o \in O$ of a {\stripspp} problem, $Z(o)$ is the set of literal defined as:
$$
Z(o) = (\textit{Pre}(o) \setminus \{p \mid \lnot p \in \textit{Eff}(o)^{-}\}) \cup \textit{Eff}(o)^{+} \cup \textit{Eff}(o)^{-}.
$$
\end{mydef}
Note that the literals in $Z(o)$ hold in any reachable state resulting from the execution of operator $o$.

To shorten the following definitions and explanation we indicate the state where the operator $o$ is applied with $s$ and the state resulting from the application of $o$ with $s'$.

\begin{mydef}\label{def:trueclauses}
Given an operator $o$ and CNF formula $\phi = \phi_{1} \land ... \land \phi_{n}$, we define the set of clauses which will surely be true
in the resulting state from the application of $o$ as:
% $$
% C_{\phi}(o) =  \{\phi_{i} \mid L(\phi_{i}) \cap Z(o) \not = \O, \;i \in \{1 \ldots n\}\}
% $$
$$
C_{\phi}(o) =  \{\phi_{i} : |L(\phi_{i}) \cap Z(o)| > 0, \;i \in \{1 \ldots n\} \}
$$
We can also define the complementary set of the remaining clauses of $\phi$ which does not satisfy the previous condition as $\overline{C}_{\phi}(o) = \{ \phi_{i} : \phi_i \not \in C_{\phi}(o), \;i \in \{1 \ldots n\} \}$
\end{mydef}

\begin{mydef}
Given a preference clause $\phi_{i} = l_1 \lor l_2 \lor ... \lor l_m$, the set $L(\phi_{i}) = \{l_1, l_2, ..., l_m\}$ is the equivalent set-based definition of $\phi_{i}$ and $\overline{L}(\phi_{i}) = \{\lnot l_1, \lnot l_2, ..., \lnot l_m\}$ is the literal complement set of $L(\phi_{i})$.
\end{mydef}

\begin{mydef}
Given an operator $o \in O$ of a {\stripspp} problem, $Z(o)$ is the set of literal defined as:
$$
Z(o) = (\textit{Pre}(o) \setminus \{p \mid \lnot p \in \textit{Eff}(o)^{-}\}) \cup \textit{Eff}(o)^{+} \cup \textit{Eff}(o)^{-}.
$$
\end{mydef}
Note that the literals in $Z(o)$ hold in any reachable state resulting from the execution of operator $o$.

To shorten the following definitions and explanation we indicate the state where the operator $o$ is applied with $s$ and the state resulting from the application of $o$ with $s'$.

\begin{mydef}\label{def:trueclauses}
Given an operator $o$ and CNF formula $\phi = \phi_{1} \land ... \land \phi_{n}$, we define the set of clauses which will surely be true
in the resulting state from the application of $o$ as:
% $$
% C_{\phi}(o) =  \{\phi_{i} \mid L(\phi_{i}) \cap Z(o) \not = \O, \;i \in \{1 \ldots n\}\}
% $$
$$
C_{\phi}(o) =  \{\phi_{i} : |L(\phi_{i}) \cap Z(o)| > 0, \;i \in \{1 \ldots n\} \}
$$
We can also define the complementary set of the remaining clauses of $\phi$ which does not satisfy the previous condition as $\overline{C}_{\phi}(o) = \{ \phi_{i} : \phi_i \not \in C_{\phi}(o), \;i \in \{1 \ldots n\} \}.$
\end{mydef}

With reference to Definition \ref{def:trueclauses}, given a clause $\phi_{i} = l_1 \lor ... \lor l_{m_i}$ of $\phi$, the condition $|L(\phi_{i}) \cap Z(o)| > 0$ requires that exists at least a literal $l_j$, with $j \in \{1 \ldots m_i\}$, that, belonging to the set $Z(o)$, will be certainly true in the resulting state from the application of the operator $o$ thus making the clause $\phi$ true in $s'$.

% Each literal $l_j \in Z(o)$ can be true in $s'$ in different ways: \textit{i)} it could be already true in the preconditions of $o$, so in $s$, and not falsified by $o$ or \textit{ii)} it could become true in $s'$ by the effects of $o$ regardless of whether it was true or false in $s$.

\begin{mydef}\label{def:becometrue}
Given an operator $o$ and a CNF formula $\phi = \phi_{1} \land ... \land \phi_{n}$, we say that $o$ \textbf{can make true} $\phi$ if
\begin{enumerate}
% $C_{\phi}(o) \not = \O$ and
\item $|C_{\phi}(o)|>0$;

\item for each clause $\phi \not \in \overline{C}_{\phi}(o), \; \overline{L}(\phi) \not \subseteq Z(o)$.

\end{enumerate}
\end{mydef}

The first condition in Definition \ref{def:becometrue} requires that exists at least a clause of the formula which contains some literals which will certainly be true true in the state resulting from the execution of $o$. The second condition in Definition \ref{def:becometrue} requires that the remaining clauses of $\phi$, which do not belong to $C_{\phi}(o)$, are certainly not falsified in $s'$, because otherwise the $\phi$ formula will be certainly falsified in $s'$.

An operator $o$ that \textit{can make} $\phi$ does not not guarantees a switch from a state $s \models \lnot \phi$ to a state $s' \models \phi$, but it only guarantees $s' \models \phi$ without imposing any condition on the truth value of $\phi$ in $s$.

In our compilation scheme of a {\stripspp} problem we have to distinguish, for each kind of preference,
different class of operators in order to specialize the operators compilation based on 
how they interact with the preferences of the problem.

Generally speaking, we can identify three classes of operators regardless of the type of preference considered. An
operator is a \textit{threat} for a preference $P$ if in case it is executed it may violate $P$. An
operator is a (potential) \textit{support} for a preference $P$ if in case it is executed it could satisfy $P$. Finally,
an operator $o$ is \textit{neutral} for a preference $P$ if its execution
can not influence the current state of the preference. In the following Subsection \ref{subsection:always_operators}--\ref{subsectionAMO}
we will decline these generic definitions of classes of operators for each type 
of considered preference providing for each of them a formal definition.

In the following definitions of operators classes we will assume that $\phi$ (and $\psi$ in the case of dual preferences) are arbitrary CNF formulae $\phi = \phi_{1} \land ... \land \phi_{n}$ where each clause $\phi_{i} = l_{1} \lor ... \lor l_{m_{i}}$ for each $i \in \{1 \ldots n\}$.


\subsection{Operators Affecting Always Preferences} \label{subsection:always_operators}
An always preference has the following {\pddl} syntax 
$$
(preference\text{ }P\text{ }({\textit{always}}\text{ }\phi))
$$
where the formula $\phi$ has to hold in each reached state of the plan.
In the following we will abbreviate it with $P = \sf{A}_{\phi}$. According to
the semantic provided in Figure \ref{fig:semantics} a 
violation of $P = \sf{A}_{\phi}$ is irreversible, when a state $s$ such that
$s \not \models \phi$ occurs then it is no longer possible to satisfy $P$.

% ----------
% VIOLAZIONE
% ----------
\begin{mydef}\label{violation_formula}
Given an operator $o$ and an always preference $P = \sf{A}_{\phi}$ of a {\stripspp} problem, $o$ is a \textbf{violation} of $P$ if there is a clause $\phi_i$ of $\phi$ such that:
\begin{enumerate}
\item $\overline{L}(\phi_i) \subseteq Z(o)$;
\item $\overline{L}(\phi_i) \not\subseteq \textit{Pre}(o)$.

\end{enumerate}

\end{mydef}

The conditions of Definition \ref{violation_formula} require that
there exists at least a clause of $\phi$ (1) whose literals are falsified after the execution of $o$ (2)
which is not already false in the state where $o$ is applied.

If an operator violates a preference, the preference is unsatisfied in any state resulting
from the application of the operator. The set of always preferences that
are violated by an operator $o$ is denoted with $\violA$.

% --------
% MINACCIA
% --------
\begin{mydef}\label{threat_formula}
Given an operator $o$ and an always preference $P = \sf{A}_{\phi}$ of a {\stripspp} problem, $o$ is a \textbf{threat} of $P$ if it is not a violation and there exists a clause $\phi_i$ of $\phi$ such that:
\begin{enumerate}
% \item $\overline{L}(\phi_i) \cap Z(o) \not = \O $
\item $|\overline{L}(\phi_i) \cap Z(o)| > 0 $
% \item $L(\phi_i) \cap Z(o) = \O $
\item $|L(\phi_i) \cap Z(o)| = 0 $
\item $\overline{L}(\phi_i) \not \subseteq \textit{Pre}(o)$.
% \end{gather}
\end{enumerate}
\end{mydef}

The conditions of Definitin \ref{threat_formula} require that there exist at least a clause of $\phi$ that
(1) has some (but not all) literals which are falsified after the execution of $o$,
(2) has not literals which are true in the resulting state from the application of $o$ and (3)
this clause is not already false in the state where $o$ is applied. The expression 
$\overline{L}(\phi_i) \not \subseteq \textit{Pre}(o)$ in Definition \ref{violation_formula}-\ref{threat_formula}
is necessary to avoids that an operator $o$ is considered a violation/threat when its precondition is
already violated in the state where it is applied.

A clause $\phi_i$ of $P = \sf{A}_{\phi}$ satisfying these conditions
is a \textit{threatened clause} of $P$. A threatened preference (clause) may be falsified by an operator depending on the state where the operator is applied. The set of always preferences threatened by an operator $o$ is denoted with $\threatA$; the set of clauses of an always preference $P$ threatened by $o$ is denoted with $T_{{\mathcal{A}}}(o, P)$.
% --------
% NEUTRALE
% --------
\begin{mydef}
Given an operator $o$ and an always preference $P = \sf{A}_{\phi}$ of a {\stripspp} problem, $o$ is a \textbf{neutral} operator for $P$ if:
\begin{enumerate}
    % \item for all clauses $\phi_i$ of $P$, $L(\phi_i) \cap Z(o) \not = \O $ or $ \overline{L}(\phi_i) \cap Z(o) = \O$ holds;
    \item for all clauses $\phi_i$ of $P$, $|L(\phi_i) \cap Z(o)| > 0 $ or $ |\overline{L}(\phi_i) \cap Z(o)| = 0$ holds;

    \item there exists a clause $\phi_i$ such that $\overline{L}(\phi_i) \subseteq \textit{Pre}(o)$.
\end{enumerate}
\end{mydef}

For example an operator $o$ such that $\textit{Pre}(o) = \{\lnot a\}$ and $\textit{Eff}(o) = \{\lnot b\}$ 
is a threat for $P1 = \mathsf{A}_{\phi^{P1}}$, a violation for $P2 = \mathsf{A}_{\phi^{P2}}$ and neutral for
$P3 = \mathsf{A}_{\phi^{P3}}$ where 
$\phi^{P1} = c \lor b$, $\phi^{P2} = a \lor b$
and $\phi^{P3} = d$.


% ----------------------------------------------
% CLASSI DI OPERATORI PER LA PREFERENZA SOMETIME
% ----------------------------------------------
\subsection{Operators Affecting Sometime Preferences} \label{subsection:sometime_operators}
A sometime preference has the following {\pddl} syntax 
$$(preference\text{ }P\text{ }(sometime\text{ }\phi))$$
where the formula $\phi$ has to become true at least once state in the plan state trajectory. In the following we abbreviate with $P = \sf{ST}_{\phi}$. According to
the semantic provided in Figure \ref{fig:semantics}
it is not possible to deliberate if a preference $P = \sf{ST}_{\phi}$ is violated
until the end of the planning but the preference can be satisfied at any time during planning 
if an operator makes $\phi$ true.

\begin{mydef} \label{sometime_definition}
Given an operator $o$ and a sometime preference $P = \sf{ST}_{\phi}$ of a {\stripspp} problem, $o$ is a \textbf{potential support} for $P$ if $o$ can make true $\phi$ otherwise the operator is \textbf{neutral} for $P$.
\end{mydef}

The set of sometime preferences of $\Pi$ which are potentially supported by the operator $o$ are denoted with $S_{\mathcal{ST}}(o)$.

For example an operator $o$ such that $\textit{Pre}(o) = \O$ and $\textit{Eff}(o) = \{\lnot b\}$ 
is a potential support for $P1 = \mathsf{ST}_{\phi^{P1}}$ where $\phi^{P1} = c \lor \lnot b$.


% -----------------------------------------------------
% CLASSI DI OPERATORI PER LA PREFERENZA SOMETIME BEFORE
% -----------------------------------------------------
\subsection{Operators Affecting Sometime-before Preferences}
A sometime-before preference has the following {\pddl} syntax and its semantic requires that,
$$(preferences\text{ }P\text{ }(sometime\text{-}before\text{ }\phi\text{ }\psi))$$
whenever $\phi$ is true in a state $s$ then $\psi$ must have been true in a state before $s$.
In the following we will abbreviate it with $P = \sf{SB}_{\phi, \psi}$. Likewise to what exposed for the
always preferences a 
violation of $P = \sf{SB}_{\phi, \psi}$ is irreversible, when a state $s$ such that
$s \not \models \phi$ occurs 
without having been made $\psi$ true in a previous state
then it is no longer possible to satisfy $P$.

Speaking informally, if an operator $o$, applied in a state where $\phi$ has never been made true before, 
makes $\psi$ true in the resulting state, 
then we consider it a \textit{support} for $P$. Once a support is
applied then preference can no longer be violated.

% POTENTIAL SUPPORT
\begin{mydef}\label{sup_sb}
Given an operator $o$ and a sometime-before preference $P = \sf{SB}_{\phi, \psi}$ of a {\stripspp} problem, $o$ is a \textbf{potential support} for $P$ if $o$ can make $\psi$ true.\\
\end{mydef}
An operator $o$ that satisfied Definition \ref{sup_sb} is a \textit{potential} support for $P$ because it can act as an \textit{actual} support only under certain preconditions which depend by the stare where it is applied. If
a potential support $o$ is applied we can distinguish two possible behaviors:
\begin{itemize}
    \item if $\psi$ does not become true in the resulting state, then $o$ is \textit{neutral} for $P$;
    
    \item if $\psi$ becomes true in the resulting state and $P$ has not been violated,
    then $o$ is a \textit{support} for $P$. 
    % $o$ is a \textit{real support} operator.
\end{itemize}
% The compilation scheme must take account of both these possibilities introducing two compiled operators.

% THREAT
\begin{mydef}
Given an operator $o$ and a sometime-before preference $P = \sf{SB}_{\phi, \psi}$ of a {\stripspp} problem, $o$ is a \textbf{threat} for $P$ if $o$ can make true $\phi$.
\end{mydef}
Similarly to Definition \ref{sup_sb} also in this case the behavior of a
threat when it is applied depends by the context. We distinguish the following situations
\begin{itemize}
    \item if $\psi$ does not become true in the resulting state, then $o$ is \textit{neutral} for $P$;
    \item if $\psi$ becomes true in the resulting state and the formula $\phi$ has become true at least once in a earlier state, than $s$ is \textit{neutral} for $P$ otherwise if the formula $\phi$ has never become true in previous states,
    then $o$ is a \textit{violation} for $P$.
\end{itemize}

The set of sometime-before preferences of $\Pi$ which are threatened and potentially supported by the operator $o$ are denoted respectively with $T_{\mathcal{SB}}(o)$ and $S_{\mathcal{SB}}(o)$.

\begin{mydef}
Given an operator $o$ and a sometime-before preference $P = \langle \phi, \psi \rangle$ of a {\stripspp} problem, $o$ is a \textbf{neutral} for $P$ if $o$ is not a support or a threat.
\end{mydef}

For example an operator $o$ such that $\textit{Pre}(o) = \O$ and $\textit{Eff}(o) = \{ b\}$ 
is a potential support for $P1 = \mathsf{SB}_{\phi^{P1}, \psi^{P1}}$, a threat for $P2 = \mathsf{SB}_{\phi^{P2}, \psi^{P2}}$ and neutral for
$P3 = \mathsf{SB}_{\phi^{P3}, \psi^{P3}}$ where 
$\phi^{P1} = c $ and $\psi^{P1} = a \lor b$
$\phi^{P2} = c \lor b$ and $\psi^{P2} = d$
and $\phi^{P3} = d$ and $\psi^{P3} = e$.


\subsection{Operators Affecting At-most-once Preferences}\label{subsectionAMO}

An at-most-once preference has the following {\pddl} syntax 
$$(preference\text{ }P\text{ }(at\textit{-}most\textit{-}once\text{ }\phi)$$
where the formula $\phi$ has to become true at most once in the plan state trajectory. In the following we will abbreviate it with $P = \sf{AO}_{\phi}$. 

% Definizione di minaccia
\begin{mydef}\label{threat_amo}
Given an operator $o$ and an at-most-once preference $P = \sf{AO}_{\phi}$ of a {\stripspp} problem, $o$ is a \textbf{threat} operator for $P$ if $o$ can make true $\phi$.
\end{mydef}

We distinguish the following situations:
\begin{itemize}
    % \item if $\Phi$ has never become true in states earlier than the state $s$ where $o$ is applied and $\Phi$ becomes true in the state resulting from the application of $o$ in $s$, then the corrispondent compiled operator $o'$ has to take account this fact, otherwise, if $\Phi$ has become true in a earlier state, then $o$ is a \textit{violation};
    % state resulting from the application of $o$

    \item if $\phi$ has never become true in all the states earlier than the state $s$ where $o$ is applied and becomes true in the resulting state from the application of $o$ then the operator behaves as \textit{neutral} for $P$;

    \item if $\phi$ does not become true in the state resulting state from the application of $o$, then $o$ behaves as \textit{neutral} for $P$;

    \item if $\phi$ has become true in a earlier state and will become true in the resulting state from the application of $o$, the $o$ behaves as a \textit{violation} for $P$.
\end{itemize}

The set of at-most-once preferences of $\Pi$ which are threatened by the operator $o$ are denoted with $T_{\mathcal{AO}}(o)$.

For example an operator $o$ such that $\textit{Pre}(o) = \O$ and $\textit{Eff}(o) = \{\lnot b\}$ 
is a potential support for $P1 = \mathsf{AO}_{\phi^{P1}}$ where $\phi^{P1} = c \lor \lnot b$.

\section{Compilation of Qualitative Preferences}

Before describing the general compilation scheme, some further definitions should be provided.

% ------------------
% OPERATORE NEUTRALE
% ------------------
\begin{mydef} \label{def:neutral-operators}

Given a {\stripspp} problem $\Pi$ if an operator $o$ of $\Pi$ is neutral for every given preference of $\Pi$ over
$\mathcal{A}$, $\mathcal{SB}$, $\mathcal{ST}$, $\mathcal{AO}$ and $\mathcal{SG}$
then we say that $o$ is \textbf{neutral} for $\Pi$.
The set of all the neutral operators for $\Pi$ is denoted by $O_{\textit{neutral}}$.
\end{mydef}

% ------------------
% AFFECTED OPERATORS
% ------------------
\begin{mydef}
Given an operator $o$ of a {\stripspp} $\Pi$ problem the set $P_{\textit{affected}(o)}$ of prefereces affected by $o$ is defined as:
$$
P_{\textit{affected}(o)} =
\threatA \cup \threatSB \cup \supportSB \cup \threatAMO \cup \supportST.
$$
\end{mydef}

Given a {\stripspp} problem, an equivalent {\stripsp} problem can be derived by translation which has some similarities to what proposed by Keyder and Geffner for soft goals but also significant difference. The scheme proposed by Keyder and Geffner is considerable simpler than ours because it does not to consider the interaction between actions and preferences such as threats, supports and violations. In order to simplify the compilation scheme we don't consider the compilation of soft goals because it can be easily added using the same method of Keyder and Geffner.

Moreover we assume that every always and sometime-before preference are not violated and every sometime preference are not satisfied in the problem initial state $I$. Before starting the compilation, we carry out the following checks
in order to exclude some preferences from the process:
\begin{itemize}

\item for each $P = {\mathsf{A}_{\phi} \in \mathscr{P}_{\mathcal{A}}}$ we check that $I \models \phi$, if the condition
does not hold
we exclude $P$ from the compilation increasing the cost of the plan by $u(P)$ because it is already violated in $I$;

\item for each $P = {\mathsf{SB}_{\phi, \psi}} \in \mathscr{P}_{\mathcal{SB}}$ we check that $I \not \models \phi$, if the condition
does not hold
we exclude $P$ from the compilation increasing the cost of the plan by $u(P)$
because it is already violated in $I$; after that we check that 
$I \not \models \phi \land I \models \psi$, if the condition hold we exclude $P$ because it is already satisfied in $I$;

\item for each $P = {\mathsf{ST}_{\phi} \in \mathscr{P}_{\mathcal{ST}}}$ we check that $I \models \phi$, if the conditiom holds
we exclude $P$ because it is already satisfied in $I$;

\end{itemize}



Given a {\stripspp} problem $\Pi = \langle F,I,O,G,\mathscr{P},c,u \rangle$, the compiled \strips+ problem of $\Pi$ is $\Pi' = \langle F,'I',O',G',c' \rangle$ where:

\begin{itemize}
    \item {$F' = F \cup V \cup D \cup C \cup \overline{C'} \cup \{\textit{\it normal-mode}, \text{\it end-mode}\}$};

    \item $I' = I \cup \overline{C'} \cup V_{\mathcal{ST}} \cup $ $S_{\mathcal{AO}}$  $ \cup \{\text{\it normal-mode}\}$;

    \item $G' = G \cup C'$;

    \item $O' = \{\text{\it collect}(P_i), \text{\it forgo}(P_i) \mid P_i \in \mathscr{P}\} \cup \{end\} \cup 
    \{{comp}(o, \mathscr{P}) \;\mid\; o \in O\}$



    \item 
$ c'(o') =
\left\{
    \begin{array}{ll}
        u(P)  & \mbox{if } o' = \text{\it forgo}(P) \\
        % c(o) & \mbox{if } o \in O_{{comp}} \\
        c(o') & \mbox{if } o' = \textit{comp}(o , \mathscr{P}) \\
        0 & \mbox{otherwise}

    \end{array}
\right.
$
\end{itemize}

\noindent where:

\begin{itemize}
    % CAMBIO DELL'INDICE
    % \item$PV = \cup_{i=1}^{k} \{P_i\text{\it-violated}\}, k = |P|$; 

    \item$V = \cup_{i=1}^{|\mathscr{P}|} \{P_i\text{\it-violated}\}$;

    \item$V_{\mathcal{ST}} = \cup_{i=1}^{|\mathscr{P}_\mathcal{ST}|} \{P_i\text{\it-violated}\}$, $\mathscr{P}_\mathcal{ST} \subseteq \mathscr{P}$; this is the set of the violation predicates restricted to the subset of the sometime preferences of all preferences $\mathscr{P}$;

    \item {
    $S_{\mathcal{AO}} = \cup_{i=1}^{|\mathscr{P}_{\mathcal{AO}}(I)|}\{P_i\text{-}seen\}$, $\mathscr{P}_\mathcal{AO}(I) 
    = \{P_i = \mathsf{AO}_{\phi} \mid P_i \in {\mathscr{P}}_{\mathcal{AO}}, I \models \phi\} $; this is the set of \textit{seen} predicates (see Section \ref{subsect:amo_comp}) for those at-most-once preferences such that $I \models \phi$;}

    % NON PIU' NECESSARIO CON GLI EFFETTI CONDIZIONALI
    % \item$D = \cup_{i=1}^n\{P_i\text{-}done\}$ where $n = |\mathscr{P}|$;

    \item$C'_{} = \{P' \mid P \in \mathscr{P}\}$ and $\overline{C'}_{} = \{\overline{P'} \mid P \in \mathscr{P}\}$;

    \item$\text{\it forgo}(P_i) = \langle \{\text{\it end-mode}, P_i\text{\it-violated}, \overline{P_i'}\}, \{{P}', \lnot \overline{{P}'}\} \rangle$;

    \item$\textit{end} = \langle \{ \text{\it normal-mode} \}, \{\text{\it end-mode}, \text{\it normal-mode}\} \rangle$;

    % NON PIU' NECESSARIO - VERSIONE SENZA EFFETTI CONDIZIONALI
    % \item $O_{comp} = O_{{\it neutral}} \cup O_{chained} \cup O_{violation}$

    % \item $O_{comp} = O_{\mathcal{N}} \cup O_{\overline{\mathcal{N}}}$;
    % \item $O_{\mathit{comp}} = O_{\mathit{comp}}^{N} \cup O_{\mathit{comp}}^{\overline{N}}$;

    % ALTERNATIVA O_{neutral/safe}
    % \item $O_{\mathit{comp}}^{{N}} = \{\langle \textit{Pre}(o) \cup \{\text{\it normal-mode}\}, \textit{Eff}(o)$\}$\rangle \mid o \in O_{\textit{neutral}}$;

    % NON PIU' NECESSARIO - VERSIONE SENZA EFFETTI CONDIZIONALI
    % \item $O_{chained}$ and $O_{violation}$ are the operator sets generated by the operator transformation schema applied to the operators of $\Pi$ that affect at least one preference of $\Pi$. An operator $o \in O$ is compiled through the compilation schema if $|I(o)| > 0$ otherwise it is considered neutral. Such set will be defined after presenting the general idea for compiling an operator affecting a preference.

    % \item  $O_{\mathit{comp}}^{\overline{N}}$ is the set of the compiled not neutral operator; the description of how these operators are compiled is shown in Definition \ref{compiledOcondeff}.

    % \item  $c_{\overline{\mathcal{N}}}$ \textbf{TODO}

    % \item the compiled operators $o_{chained}$ of the non-neutral operators are defined as:
    % $$
    % o_{chained} =\bigcup\limits_{o \in \mathit{NN}(\Pi)} chain(o, SI(o))
    % $$
    % where $SI(o)$ an arbitrary ordering of affected preferences by $o$ and $chain(o, SI(o))$ is a set of new compiled operators defined further down in following section \ref{comp_sect};

    % \item $c_{tv}(o)$ is the cost of an operator $o \not \in N(\Pi)$, or equivalently that $o \in O_{chained} \cup O_{violation}$ that we define after describing how exactly these sets are formed.


    % VERSIONE FINALE
    \item $\textit{comp}(o , \mathscr{P})$, which definition is provided in Definition \ref{compiledOcondeff};


% $\textit{comp}(o , \mathscr{P}) = \\
%     \left\{
%     \begin{array}{ll}
%         \langle \textit{Pre}(o) \cup \{\text{\it normal-mode}\}, \textit{Eff}(o) \rangle & \mbox{if } o \in O_{{\it neutral}}\\
%         \langle \textit{Pre}(o) \cup \{\text{\it normal-mode}\}, \textit{Eff}(o) \cup 
%         {\displaystyle \bigcup_{P_i \in P_{\textit{affected}}(o)}{\mathcal{W}(o, P_i)}} \rangle & \mbox{if } o \in O - O_{{\it neutral}}\\
%     \end{array}
% \right.
% $



\end{itemize}

% The specific definition of $\mathcal{W}(o, P_i)$ for an operator that is not neutral for $\mathscr{P}$
% depends on the type of prefrence $P_i$ as specified in the following by Definition \ref{when_always}--\ref{when_atmostonce}

% \text{\it forgo} & \text{\it collect}
\paragraph{{Forgo} and {Collect} Actions}
For each preference $P$ the transformation of $\Pi$ into $\Pi'$ adds a dummy hard goal $P'$ to $\Pi'$ which can be achieved by two ways: with action $\text{\it collect}(P)$, that has cost 0 but requires $P$ to be satisfied (i.e. $\pviol$ is false in the goal state for all kinds of preferences except for sometime), or with action $\text{\it forgo}(P)$, that has cost equal to the utility of $P$ and can be performed only if $P$ is false ($\pviol$ is true in the goal state). For each preference, exactly one of $\text{\it collect}(P)$ and $\text{\it forgo}(P)$ appears in the plan. 

\paragraph{Operator Compilation Function}
The function $ \textit{comp}(o , \mathscr{P}) $ which transforms an original operator $o$
into the equivalent compiled one is splitted in two parts. If the operator is neutral 
($o \in O_{{\it neutral}}$) then the function just extends $\textit{Pre}(o)$
with the predicate $\text{\it normal-mode}$ in order to scorporate the execution of the domain operators and
the evaluationof $\text{\it forgo}$ and $\text{\it collect}$ actions at the end of the planning.

If the operator $o$ is not neutral ($o \in O - O_{{\it neutral}}$) where $|P_{\textit{affected}}(o)| = n$,
then the compilation function $\textit{comp}(o , \mathscr{P})$ extends its effects,
for each affected preference $P_i$, by adding a set of conditional effects denoted with 
$ \mathcal{W}(o, P_i) $ whose definition depends by $o$,
the class of preference $P_i$ belongs to and the way how $o$
interacts with $P_i$. In Subsections \ref{compilation_always}--\ref{subsect:amo_comp}
we will detail the definition of $ \mathcal{W}(o, P_i) $ for each class of preference.

But we want a compiling problem which belongs to {\strips+} class and so
we have to compile away the conditional effects (see Section \ref{subsection:conditional-effect}).

% COPIA FORMA GENERICA conditional effects capitolo introduttivo!
%     It will be explained in the next section because $n$, the number of affected preferences, can differ to $n'$ the number of additional conditional effects.
% }, where $c_i$ is a formula that, when satisfied in the state where $o$ is applied, makes the formula $e_i$ an effect of $o$. 
% If the operator is not neutral ($o \in O - O_{{\it neutral}}$), 
% which means that $o$ affects $n$ preferences $P_{\textit{affected}}(o) = \{ P_1, ..., P_n\}$, then
% it can be compiled by adding a number of conditional effects \textit{(when $c_i(o, P_i)$\, $e_i(o, P_i)$)}, for $i = 1\dots n'$\footnote{
%     It will be explained in the next section because $n$, the number of affected preferences, can differ to $n'$ the number of additional conditional effects.
% }, where $c_i$ is a formula that, when satisfied in the state where $o$ is applied, makes the formula $e_i$ an effect of $o$. 
% % The exact Definition of $c_i$ and $e_i$
% % depend on the class of preference $P_i$ belongs to which
% % are specified below.

% In detail the compilation function $\textit{comp}(o , \mathscr{P})$ extends,
% for each affected preference by the execution of $o$, 
% with a set of conditional effects $ \mathcal{W}(o, P_i) $
% whose definition depends on $o$, the type of affected preference and the way
% in which $o$ interacts with the preference. In the following subsections we will detail,
% for each type of preference, the definition of function $ \mathcal{W}(o, P_i) $. 

% \textit{comp}(o , P)

% ESTENSIONE STATO INIZIALE
\paragraph{Extension of the initial state}
Note that the original initial state $I$ is extended to $I'$ with the set of literals $V_{\mathcal{ST}}$, which contains the literal $P_i\textit{-violated}$ for each sometime preference of the problem. The literal $P_i\textit{-violated}$ states that the related preference $P_i$ is (temporarily) violated in $I$ until a support operator for $P$ is applied.

Furthermore the original initial state $I$ is extended with the set of literals $S_{\mathcal{AO}}$
which contains, for each at-most-once preference $P_i = {\sf AO}_{\phi_i}$ of the problem such that
$I \models \phi_i$, the literal $\textit{seen-}P_i$. This additional fluent
$\textit{seen-}P_i$ states that the formula $\phi_i$, involved in $P_i$, is satisfied in $I$. This precaution is necessary to correctly capture any possible violations of at-most-once preferences.

% We now present the transformation of not neutral operators that affects a set of preferences i.e. the operators in
% $O - O_{{neutral}}$.

% and the will specified in the following section.
% Condition $c_i$ can be derived by regression of $P_i$ over $o$ (it is analogous to precondition $l_1 \wedge \ldots \wedge l_q$ in the $\overline{o}$-operators of Definition \ref{def:operatorcomp}).
% There are two alternatives to do this,
% one leads to an exponential blow up of the operators 
% {(in our case $O(2^n)$ for each operator affecting $n$ preferences)} \cite{gazen97:ecp} and the other generates a linear number of operators but increase
% polynomially the plan lenght \cite{compilationnebel}.\\
% \redtext{\textbf{NOTA}: non e' proprio $O(2^n)$, sarebbe piu' preciso $O(2^m)$ dove $m$ dipende dal numero e dal tipo di preferenze influenzate da $o$.}
% We use a variant of the linear compilation optimized in attempt to make it more efficient for the planner 
% Our compilation scheme is based on the use of conditional effects but since an original operator can affects many preferences (e.g., $k=20$), we used a slight different of what proposed by Nebel to transform conditional effects generating only a linear number of operators ($2k$ instead of $2^k$). We describe in detail our compilation of conditional effects in section [...].
% In definition \ref{compiledOcondeff} we describe how a not neutral operator $o$ is compiled. 

% --------------------------------------------------------
% DEFINIZIONE OPERATORE COMPILATO CON EFFETTI CONDIZIONALI
% --------------------------------------------------------
\begin{mydef} \label{compiledOcondeff}
Given an operator $o$ the corresponding compiled operator is defined using the following function:\\

% whose execution affects the set of preferences indicated as $\affectedd = \{P_1, P_2, ..., P_N \}$, the corresponding compiled operator is defined as

% $\textit{comp}(o , \mathscr{P}) = \\
%     \left\{
%     \begin{array}{ll}
%         \langle \textit{Pre}(o) \cup \{\text{\it normal-mode}\}, \textit{Eff}(o) \rangle & \mbox{if } o \in O_{{\textit neutral}}\\
%         \langle \textit{Pre}(o) \cup \{\text{\it normal-mode}\}, \textit{Eff}(o) \cup 
%         {\displaystyle \bigcup_{P_i \in P_{\textit{affected}}(o)}{\mathcal{W}(o, P_i)}} \rangle & \mbox{if } o \in O - O_{{\textit neutral}}\\
%     \end{array}
% \right.
% $

$ 
{\textit{Pre}}(o') = {\textit{Pre}}(o) \cup \{\text{\it normal-mode}\}
$

$
{\textit{Eff}}(o') = {\textit{Eff}}(o) \cup 
{\displaystyle 
\bigcup_{P_i \in P_{\textit{affected}}(o)}{ \mathcal{W}(o, P_i)}
}
$

\noindent where
$\mathcal{W}(o, P_i)$ is the set of conditional effects concerning the affected preference $P_i$. If $o \in O_\text{neutral}$ then $\textit{Eff}(o') = {\textit{Eff}(o)}$.

\end{mydef}

% Hereafter, given an operator $o$, we will denote with $o'$ the compiled operator, with $s$ the state where $o'$ is applied and with $s'$ the state resulting from the application of $o'$.




% -------------------
% COMPILAZIONE ALWAYS
% -------------------
\subsection{Compilation of an Always Preference} \label{compilation_always}
We now present the transformation of operators that threaten or violate a preference in class $\mathcal{A}$. 

Before defining the extending effects used to compile an operator $o$ which affects an always preference, we introduce some useful notation in order to simplify the formalisation. For an operator $o$ and a preference clause $\phi_i$:
\begin{itemize}
\item  $\mathit{NA}(o)_{\phi_i}= \{l_j \in L(\phi_i) \mid \neg{\l_j} \in ({\textit{Eff}}(o)^+ \cup {\textit{Eff}}(o)^-) \}$ is the set of literals in $L(\phi_i)$ falsified by the effects of o; 
\item  $AA(o)_{\phi_i} = L(\phi_i) \setminus \mathit{NA}(o)_{\phi_i}$ is the set of literals in $L(\phi_i)$ \emph{not} falsified by the effects of o;
\item  $\overline{AA}(o)_{\phi_i}$ is the literal-complement set of $AA(o)_{\phi_i}$.
\end{itemize}


% \textbf{alternativa?} \texttt{true}

\begin{mydef} \label{when_always}
Given an always preference $P = \sf{A}_{\phi}$ and an operator $o$ which affects $P$,
the conditional effect set $\mathcal{W}(o, P)$ in the compiled version $o'$ of $o$
(according to Definition \ref{compiledOcondeff}) is defined as:
$$\mathcal{W}(o, P) = 
\begin{cases}

\{\textit{when} \; (\textit{cond}(o, P)) \; (\pviol)\}\\
\;\;\;\;\;\;\text{if}\;o\;\text{is a threat for }P\\
\{\textit{when} \; (\textsc{true}) \; (\pviol)\}\\
\;\;\;\;\;\;\text{if}\;o\;\text{is a violation for }P\\


\end{cases}$$
\\where:

\begin{itemize}
\item  $cond(o, P) = \;\;\;\bigvee_{\phi_i\,\in\,T_{\mathcal{A}}(o, P)}(l_1 \wedge \ldots \wedge l_q), \; \{l_1, \ldots, l_q\}=\overline{AA}(o)_{\phi_i}$.\\

\end{itemize}
\end{mydef}

For each preference $P \in P_\mathcal{A}$ affected by an operator $o$,
the compiled operator $o'$ contains a conditional effect whose effect $\pviol$ and conditions depends on how
$o$ affects $P$. If $o$ is a violation for $P$, then the condition is always true (i.e. it is the special literal
{\truettt} that holds in every state). Instead, if $o$ is a \textit{threat} to $P$, the effects are extended
with a conditional described below.

If $o$ is a threat for $P$ the condition ($\textit{cond}(o, P)$) requires
$\overline{AA}(o)_{\phi_i}$ to hold 
% in the state where $o'$ is applied 
for at least one 
$\phi_{i} \in T_{\mathcal{A}}(o, P)$,
which implies that $\phi$ is false in the state generated by $o'$.

% If this happens, then the predicate $P\text{-violated}$ in $\textit{Eff}(o, P)$ is made true and the preference $P$ will be violated in $s'$.
% After the \emph{end} action is applied, $P{\textit{-}violated}$ serves as a precondition of the operator ${\text{\it forgo}}(P)$ that has cost equal to the utility of $P$. \\



% ---------------------
% COMPILAZIONE SOMETIME
% ---------------------

\subsection{Compilation of a Sometime Preference}
We now present the transformation of an operator that is a potential supports for a preference $P$ in class $\mathsf{ST}$. 
\begin{mydef} \label{when_sometime}

Given a sometime preference $P = \sf{ST}_{\phi}$ and an operator $o$ which potentially supports $P$,
the conditional effect set $\mathcal{W}(o, P)$ in the compiled version $o'$ of $o$
(according to Definition \ref{compiledOcondeff}) is defined as:

$$
\mathcal{W}(o, P) = \{
\text{when} (\text{cond}_{\mathcal{S}}(o, P))\text{ } (\lnot \pviol )\}$$
where:
\begin{itemize}
\item  $\textit{cond}_{\mathcal{S}}(o, P) = 
\{{\phi_i}{ \; \mid \; \phi_i \in \overline{C}_{\phi}(o)}\}
$.

% \item  $\textit{eff}_{\mathcal{S}}(o, P) = $ 
\end{itemize}


\end{mydef}

% SCRITTURA IN CORSO
As specified above, the general compilation scheme introduces for each problem preference $P = \sf{ST}_{\phi}$
$P \in \mathscr{P}_{\mathcal{ST}}$ a predicate $\pviol$ in the compiled initial state $I'$
if $\phi$ does not hold in the original problem initial state
(otherwise the preference is considered satisfied and therefore not compiled).

This is necessary because, according to the semantics of the preference in class $\mathsf{ST}$,
a plan $\pi'$ in the compiled problem satisfies $\sf{ST}_{\phi}$ iif $\phi$ is
true at least once in the state trajectory of $\pi'$.

These $\pviol$ predicates are falsified
by the operators that could make $\phi$ true in the state $s'$
resulting from this application. Such operators are potential support that we have
defined in Section \ref{subsection:sometime_operators}.

An operator $o$ could make a formula $\phi$ true 
when there are some clauses of $\phi$ that will surely be true in the state $s'$ resulting
from the application of $o$. So, if all $ { \phi_i \in \overline{C}_{\phi}(o)} $ hold in $s$ 
(where $\overline{C}_{\phi}(o)$ is the set of clauses of $\phi$ that
are not surely true in $s'$ - see Definition \ref{def:trueclauses}),
i.e. $\textit{cond}_{}(o, P)$ holds in $s$,
then $\phi$ will be true in $s'$, and $\pviol$ should be falsified
by the effects of $o'$.

In order to satisfy a given preference $\sf{ST}_{\phi}$, which is temporarily violated in $I'$,
a potential support $o$ for $P$ has to be inserted in the plan
and applied in a state where ${(cond}_{\mathcal{S}}(o, P))$ holds,
making an actual support for $P$.
Otherwise, operator $o$ 
behaves as a neutral operator for $P$, leaving the preference violated in $s'$.





% ----------------------------
% COMPILAZIONE SOMETIME-BEFORE
% ----------------------------
\subsection{Compilation of a Sometime-before Preference} \label{subsection:sometime_before_compilation}
We now present the transformation of operators that potentially support or threat a preference in $\mathcal{SB}$. 

\begin{mydef} \label{when_sometimebefore}
Given a sometime-before preference $P = \sf{SB}_{\phi, \psi}$ and an operator $o$ which affects $P$,
the conditional effect set $\mathcal{W}(o, P)$ in the compiled version $o'$ of $o$
(according to Definition \ref{compiledOcondeff}) is defined as:
$$
\mathcal{W}(o, P) = 
\begin{cases}
\{\textit{when} \; (\textit{cond}_{\mathcal{S}}(o, P)) \; (\textit{seen-}\psi)\} \;
\\\;\;\;\;\;\;\text{if}\;o\;\text{is a potential support for }P\\
\{\textit{when} \; (\textit{cond}_{\mathcal{T}}(o, P)) \; (\pviol)\} \;
\\\;\;\;\;\;\;\text{if}\;o\;\text{is a threat for }P\\
\{
\textit{when} \; (\textit{cond}_{\mathcal{S}}(o, P)) \; (\textit{seen-}\psi),\\
\ \ \textit{when} \; (\textit{cond}_{\mathcal{S}}(o, P)) \; (\pviol)
\} 
\\\;\;\;\;\;\;\text{if}\;o\;\text{is a both a threat and support for }P\\
\end{cases}$$
\\where:
\begin{itemize}
\item  $\textit{cond}_{\mathcal{S}}(o, P) = 
% \{\bigcup_{\psi_i \in \overline{C}_{\psi}(o)} \psi_i\} = \{ { \psi_i \in \overline{C}_{\psi}(o)} \}
\{
\psi_i \;\mid\; \psi_i \in \overline{C}_{\psi}(o)    
\}
$

% \item  $\textit{eff}_{\mathcal{S}}(o, P) = \textit{seen-}\psi$ 
% \end{itemize}
% and:
% \begin{itemize}
\item  $\textit{cond}_{\mathcal{T}}(o, P) = 
\{\lnot \textit{seen-}\psi\} \cup \{
\phi_i \;\mid\; \phi_i \in \overline{C}_{\phi}(o) \}    
% \{\lnot \textit{seen-}\psi\} \cup \{ { \phi_i \in \overline{C}_{\phi}(o)} \}
$.
% \item  $\textit{eff}_{\mathcal{T}}(o, P) = \pviol$ 
\end{itemize}

% -------
% RIMOSSO
% -------
% and:
% \begin{align*}
% W_{\mathcal{ST}}(o, & P) = \{  \\
% & \text{when (cond}_{\mathcal{S}}(o, P))\text{ (eff}_{\mathcal{S}}(o, P))\\
% & \text{when }\text{ (cond}_{\mathcal{T}}(o, P))) \text{ (eff}_{\mathcal{T}}(o, P))
% \}
% \end{align*}

\end{mydef}


The definition of the effects used to extend the effect of an operator $o$ which affects a sometime-before preference $P$ in 
Definition \ref{when_sometimebefore} depends on the class of operators which $o$ belongs. We have to distinguish 
if $o$ is a potential support, a threat or both for $P$. Remember that the semantics of the 
preferences belonging to $\mathcal{SB}$
requires that a preference $\sf{SB}_{\phi, \psi}$ is satisfied by a state-trajectory if,
whenever $\psi$ becomes true in a state then $\phi$ must have become true in a previous state.

If $o$ is a {potential support} for $P$ then this operator can behave in two different
ways when it is executed.
Recalling Definition \ref{def:becometrue}, an operator $o$ could make true a formula $\phi$
when there exists some clauses of $\phi$ that will surely be true in the state resulting
from the application of $o$. So, if all $ { \psi_i \in \overline{C}_{\psi}(o)} $ hold in $s$ 
(where $\overline{C}_{\psi}(o)$ is the set of clauses of $\psi$ that
are not surely true in $s'$ - see Definition \ref{def:trueclauses})
, i.e. $\textit{cond}_{\mathcal{S}}(o, P)$ holds in $s$,
then $\psi$ will be true in $s'$ and then we have to keep track of this fact by
making the predicate $\textit{seen-}\psi$ true in the effects $\textit{Eff}_{\mathcal{T}}(o, P)$ of $o'$.

% RIDONDANTE
% In order to check if $\psi$ will be true in the resulting state $s'$, we have to check
% the preconditions of the compiled operator to verify that the clauses, which do not belong to
% the set of clauses $C_{\psi}(o)$ certainly true in $s'$, are already true in $s$. This
% condition is specified in $\textit{cond}_{\mathcal{S}}(o, P)$ with the expression
% $\{ { \psi_i \in \overline{C}_{\psi}(o)} \}$. If this condition holds then the effect 
% $\textit{eff}_{\mathcal{S}}(o, P) = \{\textit{seen-}\psi\}$ is made true is $s'$.

If $o$ is a threat for $P$ its compilation is similar. A threat for $\mathsf{SB}_{\phi, \psi}$
behaves as violation 
in the case that the operator makes $\phi$ true in $s'$
% specifing in $\textit{cond}_{\mathcal{T}}(o, P)$ the expression $\{ { \phi_i \in \overline{C1}_{\phi}(o)} \}$)
(i.e., when all $\phi_i \in \overline{C}_{\phi}(o)$ holds in $s$,
which is the condition of $\textit{cond}_{\mathcal{T}(o, P)}$), and
that the $\psi$ has never been made true in the states preceeding $s$. If 
both these
conditions, specified in $\textit{cond}_{\mathcal{T}}(o, P)$, hold in $s$ then predicate
$P{\it-violated}$ is included in the effects of $o'$.

We have also to consider the case in which an operator $o$ is both a threat and a support for $P$. In this 
case $o$ can behaves in the following ways: making $\phi$ true in $s'$, making $\psi$ true in $s'$
and making both $\phi$ and $\psi$ true in $s'$. In order to handle these situations,
the compiled operator $o'$ contains both conditional effects of $o$ as threat of $P$ and
as support of $P$. Note that this correctly captures the violation of $P$ determined by $\phi$
and $\psi$ becoming simultaneously true by execution of $o$.


% DA FARE PROX
% \subsection{Compilation of a Sometime-After Preference}



% -------------------------
% COMPILAZIONE AT-MOST-ONCE
% -------------------------
\subsection{Compilation of an At-Most-Once Preference} \label{subsect:amo_comp}

We now present the transformation of an operator that threats a preference in $\mathcal{AO}$. 

\begin{mydef} \label{when_atmostonce}

Given an at-most-once preference $P = \sf{AO}_{\phi}$ and an operator $o$ which affects $P$,
the conditional effect set $\mathcal{W}(o, P)$ in the compiled version $o'$ of $o$
(according to Definition \ref{compiledOcondeff}) is defined as:


\begin{align*}
\mathcal{W}(o,&P) = \{  \\
& \text{when } (\text{cond}_{{N}}(o, P))\text{ }(\textit{seen-}\phi))\\
& \text{when } (\text{cond}_{{T}}(o, P)))\text{ }(\pviol )
\}
\end{align*}
\\
where:

\begin{itemize}

\item  $\text{cond}_{{N}}(o, P) = \{\lnot \textit{seen-}\phi\} \cup \{ { \phi_i \mid \phi_i \in \overline{C}_{\phi}(o)} \}$

% \item  $\text{eff}_{\mathcal{N}}(o, P) =  $

\item  
% \begin{align*}
$
\text{cond}_{{T}}(o, P) = \{\textit{seen-}\phi\} \cup \{ { \phi_i \mid \phi_i \in \overline{C}_{\phi}(o)} \} \cup $

$
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;
\{ \bigvee_{ \phi_i \in {C}_{\phi}(o)} (\lnot l_1 \land ... \land \lnot l_q) \mid  \{l_1, ..., l_q\} = {L}(\phi_i)\}
$.
% \end{align*}

% \item  $\text{eff}_{\mathcal{V}}(o, P) = \pviol $

% \item  $\mathcal{R}_{\phi}(o, P) = $

\end{itemize}
\end{mydef}

The semantic of a preference in class $\mathcal{AO}$ (Figure \ref{fig:semantics}) requires that a preference
$P = \sf{AO}_{\phi}$ is satisfied by the state-trajectory generated by a 
plan $\pi$ if $\phi$ \textit{becomes} true in a state $s'$ at most once during the execution of the plan. A
formula $\phi$ becomes true in a state $s'$ due to the execution of an operator $o$ applied in a state $s$ iif 
$s \models \lnot \phi$ and $s' \models \phi$.

If $\phi$ holds in the problem initial state $I$, then it is required that
either it stay true until the end of the plan. or it becomes false at some
successor state of $I$ in the state-trajectory generated by the plan and it then stays
always false.

% If $I \models \lnot \phi$ then an operator 
% $o$ can make $\phi$ true in the resulting state for the first time in the state-trajectory,
% leaving the preference satisfied. In this case $o$ behaves as a neutral operator for $P$
% but if in a succeeding state another operator $o'$ make $\phi$ false then $o$ could behave as
% a violation for $P$ if it applied again. So, an operator that can make true $\phi$ is
% generally considered as a threat to $P$ because,
% as we have described, under certain conditions it could act as a violation.
If an operator $o$ can make $\phi$ true for the first time in the plan  trajectory
of a plan, then it behaves as a neutral operator for $P$. On the other hand, if $o$
can make $\phi$ true after having been true and become false in past states of the trajectory,
then $o$ behaves as a threat for $P$.

In order to correctly capture the possible violation of $P$,
the set of effects extending $o$ $\mathcal{W}(o,P)$ has two conditional effects. The
first one is a neutral effect that is used to catch the behavior
of $o$ as neutral operator for $P$. Condition 
$\text{cond}_{{N}}(o, P)$ requires that $\phi$ has been never changed truth value
from true to false in preceeding state using the negated predicate
$\lnot \textit{seen-}\phi$ and that $\phi$
will be true in $s'$ using the condition $\{ { \phi_i \mid \phi_i \in \overline{C}_{\phi}(o)} \}$
similarly to what done in the compilation schemes previously presented. If the conditions 
specified in $\text{cond}_{{N}}(o, P)$ hold in 
the state $s$ where $o$ is applied,
then we take into account that $\phi$ becomes true in $s'$ for the first time stating in the effects of 
the neutral conditional effect predicate $\textit{seen-}\phi$.

% \bluetext{
    The second conditional effect is a violating effect that 
    is used to catch the the behavior of of $o$ as a threat for $P$. This
    happens when the following conditions, specified in $\text{cond}_{{T}(o, P)}$ hold:
    (specified in $\text{cond}_{\mathcal{V}(o, P)}$):
    \begin{itemize}
        \item  $\phi$ has alreay been made true in a state preceding s; this is
        expressed by the predicate $\textit{seen-}\phi$ when $o$ is applied;
        \item  $\phi$ is made true in the resulting state $s'$; this is guaranteed by the conditions
        $\{ { \phi_i \mid \phi_i \in \overline{C}_{\phi}(o)} \}$;
        \item  $\phi$ is false in the state $s$ where $o$ is applied; this is specified by requiring that at least
        a clause in $C_{\phi}(o)$ is false in $s$. 
    \end{itemize}
% }


% \bluetext{
    If all these conditions hold in the state where $o$ is applied then $P$ will be violated in the state resulting from
    the application of $o$.
% }





% ----------------------------
% SEZIONE EFFETTI CONDIZIONALI
% ----------------------------
\subsection{Compilation of Conditional Effects} \label{subsection:conditional-effect}

As described before, given an operator $o$ of a {\stripspp} problem which affects a set of
$n$ preferences, the corresponding compiled operator should have in its effects
a set \textit{$\{$(when $c_i$\, $e_i$) $|$ $i = 1 \dots m\}$} of $m \leq 2n$ conditional effects,
which are built using the compilation schema described in the previous subsections.

In order to keep the compiled problem in the {\strips+} class, the conditional effects of $o'$ 
should be compiled away by replacing $o'$ with an equivalent set of operators without 
conditional effects. In the literature, there are two main 
general methods for generating this equivalent set of unconditional operators.

% PRIMO METODO GAZEN AND KNOBLOCK
The first method, introduced by Gazen and Knoblock \cite{gazen97:ecp}, works by recursively splitting $o'$
for each conditional effect \textit{(when $c_i$\, $e_i$)}
into a couple of new operator, $o''$ and $\overline{o}''$, such that:
\begin{align*}
& pre(o'') = pre(o') \cup \{c_i\} \\
& \textit{eff}(o'') = \textit{eff}(o') \cup {e_i} \\
& pre(\overline{o}'') = pre(o') \cup \{\lnot c_i\} \\
& \textit{eff}(\overline{o}'') = \textit{eff}(o').
\end{align*}

This method is not practicable because it leads to an exponential blow up of operators 
(in our case $O(2^m)$ for each operator affecting $n$ preferences), but
the compiled plan preserves exactly the length of the original plan.

% METODO NEBEL
The second method, proposed by Nebel \cite[see proof of Theorem 20]{nebel2000compilability}, generates a polynomial number of new operators, but it increases polynomially the plan length. 
The main idea is to simulate the parallel behaviour of the conditional effects of an operator by a replacing it with an equivalent sequence of unconditional operators.
For each operator $o'$ with $m$ conditional effects, Nebel's schema introduces $m$ pairs of new operators, that separately evaluate the condition $c_i$ of each conditional effect \textit{(when $c_i$\, $e_i$)} and possibly ``activates'' the corresponding effect $e_i$. One of the operators in the pair for \textit{(when $c_i$\, $e_i$)} contains precondition $c_i$ and an effect indicating that $e_i$ is activated, while the other, that is mutex with the first, contains precondition $\neg c_i$ and does not activate $e_i$. In order to avoid possible (positive or negative) interference in the sequentialisation of the conditional effects through the new operators (e.g.,  if
\textit{(when $c_1$\, $e_1$)}  and \textit{(when $c_2$\, $e_2$)} are conditional effects of $o'$ and $e_1 \models c_2$), the activated effects in the operator sequence are not made immediately true, but they are deferred to the end of the sequence (i.e., after all conditional effects conditions have been evaluated). This is done by using an additional set of operators, called ``copying operators'' in \cite{nebel2000compilability}, which copy the activated effects to the state description after all operators in the sequence have been executed (For more details the reader is referred to \cite{nebel2000compilability}.)

% \textbf{NOTA SUI COSTI DELLA COMPILAZIONE DA INSERIRE QUI}

% OTTIMIZZAZIONE
In order to deal with the conditional effects generated by our compilation of {\pddlIII} preferences, we
have implemented and used Nebel's compilation method because a compiled operator
can in principle contain
many conditional effects, which makes Gazen and Knoblock's method impractical given 
its exponential complexity. Moreover, the conditional effects needed to compile {\pddlIII} preference have a particular structure that 
allows us to simplify and optimize Nebel's general method. In particular we would like avoid the so-called ``copying operators'' operators
maintaining the semantics of conditional effects (which requires that all conditions are 
evaluated at the same time). First of all, note that the conditional effects 
that refer to different affected preferences can not interfere with each other because
they involve different fluents.

After that we have to pay attention to those class of preferences whose compilation
introduces more than one conditional effect,
because, affecting the same fluents, they can generate interference. In our previous discussion there are two 
classes of preferences having this feature, i.e.
sometime-before and at-most-once preferences (see Subsections \ref{subsection:sometime_before_compilation} and \ref{subsect:amo_comp}). In the first case, the set of problematic conditional effects 
refers to those operators which threats and supports a sometime-before preference at the same time, in the second case to
those operators that threats at-most-once preference.
% By Definitions \ref{when_sometimebefore} and \ref{when_atmostonce}, the set of conditional effects that can interfere when sequentialised are those of the compiled operator $o'$ of an operator affecting an at-most-once and sometime-before preferences. 

Concerning at-most-once preferences, an interference could arise through
the predicate $seen$-$\phi$, that is both an effect of the first conditional 
effect and a condition of the second. However, the conditional effect interference disappears, if in the compilation, the pair of unconditional operators
for $(\textit{when } (\textit{cond}_{\mathcal{V}}(o, P))\textit{ }(P{\it-violated}))$
is constrained to be ordered before the other pair. If we evaluated these conditional effects without following this order, 
the execution would be equivalent to not evaluating all conditional effects
of the same operator simultaneously,
thus risking to recognize a violation even if this does not happen. Indeed,
given a preference $P = \sf{AO}_{\phi}$ 
and a threat operator $o$, if $\phi$ becomes true for the first time in $s'$
after the application of $o$,
an we check the condition $\textit{cond}_{\mathcal{N}}(o, P)$
before $\textit{cond}_{\mathcal{V}}(o, P)$, then we could detect a violation
that may not have happened.

We can expose similar considerations for sometime-before preferences. In this case,
if we do not check the condition $\textit{cond}_{\mathcal{V}}(o, P)$
before $\textit{cond}_{\mathcal{T}}(o, P)$,
we risk to not correctly identifying a possible violation if
$\phi$ and $\psi$ become true at the same time.

Consequently, starting from the previous observations which show that it is possible to eliminate any interferences within our context,
Nebel's copying operators are not needed in the compilation of our conditional effects; furthermore, in the compiled problem, we can force an arbitrary total order of the unconditional operator pairs, paying attention that the ordering constraints dealing with the potential interference between the conditional effects arising from at-most-once and sometime-before preferences are satisfied.

These changes to Nebel's schema simplify it, and have some beneficial consequences: (1) the compiled problem has smaller size in terms of number of operators, and (2) the search effort of a planner can be reduced because the solution plans are shorter without the coping actions,\footnote{
Given a plan $\pi$ for a problem with conditional effects and 
 the corresponding plan $\pi'$ for the compiled problem, we have $|\pi'| \leq |\pi| * m$ while with Nebel's general method this bound is $|\pi'| \leq |\pi| * (3 + m)$ \cite{nebel2000compilability}.
}
and because the sequence of the unconditional operators can be explicit in the compiled problem, 
while with the original compilation it is built at search time by the planner. 

Since our compilation of conditional effects can be easily derived from Nebel's method, instead of giving all the formal details of the translation, we illustrate the compilation with an example, referring the reader to \cite{nebel2000compilability} for a formal description. Moreover, in \cite{ceriani2015planning} we give a detailed description of the final (without conditional effects) compiled problem for any \strips+ problem with always preferences.

\subsection{Compilation Example} \

Consider the following operator $o = \langle \textit{Pre}(o), \textit{Eff}(o) \rangle =\langle \O, \{a, \lnot c\} \rangle$
with cost $\kappa$, which affects two preferences,
$P1 \in \mathcal{A}$ and $P2 \in \mathcal{AO}$, where
$P1 = (\textit{always}\text{ }\phi^{P1})$ and $P2 = ({\textit{at-most-once}}\text{ }\phi^{P2})$
and $\phi^{P1} = \phi_{1}^{P1} \land \phi_{2}^{P1} = (a \lor b) \land (c \lor d) $
and $\phi_{1}^{P2} \land \phi_{2}^{P2} =(\lnot c \lor e) \land (d \lor f)$.

\paragraph{Compilation of P1} 
Using the conditions specified in Definition \ref{threat_formula}, we can say that
$o$ is a \textit{threat} for $P1$ because there exists a clause $\phi_{2}^{P1} = (c \lor d$) of $\phi^{P1}$ such that:

\begin{enumerate}

\item it contains at least a literal, which is $c$,
that is negated by the effects of $o$:
$|\overline{L}(\phi_{1}^{P2}) \cap Z(o)| = |\overline{L}(c \lor d) \cap Z(o)| = |\{\lnot c, \lnot d\} \cap \{a, \lnot c\}| = |\{\lnot c\}| > 0$;

\item the other literal in the clause, which is $d$, is not made true in the resulting
state from the application of $o$:
$|{L}(\phi_{1}^{P2}) \cap Z(o)| = |L(c \lor d) \cap Z(o)| = |\{c, d\} \cap \{a, \lnot c\}| = 0$;

\item (the clause) is not already negated in the state where $o$ is applied because its
negation is not implied by the precondition of $o$:
$\overline{L}(c \lor d) = \{\lnot c, \lnot d\} \not \subseteq \textit{Pre}(o)$.

\end{enumerate}

Operator $o$ threatens only one clause of $P1$, i.e. $T_{\mathcal{A}}(o, P1) = \{ \phi_2^{P1}\} = \{ c \lor d\}$
(remember that $T_{\mathcal{A}}(o, P1)$ is the set of threatened clauses of $P1$ by $o$),
which could be falsified if $o$ is applied.

Using the preliminary definitions provided in Section \ref{compilation_always},
we define the set $\overline{AA}(o)_{\phi_{2}^{P1}} = \{\lnot d\}$ which contain those literal of the threatened clause $\phi_2^{P1}$ that are not falsified by $o$.
% that we have to check when $o$ is applied in order to capture 
% the possibile violation of $P1$ in the resulting state.

In this case $o$, denying the literal $c$, threatens a single clause 
of $P1$, i.e. $\phi_2^{P1} = c \lor d$, and
therefore we have to check 
the literal $d$ is true in the precondition
to capture the possible violation.

Starting from these considerations and using Definition
\ref{when_always} we define the conditional effect $\mathcal{W}(o, P1)$
to add to $\textit{Eff}(o)$:
\begin{gather*}
\mathcal{W}(o, P1) = \{\textit{when} \; (\lnot d) \; (P1\textit{-violated})\}.
\end{gather*}

\paragraph{Compilation of P2} 
According to Definition \ref{threat_amo}, $o$ is also a \textit{threat} for $P2$ because it
could make true $\phi^{P2}$ in the resulting state $s'$ from
its application; indeed there exists a clause of $\phi^{P2}$, i.e. $\phi_{1}^{P2} = c \lor e$, such that:

\begin{itemize}

    \item it contains at least a literal, i.e. $\lnot c$, that will be surely true in $s'$:
        $|L(\phi_{1}^{P2}) \cap Z(o)| = |L(\lnot c \lor e) \cap Z(o)| = |\{\lnot c, e\} \cap \{a, \lnot c\}| = |\{\lnot c\}| > 0 $\\


    \item the other clause, which is $\phi_{2}^{P2} = d \lor f$, is not negated by the execution of $o$:
	$|\overline{L}(\phi_{2}^{P2}) \cap Z(o)| = |\overline{L}(d \lor f) \cap Z(o)| = |\{\lnot d, \lnot f\} \cap \{a, \lnot c\}| = 0 $

\end{itemize}

We denote with $C_{\phi^{P2}}(o) = \{\phi_{1}^{P2}\} = \{\lnot c \lor e\}$
the set of clauses of $\phi^{P2}$ that will be surely true in the resulting state $s'$
and with $\overline{C}_{\phi^{P2}}(o) = \{\phi_{2}^{P2}\} = \{d \lor f\}$
the set of clauses of $\psi$
the remaining ones. With
reference to Section \ref{subsect:amo_comp}, we define
the set of conditional effects related to the threatened at-most-once preference $P2$ as: \begin{align*}
\mathcal{W}(o,&P_2) = \{  \\
& \textit{when } (\textit{cond}_{{N}}(o, P_2))\text{ }(\textit{seen-}\phi^{P2}))\\
& \textit{when } (\textit{cond}_{{T}}(o, P_2)))\text{ }(\pviol )
\}
\end{align*}
\noindent where:

\begin{itemize}

\item  $\textit{cond}_{{N}}(o, P2) = \{\lnot \textit{seen-}\phi^{P2}\} \cup \{ { \phi_i \mid \phi_i \in \overline{C}_{\phi^{P2}}(o)} \} =$

$\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\,\,\,\,\,= \{\lnot \textit{seen-}\phi^{P2}\} \cup \{ \phi_2^{P2} \} = \{\lnot \textit{seen-}\phi^{P2}, d \lor f\}$
\item  
$
\textit{cond}_{{T}}(o, P2) = \{\textit{seen-}\phi^{P2}\} \cup \{ { \phi_i \mid \phi_i \in \overline{C}_{\phi^{P2}}(o)} \} \; \cup $

$
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\,\,\,
\{ \bigvee_{ \phi_i \in {C}_{\phi^{P2}}(o)} (\lnot l_1 \land ... \land \lnot l_q) \mid  \{l_1, ..., l_q\} = {L}(\phi_i)\} =
$

$
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\,\,\,
= \{ \textit{seen-}\phi^{P2} \} \cup \{ \phi^{P2}_2 \} 
\cup \{ \{ \lnot \lnot c \land \lnot e \} \mid \{\lnot c, e\} = L(\phi_1^{P2}) \}  =
$

$
\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\
= \{\textit{seen-}\phi^{P2}, d \lor f, c \land \lnot e\}
$
% \end{align*}

% \item  $\text{eff}_{\mathcal{V}}(o, P) = \pviol $

% \item  $\mathcal{R}_{\phi}(o, P) = $

\end{itemize}

The condition $\textit{cond}_{{N}}(o, P2)$ to activate the fluent $\textit{seen-}\phi^{P2}$
requires that the following condition hold in the state where $o$ is applied:

\begin{itemize}

\item the fluent $\textit{seen-}\phi^{P2}$ has to be false and so $\phi^{P2}$
never became true until then;

\item those clauses of $P2$ which are not affected by $o$,
i.e. $\phi^{P2}_2 = d \lor f$, have to be true
making sure that $\phi^{P2}$ becomes true. 
\end{itemize}

The condition $\textit{cond}_{{T}}(o, P_2))$
to activate the violation of $P2$ requires that:

\begin{itemize}

\item the fluent $\textit{seen-}\phi^{P2}$ has to be true and so $\phi^{P2}$
has already been made true;
\item those clauses of $P2$ which are not affected by $o$,
i.e. $\phi^{P2}_2 = d \lor f$, have to be true and those clauses of $P2$
which are affected by $o$, i.e. $\phi^{P2}_1 = \lnot c \lor e$, have to be false
ensuring that  $\phi^{P2}$ passes from false to true.

\end{itemize}


\paragraph {Compilation of conditional effects} According to what described above we can say that $o$ it is simultaneously a threat to both $P1$ and $P2$
and we denote the set of affected preference by $o$ as:
$$\affectedd = \{P1, P2\}\text{.}$$

The compiled operator $o'$ with conditional effects is defined, according to the compilation 
scheme provided in Section \ref{compstrips} and using Definition
\ref{compiledOcondeff}, as:\\

% \cup \{\text{\it normal-mode}, \lnot pause\}
$ 
{\textit{Pre}}(o') = {\textit{Pre}}(o) \cup \{\text{\it normal-mode}\}
$

$
{\textit{Eff}}(o') \,= {\textit{Eff}}(o) \cup \mathcal{W}(o, P1) \cup \mathcal{W}(o, P2) =
$

$
\;\;\;\;\;\;\;\;\;\;\,\,={\textit{Eff}}(o) \cup \{\textit{when } (\lnot b)\text{ }(P1\textit{-violated})\} \cup
$

$
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\,
\{\textit{when } (\lnot \textit{seen-}\phi^{P2} \land (d \lor f))\text{ }(\textit{seen-}\phi^{P2}),
$

$
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\,\,
\textit{when } (\textit{seen-}\phi^{P2} \land (d \lor f) \land (c \land \lnot e))\text{ }(P2\textit{-violated})\}.
$\\


% \begin{align*}
% { \textit{Pre}}(o') & = {\textit{Pre}}(o) \cup \{\text{\it normal-mode}\} \\
% { \textit{Eff}}(o') & = {\textit{Eff}}(o) \cup \mathcal{W}(o, P1) \cup \mathcal{W}(o, P2) = \\
%  ={\textit{Eff}}(o) \cup &\{\textit{when } (\lnot b)\text{ }(P1\textit{-violated})\} \cup \\
% & \{\textit{when } (\lnot \textit{seen-}\phi^{P2} \land (d \lor f))\text{ }(\textit{seen-}\phi^{P2}), \\
% & \textit{when } (\textit{seen-}\phi^{P2} \land (d \lor f) \land (c \land \lnot e))\text{ }(P2\textit{-violated})\}.
% \end{align*}


% We compile away the three conditional effects
% replacing $o'$ with the set of operators defined as:
% $$
% O_{\affectedd} = \{o_{P1}, \overline{o}_{P1}, o_{P2}^{{V}}, \overline{o}_{P2}^{{V}}, o_{P2}^{{N}}, \overline{o}_{P2}^{{N}}\}\text{,}
% $$

% which contains, for each conditional effect specified in $\textit{Eff}(o')$,
% a pair of mutually exclusive unconditional operators. In particular:
% \begin{itemize}

% \item  $o_{P1}$ and $\overline{o}_{P1}$ refer to the first conditional effect of $o'$ introduced 
% by the compilation to handle the possible violation of $P_1$;

% \item  $o_{P2}^{{V}}$ and $\overline{o}_{P2}^{{V}}$
% refer to the second conditional effect of $o'$ introduced 
% by the compilation to handle the possible violation of $P_2$;

% \item  $o_{P2}^{{N}}$, $\overline{o}_{P2}^{{N}}$
% refer to the third conditional effect of $o'$ introduced in order to correctly updating the predicate $\textit{seen-}\phi^{P2}$
% when $\phi^{P2}$ becomes true.
% \end{itemize}

% % Each operator $o$ such that $T(o) \not=\O$ is compiled into a set of new operators (one for each threatened preference). If $T(o)=\{A_1, \ldots, A_{m}\}$, $o$ is compiled into a set of $2m$ operators $O_{T(o)}=\{o_{A_1}, \overline{o}_{A_1}, \ldots, o_{A_{m}}, \overline{o}_{A_{m}}\}$ such that, in any state $s$ where $o$ can be applied violating a set of preferences in $T(o)$, the \emph{sequence $\omega_{T(o)}$ of $m$ operators in $O_{T(o)}$} defined as follows can be applied:
% % %\begin{equation}
% % %\label{omega}
% % %\[ \omega_{T(o)}=\langle o'_{A_1}, \ldots, o'_{A_{m}}\rangle \]
% % $\omega_{T(o)}=\langle o'_{A_1}, \ldots, o'_{A_{m}}\rangle$,
% % %\end{equation}
% % %luca7: uguale %luca1 
% % where 
% % $o'_{A_i}$= $\overline{o}_{A_i}$ if $o$ violates $A_i$ when applied in $s$, $o'_{A_i}$= $o_{A_i}$ if $o$ does not violate $A_i$ when applied
% % The six unconditional operators are defined in order to,
% % in any state $s$ where $o'$ can be applied affecting the $P_1$ and $P_2$, the
% % \emph{sequence $\omega_{\affectedd}$ of $3$ operators in $O_{\affectedd}$} defined as follows can be applied: 

% These six operators are defined in order to guaratee that, in any state $s$ where $o$
% can be applied affecting $P1$ and $P2$, the sequence $\omega_{\affectedd}$ of three operators
% in $O_{\affectedd}$, defined as follows, can be applied:

% $$
%     \omega_{\affectedd} = \langle o'_{P1}, {o'}_{P2}^{{V}}, {o'}_{P2}^{{N}} \rangle
% $$

% \noindent where:

% \begin{itemize}
%     \item  $o'_{P1} = \overline{o}_{P1}$ if $o$ violates $P1$ when applied in $s$, $o'_{P1} = o_{P1}$ if $o$ does not violate $P1$ when applied in $s$;

%     \item  ${{o}'}_{P2}^{{V}} = \overline{o}_{P2}^{{V}}$ if $o$ violates $P2$ when applied in $s$, ${o'}_{P2}^{{V}} = o_{P2}^{{V}}$ if $o$ does not violate $P2$ when applied in $s$;

%     \item  ${o'}_{P2}^{{N}} = \overline{o}_{P2}^{{N}} $ if $o$ makes $\psi$ true when applied in $s$, ${o'}_{P2}^{{N}} = {o}_{P2}^{{N}} $ if $o$ does not make $\psi$ true when applied in $s$.
% \end{itemize}

% These operators are defined in the following way:
% \begin{itemize}

% \item  operators $o_{P1}$ and $o_{P2}$:

% $\textit{Pre}({o}_{P1}) = \textit{Pre}(o) \cup \{\neg{\textit{pause}}, b \}$

% $\textit{Eff}({o}_{P1}) = \{P1\text{-}done, \textit{pause} \}\\$

% $\textit{Pre}(\overline{o}_{P1}) = \textit{Pre}(o) \cup \{\neg{\textit{pause}}, \lnot b \}$

% $\textit{Eff}(\overline{o}_{P1}) = \{P1\text{-}done, P1\textit{-violated}, \textit{pause} \}$

% \item  operators ${o}_{P2}^{V}$ and ${\overline{o}}_{P2}^{V}$:

% $\textit{Pre}({o}_{P2}^{V}) = \{P1\text{-}done,\textit{pause}\} \cup \{\lnot ((\textit{seen-}\phi^{P2}) \land (\lnot c \lor e) \land (d \lor f))\}$

% $\textit{Eff}({o}_{P2}^{V}) = \{P2\text{-}done, \lnot P1\text{-}done \}\\$

% $\textit{Pre}({\overline{o}}_{P2}^{V}) = \{P1\text{-}done, pause, \textit{seen-}\phi^{P2}, \lnot c \lor e, d \lor f \}$

% $\textit{Eff}({\overline{o}}_{P2}^{V}) = \{P2\text{-}done, \lnot P1\text{-}done, P2\textit{-violated} \}$

% \item  operators ${o}_{P2}^{N}$ and ${\overline{o}}_{P2}^{N}$:

% $\textit{Pre}({o}_{P2}^{N}) = \{P2\text{-}done,\textit{pause}\} \cup \{\lnot (\lnot \textit{seen-}\phi^{P2} \land (d \lor f))\}$

% $\textit{Eff}({o}_{P2}^{N}) = \{\lnot P2\text{-}done, \lnot pause\}\\$

% $\textit{Pre}({\overline{o}}_{P2}^{N}) = \{P2\text{-}done, pause, \lnot \textit{seen-}\phi^{P2}, d \lor f \}$

% $\textit{Eff}({\overline{o}}_{P2}^{N}) = \{\lnot P2\text{-}done, \lnot pause, \textit{seen-}\phi^{P2}\}\\$
% \end{itemize}

% The operators of $O_{\affectedd}$ involve additional predicates that are used for
% two purposes: (1) evaluating the conditional effects of $o'$ in sequence and (2)
% avoiding that the domain state changes because of the execution of domain operator during the evaluation.

% The $P_{i}\text{-}done$ predicates force the planner to strictly follow a fixed
% order avoiding repetitions in $\omega_{\affectedd}$. Once the planner starts
% the sequence $\omega_{\affectedd}$, no other operator, except those in $O_{\affectedd}$,
% can be applied before the application of the sequence is completed.

% The order of execution may be arbitrary, except
% for those operators concerning the violation of $P2$, i.e. ${o}_{P2}^{V}$
% and $\overline{o}_{P2}^{V}$, 
% whose evaluation takes place before that of the operators
% ${o}_{P2}^{N}$ and $\overline{o}_{P2}^{N}$, 
% appointed for the update the $\textit{seen-}\phi^{P2}$ predicate. As described previously,
% this expedient is necessary in order to correctly capture the violations 
% of at-most-once preference when they occour.
% % 

% Concerning the cost of the compiled operators of $O_{\affectedd}$ we have
% $c(o_{P1}) = c(\overline{o}_{P1}) = \textit{cost}(o) = \kappa$ and 
% $c(o_{P2}^{V}) = c(\overline{o}_{P2}^{V}) = c({o}_{P2}^{N}) = c(\overline{o}_{P2}^{N}) = 0$.


% EXPERIMENTAL RESULTS
\section{Experimental Results}

\subsection{Experiments Description}


We implemented the proposed compilation scheme and have evaluating it
by two sets of experiments with different purposes. On the one hand we
evaluated the scheme in a satisficing planning context in which we focused
on the search for sub-optimal plans using different planning systems,
while in the other we focuse on the
search of optimal plans using admissible heuristics.

Regarding the comparison in the context of the satisficing planning
we have considered the following {\stripsp} planning system
{\lama}\cite{RicLAMA}, {\mercury} \cite{MercuryIPC8},
{\miplan} \cite{nunez2014miplan}, {\ibacop} \cite{cenamor2014ibacop}, which are some of the best
performing planning system in IPC8 \cite{vallati20152014}, and
\fdss \cite{fdss2018}, \fdremixcomplete \cite{fdremix} (abbreviated with \fdremix),
which are some of the best performing planning system in the last IPC9 \cite{ipc9website}
which have been compared with {\lprpgp} \cite{coles2011lprpg}, which is one of the
performing planner which supports PDDL3 preferences. Moreover have considered our 
specifically enhanced version of {\lama} for planning with soft goal,
which is {\lamapruning}, which makes use of admissibile
heuristic $h_\text{R}$ to test the reachability of the soft goals
of the problem \cite{percassi2017improving}.

As benchmark we have considered the five domains 
of the qualitative preference track of IPC5 \cite{GHLS+09}
which involve always, sometime, sometime-before, at-most-once and soft goal preferences, i.e
Rovers, TPP, Trucks, Openstacks and Storage. 

For each original 
problem all preferences and each original utility were kept. The 
the classical planners were runned on the compiled problems while {\lprpgp}
was runned on the original problems of the competition. All the experiments
were conducted on a 2.00GHz Core Intel(R) Xeon(R) CPU E5-2620 machine with CPU-time
and memory limits of 30 minutes and 8GiB, respectively, for
each run of every tested planner. We have tested 8 planners for 5 domains
each of which consists of 20 instances for a total of 800 runs.

Table \ref{tab:ipc_score} shows the performances of the considered planning system
in term of plans quality. As quality measure we have used the IPC quality score, a
popular metric of which we have reported a brief description \cite{IPC6}. 

Given a planner $p$ and a task $i$ we assign, if $p$ solves $i$, the
following score to $p$:
$$
% \textit{score}(p, i) = \textit{cost}_{\textit{best}} (i) / \textit{cost}(p, i)
\textit{score}(p, i) = \frac{\textit{cost}_{\textit{best}} (i)}{\textit{cost}(p, i)}
$$

where $\textit{cost}_{\textit{best}}(i)$ is the cost of the best know
solution for the task $i$ 
found by any planner, and $\textit{cost}(p, i)$
is the cost of the solution found by the considered planner $p$ in 30 minutes. In our 
case our reference for $\textit{cost}_{\textit{best}} (i)$ is equal to the
cost of the best solution among the tested planners within 30 minutes. If $p$
did not find a solution within the time assigned, then $\textit{score}(p, i)$
is equal to $0$ in order to reward both quality and coverage. 

The quality score assigned to each tested planner $p$ is set equal to 
sum of the quality scores assigned to $p$ over all the considered instances:
$$
\textit{score}(p) = \sum_{i \in \textit{tasks}}{\textit{score}(p, i)}
$$

Table \ref{tab:ipc_score} reports the quality comparison using the IPC score
described above. It is splitted into six parts, at top we
have reported the qualitative comparision considering all kinds of preferences together
in the computation of the IPC score, while in the remaining subtables we have splitted the 
have considered each considered class of preference seperately (which is indicated in the
header of each subtable).

\noindent\hl{\textbf{NOTA (FP):} selezionare un numero di pianificatori significativi da includere nella tabella
per alleggerire la tabella.}

% Figures \ref{eps:histogram_histograms_ALL_PERCENTAGE_COST}---\ref{eps:histogram_histograms_storage.eps}
Figures \ref{lst:file1}---\ref{lst:file5} and \ref{lst:tpp:ag}---\ref{lst:tpp:sg}
show the qualitative comparison in a different and detailed way. In these histograms
we have reported, for each istance of each domain, another quality measure denoted with
$\alpha_{\textit{cost}}$. Each figure is associated
with one of the considered domains and involve two planners, the best performing planner, in term of 
IPC score and according to Table \ref{tab:ipc_score},
among the classical planner and {\lprpgp} which is the competitor.
% according to the IPC results showed in Table \ref{tab:ipc_score} in the considered domain.

% In particular we have evaluated
% the best performing planners according to the results showed in Table \ref{tab:ipc_score}
% which are {\lamapruning} and {\lprpgp} respectively.

We reported a brief description of the metric $\alpha_{\textit{cost}}$ that 
we used. Given a planner $p$ and a task $i$ we assign, if $p$ solves $i$,
the following score to $p$:
$$
\alpha_{\textit{cost}}(p, i) = \textit{cost}(p, i) / \textit{cost}_{\textit{total}}(i) = \frac{
\sum_{P \in \mathscr{P}(i) \;:\; \pi \not \models P}{c(P)}}{\sum_{P \in \mathscr{P}(i)}{c(P)}}
$$
where $\textit{cost}(p, i)$ is the cost of the solution found by planner $p$
for the task $i$ within 30 minutes and $\textit{cost}_{\textit{total}}(i)$ is
the sum of the costs of all the preferences involved in the task $i$ 
(note that $\mathscr{P}(i)$ denote the set of the preferences of the task $i$).

If we want to restric the calculation of $\alpha_{\textit{cost}}$ to
a single type of preference, for example just always preferences, we denote
the cost as $\alpha_{\textit{cost}}(\mathcal{A})$ while if nothing is indicated,
it means that we have considered all the classes of preferences.
% if we indicate
% $\alpha_{\textit{cost}}(\mathcal{\lnot SG})$

From the previous definition, $\alpha_{\textit{cost}}(p, i)$ could vary between $0$ 
and $1$. If $\alpha_{\textit{cost}}(p, i) = 0$, then it means that the numerator
$\textit{cost}(p, i)$ is equal to $0$ and that
$p$ has found an optimal plan for $i$ which satisfies all the preferences of the problem. On
the contrary, if $\alpha_{\textit{cost}}(p, i) = 1$, then it means that $p$ has found the worst plan 
for $i$ where all the preferences of the problem are violated.

More generally given an instance $i$, the ratio $\alpha_{\textit{cost}}(p, i)$, comparing plans
produced by different systems, tell us which planner has achieved
the satisfaction of the most useful subset of preferences in absolute
terms. In particular, the planner with the lowest ratio is the planner who
got the best performance on that particular instance.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DESCRIZIONE RISULTATI PIANI SUBOTTIMI
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Satisficing Planning Results}
%%%%%%%%%%%%%%%%%%%%%
% COMMENTO GENERALE %
%%%%%%%%%%%%%%%%%%%%%
The results obtained comparing the planners considered above show that the 
compilative approach is almost always preferable since the tested classical planners obtain 
an higher IPC score than {\lprpgp} except for {\mercury} and {\miplan}.

% In Table \ref{tab:ipc_score} we have incuded, for the lack of space, just the more significant 
% planning systems which are {\lamapruning}, {\fdremix}, {\lprpgp} and {\mercury}
% which were compared in terms of the IPC score. They were chosen
% because {\lamapruning} and {\fdremix} realize the best performance in term
% of IPC score among those considered
% classical planners while {\mercury} the worst.

With reference to Table \ref{tab:ipc_score} the compilative approach seems at glance
to be particularly preferable in Rovers, Trucks and Storage. In these
domains each classical planner performs better or at least comparable than {\lprpgp}
(except for {\mercury} in Trucks); {\ibacop} performs particularly well in Rovers, {\fdremix} in Trucks
and {\lamapruning} in Storage. Also {\miplan} works well in Trucks but it
is penalized due to coverage (it solves only 15 instances out of 20).

The planning system from the more recent IPC9 {\fdremix}
and {\fdss} perform overall better in this benchmark than the planning system from the
previous IPC8 but the enhanced version {\lamapruning} is better than everyone else indeed
it improves the performance of LAMA in all the considered domains except in
Rovers (where there is no soft goal).

Figures \ref{lst:file1}--\ref{lst:file5} show the $\alpha_{\text{cost}}$ comparison
for each domain comparing the best perfoming classical planner 
in term of IPC score in such domain with {\lprpgp}.

Looking at Figure \ref{lst:file1},
where we have reported the $\alpha_{\text{cost}}$ comparison for 
the best performing planners in Rovers in term of IPC score,
% {\ibacop} and {\lprpgp},
% the best performing competitor planning system in Rovers according to Table \ref{tab:ipc_score},
we can say that the compilative approach, combined with the use of {\ibacop},
achieves to satisfy a better subset of preferences in all the compared instances
compared to {\lprpgp}.


%%%%%%%%%%%%%%%%
% COMMENTO TPP %
%%%%%%%%%%%%%%%%
In TPP the compilative 
approach seems to be very ineffective, each classical planner
achieves an extremely lower quality performance compared to {\lprpgp}. The bad
performances in this domain are probably due to the many soft goals and sometime
preferences because, as shown in \cite{percassi2017improving}, the compilation
of soft goals can be sometime problematic and neither the use of the reachability
heuristic $h_{\text{R}}$ in {\lamapruning} can compensate this weakness.

Indeed the part of Table \ref{tab:ipc_score}, 
which concerns soft goals and sometime preferences,
cleary shows that {\lprpgp} is overall more performing than the classical planners
especially in TPP in term of these kinds of preferences achieved. This
aspect is clearly observable Figures \ref{lst:tpp:sg} and \ref{lst:tpp:st}
which report the $\alpha_{\text{cost}}(\mathcal{SG})$ and 
$\alpha_{\text{cost}}(\mathcal{ST})$ comparison respectively between {\lamapruning} and
{\lprpgp} in TPP. In these figures we can see that the compilative approach fails to achieve
a better subset of soft goals and sometime preferences compared to what {\lprpgp}
in half of the instances does. But looking at Table \ref{tab:ipc_score}
we can also observe that {\lamapruning} achieves a better result
of always, sometime-before and at-most-once preferences compared to {\lprpgp} in
term of IPC score, but this is not significant because
apparently it happens
at the expense of soft-goal and sometime preferences which
are clearly more expensive to violate (or equivalently more useful to satisfy).

% In Trucks {\ibacop} gets a poorer performance while {\fdremix} gets the best result
% in term of IPC score 

Looking at Figure \ref{lst:file3},
where we have reported the $\alpha_{\text{cost}}$ comparison for the
% best performing competitor planning system in Trucks according to Table \ref{tab:ipc_score},
best performing planners in Trucks in term of IPC score,
we can say that the compilative approach, combined with the use of {\fdremix}, 
achieves to satisfy a better, worst and equal subset of preferences in all instances compared to
{\lprpgp} in 15, 4 and 1 respectively.

% Looking at Figure \ref{lst:file2},
% where we have reported the $\alpha_{\text{cost}}$ for the
% best performing competitor planning system, according to Table \ref{tab:ipc_score} in TPP,
% we can say that the compilative approach combined with the use of {\ibacop} 
% achieves to satisfy a better, worst and equal subset of preferenes in 12, 7 and 1 instances
% respectively compared to {\lprpgp}.

% Looking at Figure \ref{lst:file2},
% where we have reported the $\alpha_{\text{cost}}$ comparison between {\ibacop}
% and {\lprpgp} calculated considering all kind of preferences, we can say that our classical planner

% Looking at Figure \ref{lst:file2},
% where we have reported the $\alpha_{\text{cost}}$ for the
% best performing competitor planning system, according to Table \ref{tab:ipc_score} in Openstacks,
% we can say that our approach combined with the use of {\ibacop} 
% achieves to satisfy a better subset of preferences in all instances.


% Looking at Figure \ref{lst:file3},
% where we have reported the $\alpha_{\text{cost}}$ comparison between {\lamapruning}
% and {\lprpgp} calculated considering all kind of preferences, we can say that our classical planner
% achieves to satisfy a better, worst and equal subset of preferenes in 12, 7 and 1 instances respectively.

%%%%%%%%%%%%%%%%%%%%%%%
% COMMENTO OPENSTACKS %
%%%%%%%%%%%%%%%%%%%%%%%
Regarding Opentacks the tested planners achieve a comparable performance even if the classical planners are slightly penalized compared to \lprpgp. Looking at Figure \ref{lst:file4},
where we have reported the $\alpha_{\text{cost}}$ comparison between {\lamapruning}
and {\lprpgp} calculated considering all kind of preferences, we can say that our classical planner
achieves to satisfy a better, worst and equal subset of preferenes in 12, 7 and 1 instances respectively


% {\lama} and {\\fdremix} compute lower quality plans that {\lprpgp}
% for more than half of the instances. Both classical system work better than
% LPRPGP in smaller instances but they get worse as the size increases. Looking
% at Figure \ref{lst:file3} we can say that {\lama} and {\\fdremix} performs better
% for more than half of the instances, in particular they find better plan in 13 and 16
% instances out of 20. Note that the classical planners get the optimal solution for some
% of the first seven instances. Looking at Figure \ref{lst:file4} we can say 
% that both approaches achieve a comparable performance even if {\lprpgp} generally finds 
% slightly better solutions than both classical competitors in 13 instances out of 20.

\noindent\hl{\textbf{NOTA (FP):} scrivi commento Storage.}


\begin{table}[]
\tiny
\setlength\tabcolsep{2pt}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
\multicolumn{7}{|c|}{$\mathscr{P}_{\mathcal{ALL}}$}\\ \hline 
Planner & Rovers & TPP & Trucks & Openstacks & Storage &TOTAL \\ \hline 
$\text{LAMA}_{\text{B}}(h_r)$ & 16.98 & 8.34 & 15.32 & 19.28 & \textbf{18.47} & \textbf{78.39}\\ \hline 
\fdremix & 17.89 & 7.1 & \textbf{17.67} & 18.99 & 16.2 & 77.86\\ \hline 
\fdssshort & 17.6 & 7.03 & 17.08 & 18.7 & 17.11 & 77.52\\ \hline 
LAMA(2011) & 17.01 & 7.53 & 13.04 & 18.42 & 17.81 & 73.82\\ \hline 
IBaCoP2 & \textbf{19.62} & 9.68 & 10.0 & 17.85 & 15.72 & 72.87\\ \hline 
LAMA(2018) & 16.44 & 7.63 & 13.34 & 16.03 & 17.78 & 71.22\\ \hline 
LPRPG-P & 11.36 & \textbf{18.74} & 6.99 & \textbf{19.71} & 12.87 & 69.66\\ \hline 
MIPlan & 17.65 & 8.8 & 9.23 & 17.35 & 14.42 & 67.45\\ \hline 
Mercury & 16.07 & 6.57 & 7.78 & 18.06 & 14.5 & 62.97\\ \hline
\hline
\multicolumn{7}{|c|}{$\mathscr{P}_{\mathcal{A}}$}\\ \hline 
Planner & Rovers & TPP & Trucks & Openstacks & Storage &TOTAL \\ \hline 
$\text{LAMA}_{\text{B}}(h_r)$ & 13.46 & 20.0 & 15.0 & 20.0 & 19.0 & 87.46\\ \hline 
\fdssshort & 13.51 & 17.0 & 18.0 & 17.83 & 20.0 & 86.34\\ \hline 
LAMA(2011) & 13.8 & 20.0 & 13.0 & 20.0 & 19.0 & 85.8\\ \hline 
IBaCoP2 & 14.73 & 20.0 & 13.0 & 19.0 & 19.0 & 85.73\\ \hline 
MIPlan & 14.03 & 20.0 & 12.0 & 19.0 & 20.0 & 85.03\\ \hline 
LAMA(2018) & 15.64 & 20.0 & 10.0 & 15.0 & 19.0 & 79.64\\ \hline 
\fdremix & 11.96 & 15.0 & 15.0 & 18.5 & 19.0 & 79.46\\ \hline 
Mercury & 13.07 & 20.0 & 4.0 & 20.0 & 20.0 & 77.07\\ \hline 
LPRPG-P & 13.78 & 7.0 & 0.0 & 19.5 & 11.0 & 51.28\\ \hline 
\hline 
\multicolumn{7}{|c|}{$\mathscr{P}_{\mathcal{SG}}$}\\ \hline 
Planner & Rovers & TPP & Trucks & Openstacks & Storage &TOTAL \\ \hline 
LPRPG-P & --- & 19.45 & 16.48 & 19.43 & 14.94 & 70.3\\ \hline 
\fdremix & --- & 14.73 & 17.12 & 18.63 & 18.57 & 69.05\\ \hline 
\fdssshort & --- & 14.66 & 16.49 & 18.4 & 18.44 & 67.99\\ \hline 
$\text{LAMA}_{\text{B}}(h_r)$ & --- & 14.82 & 14.36 & 18.7 & 18.9 & 66.78\\ \hline 
LAMA(2011) & --- & 13.95 & 13.57 & 17.78 & 18.29 & 63.6\\ \hline 
IBaCoP2 & --- & 16.03 & 10.7 & 17.23 & 18.61 & 62.57\\ \hline 
LAMA(2018) & --- & 14.82 & 13.17 & 15.84 & 18.3 & 62.13\\ \hline 
MIPlan & --- & 15.08 & 9.85 & 16.7 & 19.3 & 60.92\\ \hline 
Mercury & --- & 14.83 & 7.78 & 17.39 & 18.29 & 58.29\\ \hline 
\hline 
\multicolumn{7}{|c|}{$\mathscr{P}_{\mathcal{AO}}$}\\ \hline 
Planner & Rovers & TPP & Trucks & Openstacks & Storage &TOTAL \\ \hline 
Mercury & 16.61 & 20.0 & 19.0 & --- & 20.0 & 75.61\\ \hline 
\fdssshort & 16.4 & 17.0 & 20.0 & --- & 19.0 & 72.4\\ \hline 
LAMA(2018) & 14.92 & 17.0 & 20.0 & --- & 20.0 & 71.92\\ \hline 
$\text{LAMA}_{\text{B}}(h_r)$ & 14.54 & 18.0 & 20.0 & --- & 19.0 & 71.54\\ \hline 
LAMA(2011) & 14.36 & 17.0 & 20.0 & --- & 20.0 & 71.36\\ \hline 
\fdremix & 16.37 & 16.0 & 20.0 & --- & 18.0 & 70.37\\ \hline 
MIPlan & 14.03 & 16.0 & 15.0 & --- & 20.0 & 65.03\\ \hline 
IBaCoP2 & 13.21 & 15.0 & 16.0 & --- & 19.0 & 63.21\\ \hline 
LPRPG-P & 13.42 & 1.0 & 19.0 & --- & 12.0 & 45.42\\ \hline 
\hline 
\multicolumn{7}{|c|}{$\mathscr{P}_{\mathcal{SB}}$}\\ \hline 
Planner & Rovers & TPP & Trucks & Openstacks & Storage &TOTAL \\ \hline 
$\text{LAMA}_{\text{B}}(h_r)$ & 18.25 & 20.0 & 18.0 & --- & 19.0 & 75.25\\ \hline 
LAMA(2011) & 18.18 & 20.0 & 15.5 & --- & 18.0 & 71.68\\ \hline 
Mercury & 17.07 & 20.0 & 14.5 & --- & 20.0 & 71.57\\ \hline 
MIPlan & 18.24 & 20.0 & 12.0 & --- & 20.0 & 70.24\\ \hline 
\fdremix & 17.49 & 16.0 & 16.5 & --- & 20.0 & 69.99\\ \hline 
\fdssshort & 17.46 & 17.0 & 16.5 & --- & 19.0 & 69.96\\ \hline 
IBaCoP2 & 19.72 & 20.0 & 12.0 & --- & 18.0 & 69.72\\ \hline 
LAMA(2018) & 17.21 & 20.0 & 13.33 & --- & 17.0 & 67.54\\ \hline 
LPRPG-P & 8.21 & 14.0 & 15.5 & --- & 7.0 & 44.71\\ \hline 
\hline 
\multicolumn{7}{|c|}{$\mathscr{P}_{\mathcal{ST}}$}\\ \hline 
Planner & Rovers & TPP & Trucks & Openstacks & Storage &TOTAL \\ \hline 
LAMA(2018) & 13.23 & 11.0 & --- & --- & 20.0 & 44.23\\ \hline 
$\text{LAMA}_{\text{B}}(h_r)$ & 14.28 & 10.0 & --- & --- & 19.0 & 43.28\\ \hline 
\fdssshort & 16.17 & 8.0 & --- & --- & 19.0 & 43.17\\ \hline 
IBaCoP2 & 16.81 & 10.0 & --- & --- & 16.0 & 42.81\\ \hline 
LAMA(2011) & 14.41 & 9.0 & --- & --- & 19.0 & 42.41\\ \hline 
LPRPG-P & 9.56 & 17.0 & --- & --- & 14.0 & 40.56\\ \hline 
\fdremix & 15.44 & 8.0 & --- & --- & 16.0 & 39.44\\ \hline 
MIPlan & 14.91 & 9.0 & --- & --- & 14.0 & 37.91\\ \hline 
Mercury & 12.78 & 4.0 & --- & --- & 12.0 & 28.78\\ \hline 

\end{tabular}

\caption{Temp caption}
\label{tab:ipc_score}
\end{table}\newpage


\begin{figure}[]
\centering

\setcounter{tmp}{\thefigure}
\setcounter{figure}{\thelstlisting}
% \captionsetup{list=no,name=Listing}

% \begin{tabular}{cc}
% \begin{tabular}{c}

% ROVERS ALL HISTOGRAM
\begin{subfigure}[b]{\linewidth}
\includegraphics[width=8.5cm]{histogrammi/histogram_rovers_ALL_PERCENTAGE_COST.eps}
\caption{Rovers}
\label{lst:file1}
\end{subfigure}
% &
\\
% TPP ALL HISTOGRAM
\begin{subfigure}[b]{\linewidth}
\includegraphics[width=8.5cm]{histogrammi/histogram_tpp_ALL_PERCENTAGE_COST.eps}
\caption{TPP}
\label{lst:file2}
\end{subfigure} \\
% TRUCKS
\begin{subfigure}[b]{\linewidth}
\includegraphics[width=8.5cm]{histogrammi/histogram_trucks_ALL_PERCENTAGE_COST.eps}
\caption{Trucks}
\label{lst:file3}
\end{subfigure}

\end{figure}

\begin{figure}[htb] \ContinuedFloat
% OPENSTACKS
\begin{subfigure}[b]{\linewidth}
\includegraphics[width=8.5cm]{histogrammi/histogram_openstacks_ALL_PERCENTAGE_COST.eps}
\caption{Openstacks}
\label{lst:file4}
\end{subfigure}
\\
% \end{tabular}
% \begin{tabular}{c}
% STORAGE
\begin{subfigure}[b]{\linewidth}
\includegraphics[width=8.5cm]{histogrammi/histogram_preprocessed-storage_ALL_PERCENTAGE_COST.eps}
\caption{Storage}
\label{lst:file5}
\end{subfigure}
\\

% \end{tabular}

% \captionsetup{font=scriptsize}
\caption{Quality Comparison using $\alpha_{\textit{cost}}$ for each domain. Each bar represents the $\alpha_{\textit{cost}}$ of the best plan produced by the considered planner. The negative bar represents an instance which has not been solved or that has no preferences of that kind. From the top to bottom we have provided the results about $\alpha_{\textit{cost}}$ calculated considering each kind of preferences for Rovers, TPP, Trucks, Openstacks and Storage.}
\label{eps:histogram_histograms_ALL_PERCENTAGE_COST}
% \stepcounter{lstlisting}
% \setcounter{figure}{\thetmp}
\end{figure}


% RESTANTI HISTOGRAMMI COMMENTATI

% %%%%%%%%%%%%%%%%%%
% % ROVERS results %
% %%%%%%%%%%%%%%%%%%
% \begin{figure}[H]
% \centering
% \captionof*{table}{Rovers}
% \begin{tabular}{cc}

% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_rovers_ALL_PERCENTAGE_COST.eps}
% \caption{All}
% \label{lst:file1}
% \end{subfigure}
% &
% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_rovers_ag_PERCENTAGE_COST.eps}
% \caption{$\mathcal{A}$}
% \label{lst:file2}
% \end{subfigure} \\
% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_rovers_amo_PERCENTAGE_COST.eps}
% \caption{$\mathcal{AO}$}
% \label{lst:file1}
% \end{subfigure}
% &
% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_rovers_sbg_PERCENTAGE_COST.eps}
% \caption{$\mathcal{SB}$}
% \label{lst:file2}
% \end{subfigure} \\

% \end{tabular}

% \begin{tabular}{c}

% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_rovers_stg_PERCENTAGE_COST.eps}
% \caption{$\mathcal{ST}$}
% \label{lst:file2}
% \end{subfigure} \\

% \end{tabular}


% % \captionsetup{font=scriptsize}
% \caption{Domain: Rovers. Black: LAMA, Grey: IBaCoP2, Light-Grey: LPRPG-P. Each bar represents the $\alpha_{\textit{cost}}$ of the best plan produced by the considered planner. The negative bar represents an instance which has not been solved or that has no preferences of that kind. From left to right we have provided the results about the $\alpha_{\textit{cost}}$ calculated considering each kind of preferences, always, at-most-once, sometime-before and sometime preferences.}
% \label{eps:histogram_histograms_rovers.eps}

% \end{figure}

% %%%%%%%%%%%%%%%
% % TPP results %
% %%%%%%%%%%%%%%%
\begin{figure}[]
\centering

% \begin{tabular}{cc}

% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_tpp_ALL_PERCENTAGE_COST.eps}
% \caption{All}
% \label{lst:tpp:all}
% \end{subfigure}
% &
% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_tpp_ag_PERCENTAGE_COST.eps}
% \caption{$\mathcal{A}$}
% \label{lst:tpp:ag}
% \end{subfigure}
% \\
\begin{subfigure}{\linewidth}
\includegraphics[width=8.5cm]{histogrammi/histogram_tpp_sg_PERCENTAGE_COST.eps}
\caption{$\mathcal{SG}$}
\label{lst:tpp:sg}
\end{subfigure}
% &
% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_tpp_amo_PERCENTAGE_COST.eps}
% \caption{$\mathcal{AO}$}
% \label{lst:tpp:amo}
% \end{subfigure}
% \\
% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_tpp_sbg_PERCENTAGE_COST.eps}
% \caption{$\mathcal{SB}$}
% \label{lst:tpp:sbg}
% \end{subfigure}
% &
\begin{subfigure}{\linewidth}
\includegraphics[width=8.5cm]{histogrammi/histogram_tpp_stg_PERCENTAGE_COST.eps}
\caption{$\mathcal{ST}$}
\label{lst:tpp:st}
\end{subfigure}

% \end{tabular}

% \captionsetup{font=scriptsize}
\caption{Each bar represents the $\alpha_{\textit{cost}}$ of the best plan produced by the considered planner. The negative bar represents an instance which has not been solved or that has no preferences of that kind. From left to right we have provided the results about the $\alpha_{\textit{cost}}$ calculated considering each kind of preferences, always, at-end (or soft goals), at-most-once, sometime-before and sometime preferences.}
\label{eps:histogram_histograms_tpp.eps.sg}

\end{figure}

% %%%%%%%%%%
% % TRUCKS %
% %%%%%%%%%%
% \begin{figure}[H]
% \centering

% \begin{tabular}{cc}

% % ROVERS ALL HISTOGRAM
% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_trucks_ALL_PERCENTAGE_COST.eps}
% \caption{All}
% \label{lst:file1}
% \end{subfigure}
% &
% % TPP ALL HISTOGRAM
% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_trucks_ag_PERCENTAGE_COST.eps}
% \caption{$\mathcal{A}$}
% \label{lst:file2}
% \end{subfigure} \\
% % TRUCKS
% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_trucks_sg_PERCENTAGE_COST.eps}
% \caption{$\mathcal{SG}$}
% \label{lst:file1}
% \end{subfigure}
% &
% % OPENSTACKS
% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_trucks_amo_PERCENTAGE_COST.eps}
% \caption{$\mathcal{AO}$}
% \label{lst:file2}
% \end{subfigure} \\

% \end{tabular}

% \begin{tabular}{c}
% % STORAGE
% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_trucks_sbg_PERCENTAGE_COST.eps}
% \caption{$\mathcal{SB}$}
% \label{lst:file2}
% \end{subfigure} \\

% \end{tabular}

% % \captionsetup{font=scriptsize}
% \caption{Domain: Trucks. Black: LAMA, Grey: IBaCoP2, Light-Grey: LPRPG-P. Each bar represents the $\alpha_{\textit{cost}}$ of the best plan produced by the considered planner. The negative bar represents an instance which has not been solved or that has no preferences of that kind. From left to right we have provided the results about the $\alpha_{\textit{cost}}$ calculated considering each kind of preferences, always, at-end (or soft goals), at-most-once, sometime-before preferences.}
% \label{eps:histogram_histograms_trucks.eps}

% \end{figure}

% %%%%%%%%%%%%%%
% % OPENSTACKS %
% %%%%%%%%%%%%%%
% \begin{figure}[H]
% \centering

% \begin{tabular}{cc}

% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_openstacks_ALL_PERCENTAGE_COST.eps} 
% \caption{All}
% \label{lst:os:all}
% \end{subfigure}
% &

% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_openstacks_ag_PERCENTAGE_COST.eps}
% \caption{$\mathcal{A}$}
% \label{lst:os:ag}
% \end{subfigure}
% \\

% \end{tabular}

% \begin{tabular}{c}

% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_openstacks_sg_PERCENTAGE_COST.eps}
% \caption{$\mathcal{SG}$}
% \label{lst:os:sg}
% \end{subfigure}

% \end{tabular}

% % \captionsetup{font=scriptsize}
% \caption{Domain: Openstacks. Black: LAMA, Grey: IBaCoP2, Light-Grey: LPRPG-P. Each bar represents the $\alpha_{\textit{cost}}$ of the best plan produced by the considered planner. The negative bar represents an instance which has not been solved or that has no preferences of that kind. From left to right we have provided the results about the $\alpha_{\textit{cost}}$ calculated considering each kind of preferences, always and at end (or soft goals) preferences.}
% \label{eps:histogram_histograms_openstacks.eps}

% \end{figure}


% %%%%%%%%%%%%%%%%%%%
% % STORAGE results %
% %%%%%%%%%%%%%%%%%%%
% \begin{figure}[H]
% \centering

% \begin{tabular}{cc}

% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_preprocessed-storage_ALL_PERCENTAGE_COST.eps}
% \caption{All}
% \label{lst:file1}
% \end{subfigure}
% &
% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_preprocessed-storage_ag_PERCENTAGE_COST.eps}
% \caption{$\mathcal{A}$}
% \label{lst:file2}
% \end{subfigure} \\
% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_preprocessed-storage_sg_PERCENTAGE_COST.eps}
% \caption{$\mathcal{SG}$}
% \label{lst:file1}
% \end{subfigure}
% &
% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_preprocessed-storage_amo_PERCENTAGE_COST.eps}
% \caption{$\mathcal{AO}$}
% \label{lst:file2}
% \end{subfigure}
% \\
% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_preprocessed-storage_sbg_PERCENTAGE_COST.eps}
% \caption{$\mathcal{SB}$}
% \label{lst:file2}
% \end{subfigure}
% &
% \begin{subfigure}{\linewidth}
% \includegraphics[width=8.5cm]{histogrammi/histogram_preprocessed-storage_stg_PERCENTAGE_COST.eps}
% \caption{$\mathcal{ST}$}
% \label{lst:file2}
% \end{subfigure}

% \end{tabular}


% % \captionsetup{font=scriptsize}
% \caption{Domain: Storage. Black: LAMA, Grey: IBaCoP2, Light-Grey: LPRPG-P. Each bar represents the $\alpha_{\textit{cost}}$ of the best plan produced by the considered planner. The negative bar represents an instance which has not been solved or that has no preferences of that kind. From left to right we have provided the results about the $\alpha_{\textit{cost}}$ calculated considering each kind of preferences, always, at-end (or soft goals), at-most-once, sometime-before and sometime preferences.}
% \label{eps:histogram_histograms_storage.eps}

% \end{figure}


\subsection{Optimal Planning Results}

Similarly to what to what has been done in \cite{nebel2018}, we have tested our scheme
using some admissible heuristics which are $h^\text{blind}$, which assign 1 to all states except
for goal states to which assign 0, the maximum heuristic $h^\text{max}$, the merge and shrink heuristic $h^{\text{M\&S}}$, and the canonical pattern database heuristic $h^{\text{cpdb}}$ \cite{{haslum2007domain}}.
These heuristitcs guarantee the optimality of the solution found. Starting from the IPC5 domains,
we generated, similar to what was done \cite{nebel2018},
simpler instances by randomly sampling subsets of the soft trajectory constraints. Starting
from each instance we have generated five new instances with 1\%, 5\%, 10\%, 20\% and 40\% of
the (grounded) soft trajectory constraints while the hard goals have remained unchanged.

Since we do not have the instances that have been used in the aforementioned paper,
we have generated, for each percentage of sampling preferences (except for 100 \%), 3 sampled
instances in order to average the obtained results and be able to compare with their
results. 
All our experiments
were conducted on a 2.00GHz Core Intel(R) Xeon(R) CPU E5-2620 machine with CPU-time
and memory limits of 30 minutes and 8GiB, respectively, while the experiments 
reported in \cite{nebel2018} are conducted on a
Intel(R) Xeon(R) E5-2650v2 2.60GHz processors with 64GiB with one hour of CPU-time
for the search and then our approach is penalized.

The results about this experiment and the comparison with the automata approach
are shown Table \ref{coverage_admissible_heuristics}. The results
inherent to Openstacks have been excluded because it was not possible to find optimal
plans even for the simplest instances. 


\begin{table}[]
\scriptsize
\centering
\begin{tabular}{|c||c|c||c|c||c|c||c|c||}
\hline 
\multirow{2}{*}{DOMINO} & \multicolumn{2}{c||}{$h^{\text{blind}}$} & \multicolumn{2}{c||}{$h^{\text{max}}$} & \multicolumn{2}{c||}{$h^{\text{m\&s}}$} & \multicolumn{2}{c||}{$h^{\text{cpdb}}$} \\ \cline{2-9}
& WRB & Our & WRB & Our& WRB & Our& WRB & Our \\ \hline
% Storage & --- &  --- &  --- &  --- &  --- &  --- &  --- &  --- \hline
% Domain & $h^{\text{blind}}$ & $h^{\text{max}}$ & $h^{\text{m\&s}}$ & $h^{\text{cpdb}}$\\ \hline 
Storage & 24.78 & 57.0 & 29.2 & 45.0 & 32.50 & 24.0 & 23.10 & 57.0 \\ \hline
Rovers & 17.4 & 24.0 & 21.43 & 25.0 & 16.67 & 26.0 & 15.17 & 23.0 \\ \hline
Trucks & 18.84 & 24.0 & 23.19 & 25.0 & n/a & 25.0 & n/a & 25 \\ \hline
TPP & --- & 47.0 & --- & 45.0 & --- & 47.0 & -- & 40.0 \\ \hline

%  % & ---& ---& --- & ---\\ \hline
% \multirow{2}{*}{Rovers} & 23.0& 23.0& 23.0 &\\  \cline{2-5} 
%  % & ---& ---& --- & ---\\ \hline
% \multirow{2}{*}{TPP} & 36.0& 33.0& 35.0 &\\  \cline{2-5} 
%  % & ---& ---& --- & ---\\ \hline
% \multirow{2}{*}{Trucks} & 23.0& 28.0& 23.0 &\\  \cline{2-5} 
%  % & ---& ---& --- & ---\\ \hline
% \multirow{2}{*}{TOTAL} & 33.0& 31.0& 26.0 &\\ \cline{2-5}  
 % & ---& ---& --- & ---\\ \hline


\end{tabular}
\caption{Coverage of our and Nebel compilation scheme on the IPC5 benchmarks set with
additional instances with random sampled soft-trajectory constraints, A*
search for optimal solution. Our results concerning the sampled instances are averaged.}
\label{coverage_admissible_heuristics}
\end{table}


\section{Conclusions}



\bibliographystyle{plain}
\bibliography{biblio.bib}
\end{document}
