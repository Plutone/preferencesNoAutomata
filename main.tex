%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai19}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{color,soul}
\usepackage{listings}
\usepackage{caption,subcaption}
\usepackage{natbib}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{mathrsfs}  
\usepackage{amsfonts}  
\usepackage{amsthm}
\usepackage{comment}




\newtheorem{mydef}{Definition}
\newtheorem{define}{Definition}
\newtheorem{prop}{Proposition}

\newcounter{tmp}

% \usepackage{fontspec}
% \usepackage{unicode-math}
% \newcommand{\stripsp}{{\sc strips+}}
\newcommand{\stripsp}{{\sc STRIPS+}}
% \newcommand{\pddlthree}{{\sc PDDL3}}
\newcommand{\pddlthree}{{PDDL3}}
\newcommand{{\lama}}{{{LAMA}}}
\newcommand{\mercury}{{Mercury}}
\newcommand{{\fdss}}{{Fast Downward Stone Soup 2018}}
\newcommand{{\fdssshort}}{{FDSS 2018}}
\newcommand{\lamapruning}{$\text{LAMA}_{\text{P}}(h_\text{R})$}

\newcommand{{\miplan}}{{MIPlan}}
\newcommand{{\ibacop}}{{IBaCoP2}}
\newcommand{{\fdremixcomplete}}{{Fast Downward Remix}}
\newcommand{\lprpgp}{{LPRPG-P}}
\newcommand{{\fdremix}}{{FDRemix}}
% \newcommand{{\fifthIPC}}{{{IPC5}}}

\newcommand{\strips}{{\sc strips}}
\newcommand{\adl}{{\sc adl}}
\newcommand{\pddlIII}{{\sc{pddl3}}}
\newcommand{\stripspap}{{\sc strips+ap}}
\newcommand{\stripspp}{{\sc strips+p}}
\newcommand{\pddl}{{\sc pddl}}
% new command
\newcommand{\hplanp}{{HPlan-P}}
\newcommand{\mipsxxl}{{MIPS-XXL}}
\newcommand{\lpg}{{LPG}}
\newcommand{{\GBLcoles}}{{{GBL15-NB-B15}}}
% \newcommand{{\fifthIPC}}{{ {IPC5}}}
% \newcommand{{\fifthIPC}}{{\footnotesize {IPC5}}}
\newcommand{{\fifthIPC}}{{{IPC5}}}
\newcommand{\threatA}{T_{\mathcal{A}}(o)}
\newcommand{\violA}{V_{\mathcal{A}}(o)}
\newcommand{\threatSB}{T_{\mathcal{SB}}(o)}
\newcommand{\supportSB}{S_{\mathcal{SB}}(o)}
\newcommand{\threatAMO}{T_{\mathcal{AO}}(o)}
\newcommand{\supportST}{S_{\mathcal{ST}}(o)}
\newcommand{\affected}{\mathcal{I}(o)}
\newcommand{\truettt}{\small \texttt{TRUE}}
\newcommand{\pviol}{P\textit{-violated}}
\newcommand{\affectedd}{P_{\textit{affected}}(o)}
\newcommand{\lamap}{
$\text{LAMA}_{\text{P}}(h_{\text{R}})$}


\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
  \pdfinfo{
/Title (On Compiling PDDL3 Qualitative Preferences without Using Automata)
/Author (Anonymous)}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{On Compiling Away PDDL3 Soft Trajectory Constraints without Using Automata}
%\title{On Compiling Away PDDL3 Qualitative Preferences without Using Automata}
\author{...
}
\maketitle
\begin{abstract}
We address the problem of propositional planning extended with the class of soft temporally extended goals supported in {\pddlIII}, also called qualitative preferences since IPC-5.
%that is part of the standard planning language 
% {\pddl} since the 5th International Planning Competition (\fifthIPC). 
Such preferences are useful to characterise plan quality by allowing the user to express certain soft constraints on the state trajectory of the desired solution plans.
We propose and evaluate a compilation approach that 
extends previous work on compiling soft reachability goals and always goals to the full set of {\pddlIII} qualitative preferences.
This approach directly compiles qualitative preferences into propositional planning without using automata to represent the trajectory constraints. Moreover, since no numerical fluent is used, it allows 
many existing {\strips} planners to immediately address planning with preferences. 
%
%While other existing approaches represent {\pddlIII} soft state-trajectory constraints as particular automata that are compiled into a planning problem with numerical fluents, our approach directly compile them into propositional planning with action costs. 
%
%Our compilation extends previous work that was limited to soft reachability goals and state-invariant (always) goals to the full set of {\pddlIII} qualitative preferences.
%
%Starting from the work of Keyder and Geffner on compiling (reachability)
%soft goals, we propose a compilation scheme
%for translating a {\strips} problem enriched with qualitative  {\pddlIII}
%preferences  (and possibly also with soft goals) into an equivalent {\strips}
%problem with action costs. 
%The proposed compilation, which supports all types of qualitative preferences in the benchmarks
%from \fifthIPC, allows many existing {\strips} planners to immediately address
%planning with preferences. 
An experimental analysis presented in the paper evaluates the performance of
state-of-the-art propositional planners supporting action costs using our compilation
%approach to deal with 
of
{\pddlIII} qualitative   preferences.  The results indicate
that our approach is highly competitive with respect to current planners that
natively support the considered class of preference, as well as with a recent automata-based compilation approach.
\end{abstract}



%%%%%%%%%%%%%%%%
% INTRODUZIONE %
%%%%%%%%%%%%%%%%
\section{Introduction}
Planning with preferences, also called ``over-subscription planning" in \cite{Briel-et-al:AAAI04,do-kambhampati04,smith04}, concerns the generation of plans for problems involving soft goals or soft state-trajectory constraints (called preferences in \pddlIII \cite{GHLS+09}), that it is desired a plan satisfies, but that do not have to be satisfied. The quality of a solution plan for these problems depends on the soft goals and preferences that are satisfied.

%For instance, a useful class of preferences than can be expressed in {\pddlIII} \cite{GHLS+09} consists of  {\em always preferences}, requiring that a certain condition should hold in {\em every} state reached by a plan. As discussed in \cite{Weld:94AA,bacKab,GHLS+09,Hplanp:aij09}, adding always preferences to the problem model can be very useful to express safety or maintenance conditions, and other desired plan properties. An simple example of such conditions is ``whenever a building surveillance robot is outside a room, all the room doors should be closed". 

{\pddlIII} supports a rich set of types of preferences expressed through certain modal operators, and in particular the qualitative preferences of types {\em at-end} (soft goals), {\em sometime}, {\em sometime-before}, {\em at-most-once}, and {\em sometime-after}. One of the competition tracks of IPC5 \cite{GHLS+09} was centered on qualitative preferences, and since then some systems addressing this class of planning problems have been developed. Most of these systems represents state trajectory constraints as automata that are either compiled into the problem operators and problem states, or are handled by the planning algorithm.
 
In this paper, we study propositional planning with these types of preferences through a compilation approach. 
In particular, we extend extends previous work on compiling soft reachability goals \cite{percassi2017improving} and always goals \cite{ceriani2015planning} to a compilation that supports the full set of {\pddlIII} qualitative preferences. Differently from most other systems, our method directly compiles qualitative preferences into propositional planning with action costs without using automata to represent the state-trajectory constraints, and without using numerical fluents. Propositional planning with action costs is supported by many powerful planners, and the proposed compilation method allows them to also immediately support (through the compiled problems) qualitative preferences with no change to their algorithms and code.

An experimental analysis presented in the paper evaluates the performance of
state-of-the-art propositional planners supporting action costs using our compilation
%approach to deal with 
of
{\pddlIII} qualitative preferences.  The results indicate
that our approach is highly competitive with respect to current planners that
natively support the considered class of preference, as well as with a recent automata-based compilation approach.

%
%While other existing approaches represent {\pddlIII} soft state-trajectory constraints as particular automata that are compiled into a planning problem with numerical fluents, our approach directly compile them into propositional planning with action costs. 
%
%Our compilation extends previous work that was limited to soft reachability goals and state-invariant (always) goals to the full set of {\pddlIII} qualitative preferences.
%
%Starting from the work of Keyder and Geffner on compiling (reachability)
%soft goals, we propose a compilation scheme
%for translating a {\strips} problem enriched with qualitative  {\pddlIII}
%preferences  (and possibly also with soft goals) into an equivalent {\strips}
%problem with action costs. 
%The proposed compilation, which supports all types of qualitative preferences in the benchmarks
%from \fifthIPC, allows many existing {\strips} planners to immediately address
%planning with preferences. 
An experimental analysis presented in the paper evaluates the performance of
state-of-the-art propositional planners supporting action costs using our compilation
%approach to deal with 
of
{\pddlIII} qualitative   preferences.  The results indicate that our approach is highly competitive with respect to current planners that
natively support the considered class of preference, as well as with a recent automata-based compilation approach.






%%%%%%%%%%%%%%%%
% RELATED WORK %
%%%%%%%%%%%%%%%%
\section{Related Work}
Our compilative approach has a structure inspired by the work of Keyder and Geffner \cite{kn:geffner09}
on compiling soft goals into {\strips} with action costs (here denoted with \stripsp). In this work
the compilation scheme introduces, for each soft goal $p$ of the problem, a dummy goal $p'$ that
can be achieved only by two mutually exclusive actions. The first one, called \textit{collect(p)},
has cost 0 and requires $p$ true when it is applied; the second one, called \textit{forgo(p)},
has cost equal to the utility of $p$ and requires $p$ false when it is applied.
%\\\indent 
These actions can appear only at the end of the plan, and for each soft goal $p$
and one of them has to appear. 

%This scheme has achieved good perfomance which can be improved with the use of
%!TEX encoding = UTF-8 Unicodean ad hoc admissible heuristic based on the reachability of soft goals \cite{percassi2017improving}.
%
The most prominent existing planners supporting {\pddlIII} preferences are {\hplanp} \cite{Hplanp:aij09,BM:AAAI06},
which won the ``qualitative preference" track of IPC5, {\mipsxxl} \cite{edelkamp2006compilation,edelkamp2006large}
and the more recent {\lprpgp} \cite{coles2011lprpg}
and its extensions \cite{coles2013searching} \hl{where some different distance-to-go heuristics paired to a cost-to-go heuristic are exploited to promote
the expansion of solution of better quality but further away. 
}
% [CHECK: DIRE COSA FA L'EXTENDED VERSION PERCHE' NON LO ABBIAMO USATO]. [FP, perche' non l'abbiamo usato e' scritto negli 
% experimental results]
%
These planners represent preferences through automata whose states are synchronised with the states generated by the action plans, so that an accepting automaton state corresponds to preference satisfaction. For the synchronisation, {\hplanp} and {\lprpgp} use planner-specific techniques, while {\mipsxxl} compiles the automata by modifying the domain operators and adding new ones modelling the automata transitions of the grounded preferences. 

Our computation method is very different from the one of {\mipsxxl} since, rather than translating automata into new operators, the problem preferences are compiled by only modifying the domain operators, possibly creating multiple variants of them. Moreover, our compiled files only use {\stripsp}, while {\mipsxxl} also uses numerical fluents.\footnote{Another compilation scheme using numerical fluents is considered in \cite{GHLS+09} to study the expressiveness of \pddlIII (without an implementation).}

The works on compiling LTL goal formulas by Cresswell and Coddington \cite{CC:ECAI04} and Rintanen \cite{Rintanen:ECAI00}
are also somewhat related to ours, but with important differences. Their methods handle {\em hard} temporally extended
goals instead of preferences, i.e., every temporally extended goal must be satisfied in a valid plan, and hence there
is no notion of plan quality referred to the amount of satisfied preferences. Rintanen's compilation considers only
single literals in the always formulae (while we deal with arbitrary CNF formulas), and it appears that extending
it to handle more general formulas requires substantial new techniques \cite{Rintanen:personal2015}. 
% [CHECK FP: Come gestisco la personal note?]
%An implementation of 
%Crosswell and Coddington's approach is unavailable, but 

Bayer and McIlaraith \cite{BM:AAAI06} observed that Crosswell and Coddington's approach
suffers exponential blow up problems and performs less efficiently than {\hplanp}.

The other recent works that are related to ours are the compilation approach in \cite{ceriani2015planning}, which however supports only soft goals and always preferences, and the recent compilation schema by Wright, Mattmueller and
Nebel (here abbreviated WMN) in \cite{nebel2018}, which supports a class of soft state-trajectory constraints richer than the {\pddlIII} qualitative preferences.
%
WMN compiles soft trajectory constraints into conditional effects and state dependent action costs using
$\sf{LTL}_{\text{f}}$ \cite{de2014reasoning} and \hl{automata}.
% and B{\"u}chi automata. 
Besides the use of automata, other main differences are the following ones.
Our compilation has less additional fluents (at most two for each preference against one for each automaton state in WMN); \hl{WMN uses a numerical fluent for the plan cost while we use numerical fluent}; \hl{in WMN the plan cost can be increased as well as decreased (through the use of negative costs) at any point of the plan, while we only increase it and a the end of the plan (through the {\em forgo} actions)}.%
\footnote{The use of negative costs severely restricts the set of planners that can be used, since few planners support them. It appears that if WMN used only positive costs, it would loose optimally (from an optimal compiled plan we could not derive an optimal plan for the original problem). On the contrary, our method could be extended to use negative costs as in WMN without this problem.}

\begin{comment}
\textbf{FP: differenze approccio Nebel e nostro: 1) numero di fluenti introdotto nella compilazione diverso; 2) il nostro
e' un approccio piu' specifico PDDL3-oriented, il loro piu' generale; 3) diverso meccanismo di aggiornamento dei costi, nel loro caso
ricompensa e penalita', nel nosto solo penalità 4) il loro schema prevede quindi costi negativi e postivi, il nostro positivi; nel loro caso per avere solo costi positivi e' necessario perdere l'ottimalita' nel nostro caso no ed inoltre volendo potremmo introdurre dei costi
negativi per quelle preferenze la cui violazione puo' essere testata solo alla fine del piano mantenendo l'ottimalita'; ho fatto inoltre menzione del fatto che i costi possono essere incrementanti il prima possibile (es. always)}

\textbf{1)} In our approach, given a preference $P$,
we introduce at most a pair of boolean fluents, typically one additional fluent to 
represent if a preference is violated or not ($P\textit{-violated}$) and
in some cases an additional fluent to correctly represent the status of a preference during
the planning,
while in their approach it is introduced a boolean variable
for each state of the corresponding automaton of $P$. \textbf{2)} Their approach is more general
while ours is focused on PDDL3 constraints and therefore it is more specific.

\textbf{3)} In their work the cost of the plan during the planning is updated by using rewards
and penalties (negative and positive costs respectively). The cost of the plan is increased whenever
a violation of a preference (also reversible) occurs. On the contrary,
if an operator causes the satisfaction of a preference then the cost of the plan is decreased.
In both cases the negative or positive cost is equal to the utility of the interested preference\footnote{
\hl{Messo in footnote altrimenti era troppo lungo:}
Note that 
both the violation and the satisfaction of a preference may be temporary conditions depending on the type 
of interested preference. For example an always preference can be irreversibly violated and its satisfaction can only be evaluated at the end of the plan considering the whole trajectory of the states; on the contrary a sometime-after preference can be satisfied and violated several times during the execution of the plan.
}. \textbf{4)} This type of cost update requires the use of negative costs while our compilation scheme produces a problem
whose costs are monotonically increasing because, similarly to what was proposed in Keyder and Geffner,
costs are realized only at the end of the planning. In our scheme some costs can be anticipated for those preferences whose violation is irreversible (e.g. always, sometime-before) and negative costs could be used for those preferences that may be ``temporarily" violated (e.g. sometime, at-end). In both cases our scheme would maintain the optimality but in this work we have provided a compilation based only on positive costs in order to take advantage of a wider spectrum of classical planners (few planners still support negative costs).
\end{comment}

% [CHECK! AG: NUMERICAL FLUENTS in WMN?]

% [CHECK! AG: DIRE CHE HPLANP FUNZIONA PEGGIO DI LPRPG-P PERCHE' NON LO ABBIAMO CONSIDERATO E GLI AUTORI POSSONO ESSERE REVISORI!]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SEZIONE DEFINIZIONE PROBLEMA %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Preliminaries, Background and Notation}
%\subsection{Planning with Qualitative Preferences}
A {\strips}  problem is a tuple $\langle F, I, O, G\rangle$ where $F$ is a set of fluents, $I \subseteq F$ and $G \subseteq F$ are the initial state and goal set, respectively, and $O$ is a set of actions or operators defined over $F$ as follows.

A {\strips} operator $o \in O$ is a pair $\langle \textit{Pre}(o), {\textit{Eff}}(o)\rangle$, where 
$\textit{Pre}(o)$ is a sets of atomic formulae over $F$ and ${\textit{Eff}}(o)$ is a set of literals over $F$.  ${\textit{Eff}}(o)^+$ denotes the set of positive literals
in ${\textit{Eff}}(o)$, ${\textit{Eff}}(o)^-$ the set of negative literals in ${\textit{Eff}}(o)$.
An action sequence $\pi=\langle a_0, \ldots, a_m\rangle$ is applicable in a planning problem $\Pi$ if all actions $a_i$ are in $O$ and there exists a sequence of states $\langle s_0, \ldots, s_{m+1}\rangle$ such that $s_0=I$, $prec(a_i) \subseteq s_i$ and $s_{i+1}=s_i  \setminus 
\{p \mid \neg{p} \in {\textit{Eff}}(a_i)^-$\}$\cup{\textit{Eff}}(a_i)^+$, for $i = 0 \dots m$. Applicable action sequence $\pi$ achieves a fluent $g$ if $g \in s_{m+1}$, and is a valid plan for $\Pi$ if it achieves each goal $g \in G$ (denoted with $\pi \models G$).

A {\stripsp} problem is a tuple $\langle F,I,O,G,c \rangle $, where $\langle F,I,O,G \rangle $ is a {\strips} problem and $c$ is a function 
mapping each $o\in O$ to a non-negative real number.
The cost $c(\pi)$ of a plan $\pi$ is $\sum_{i=0}^{|\pi|-1} c(a_i)$, where $c(a_i)$ denotes the cost of the $i$th action $a_i$ in $\pi$ and $|\pi|$ is the length of $\pi$.


{\pddlIII} \cite{GHLS+09} introduced state-trajectory constraints, which are modal logic expressions expressible using LTL that ought to be true in the state trajectory produced by the execution of a plan. Let  $\langle s_0, s_1, ..., s_n\rangle$ be the  sequence of states in the state trajectory of a plan. Figure \ref{fig:semantics} defines {\pddlIII} qualitative state-trajectory constraints, i.e., constraints that do not involve numbers. Here $\phi$ and $\psi$ are first-order formulae that, without loss of generality, we assume are translated into grounded CNF-formulae; e.g., $\phi = \phi_{1} \land \phi_{2} \land ... \land \phi_{n}$ where $\phi_{i}$ ($i = 1 \ldots n$) is a clause formed by literals over the problem fluents. These constraints can be either soft or hard. When they are hard soft are called {\em qualitative preferences}. 

We will use the following notation:
$\mathcal{A}$, $\mathcal{SB}$, $\mathcal{SA}$, $\mathcal{ST}$, $\mathcal{AO}$, $\mathcal{G}$ denote the classes of qualitative preferences of type always, sometime-before, sometime, at-most-once and soft goal, respectively, for a given planning problem; $\sf{A}_{\phi}$, $\sf{SB}_{\phi,\psi}$, $\sf{SA}_{\phi,\psi}$, $\sf{ST}_{\phi}$, $\sf{AO}_{\phi}$, $\sf{G}_{\phi}$ denote a particular preference over $\mathcal{A}$, $\mathcal{SB}$, $\mathcal{SA}$, $\mathcal{ST}$, $\mathcal{AO}$, $\mathcal{G}$, respectively, involving formulae $\phi$ and $\psi$; moreover,
if a plan $\pi$ satisfies a preference $P$, we write  $\pi \models P$. 
% [CHECK: NOTAZIONE MODIFICATA: $\mathcal{G}$ invece di $\mathcal{SG}$] -> controllati
 
\begin{comment}
Starting from a {\stripsp} problem we define the 
notion of state trajectory generatered by a plan. Given a 
{\stripsp} $ \Pi = \langle F,I,O,G,c \rangle $ problem, a plan $\pi$ generates the trajectory
$\langle s_0, s_1, ..., s_n \rangle$ iif
$S_0 = I$ and for each happening $h$ generated by $\pi$, with $h$
at time $t$, there is some $i$ such that $t_i = t$ and $S_i$ is the
result of applying the happening $h$ \toprule
 $S_{i-1}$, and for every
$j \in \{1 \ldots n\}$ there is a happening $\pi$ at $t_j$.

The following is a fragment of the grammar of {\pddlIII},
describing the new modalities of {\pddlIII} for expressing these soft state-trajectory constraints
(indicated with {\small \texttt{con-GD}})
(the full BNF grammar is given in \cite{techReportGerevini1,techReportGerevini2})
where \texttt{<GD>} is a goal description (a first order logic formula):

{\small
\begin{verbatim}
  <con-GD> ::= (at end <GD>) | (always <GD>) |
               (sometime <GD>) |
               (at-most-once <GD>) | 
               (sometime-after <GD> <GD>) | 
               (sometime-before <GD> <GD>)
\end{verbatim}
}
Let $\phi$ and $\psi$ be atomic formulae over the predicates of a planning problem,
then interpretation of the modal operators considered in this work is specified in Figure \ref{fig:semantics} [COMMENTATA].
\end{comment}

 \begin{figure}[t]
\vspace{-7mm}
{\small
 \[\begin{array}{ll}
 \multicolumn{2}{l}{\langle s_0, s_1, ..., s_n \rangle \models (\mbox{\textit{at\,end}}~\phi) \hspace{0.25cm} \mbox{iff} \hspace{2mm} S_n \models \phi} \\

 \multicolumn{2}{l}{\langle s_0, s_1, ..., s_n \rangle \models (\mbox{\textit{always}}~\phi)} \\
   &\hspace{0.25cm} \mbox{iff} \hspace{2mm} \forall i: 0 \leq i\leq n \cdot S_i \models \phi\\
 \multicolumn{2}{l}{\langle s_0, s_1, ..., s_n \rangle \models (\mbox{\textit{sometime}}~\phi)} \\
  &\hspace{0.25cm} \mbox{iff} \hspace{2mm} \exists i: 0 \leq i \leq n \cdot S_j \models \phi\\

 \multicolumn{2}{l}{\langle s_0, s_1, ..., s_n \rangle \models (\mbox{\textit{at-most-once}}~\phi)}\\
 &\hspace{0.25cm} \mbox{iff} \hspace{2mm} \forall i : 0 \leq i \leq n \cdot~\mbox{if}~S_i \models \phi~\mbox{then}~\\
 &\hspace{1cm} \exists j : j \geq i \cdot \forall k : i \leq k \leq j \cdot S_k \models \phi \mbox{ and}~\\
 &\hspace{1cm} \forall k : k > j \cdot S_k \models \neg\phi\\

 \multicolumn{2}{l}{\langle s_0, s_1, ..., s_n \rangle \models
 (\mbox{\textit{sometime-after}}~\phi~\psi)}\\
 & \hspace{0.25cm} \mbox{iff} \hspace{2mm} \forall i \cdot 0 \leq i \leq n \cdot \mbox{if}~S_i \models \phi~\mbox{then}~ \\
 &\hspace{1cm}\exists j : i \leq j \leq n \cdot S_j \models \psi \\
 \multicolumn{2}{l}{\langle s_0, s_1, ..., s_n \rangle \models
 (\mbox{\textit{sometime-before}}~\phi~\psi)}\\
 & \hspace{0.25cm} \mbox{iff} \hspace{2mm} \forall i \cdot 0 \leq i \leq n \cdot \mbox{if}~S_i \models \phi~\mbox{then}~ \\
 &\hspace{1cm} \exists j : 0 \leq j < i \cdot S_j \models \psi \\

 \end{array}\]
 }

 \vspace{-0.5cm}
 \caption{\label{fig:semantics}
 Semantics of the basic modal operators in {\pddlIII}
 %NB: FIGURA DA SEMPLIFICARE: TOGLIERE GLI HAPPENINGS .
 % $\phi$ and $\psi$ stand for arbitrary (syntactically valid) goal formulae of {\pddlIII}.
 }
 \end{figure}

\begin{mydef}
A {\strips+} problem with preferences is a tuple $\langle F, I, O, G, \mathscr{P}, c, u \rangle$ where:
\begin{itemize}
    \item $\langle F,I,O,G,c \rangle$ is a \strips+ problem;
    \item $\mathscr{P} = \{ \mathscr{P}_{\mathcal{A}}  \cup  \mathscr{P}_{\mathcal{SB}} \cup  \mathscr{P}_{\mathcal{SA}} \cup  \mathscr{P}_{\mathcal{ST}} \cup  \mathscr{P}_{\mathcal{AO}} \cup  \mathscr{P}_{\mathcal{G}} \}$ is the set of the preferences of\ $\Pi$ where $\mathscr{P}_{\mathcal{A}} \subseteq \mathcal{A}$, $\mathscr{P}_{\mathcal{SB}} \subseteq \mathcal{SB}$, $\mathscr{P}_{\mathcal{SA}} \subseteq \mathcal{SB}$, $\mathscr{P}_{\mathcal{ST}} \subseteq \mathcal{ST}$, $\mathscr{P}_{\mathcal{AO}} \subseteq \mathcal{AO}$ and $\mathscr{P}_{\mathcal{G}} \subseteq \mathcal{G}$;
    \item $u$ is an utility function $u: \mathscr{P} \rightarrow \mathbb{R}_0^{+}$. 
    %mapping each $P \in \mathscr{P}$ to a value in $\mathbb{R}_0^{+}$.
\end{itemize}
\end{mydef}

\noindent
{\strips+} with preferences will be indicated with \stripspp.

\begin{mydef}
Let $\Pi$ be a {\stripspp} problem with preferences  $\mathscr{P}$. The utility $u(\pi)$ of a plan $\pi$ solving $\Pi$ is the difference between the total amount of utility of the preferences by the plan and its cost:
%$
$u(\pi) = \sum_{P \in \mathscr{P}: \pi \models_{} P}{u(P)} - c(\pi).$
%$
\end{mydef}

The definition of plan utility for {\stripspp} is similar to the one given for {\stripsp} with soft goals by Keyder and Geffner \cite{kn:geffner09}. 
A plan $\pi$ with utility $u(\pi)$ for a {\stripspp} problem is optimal when there is no plan $\pi'$ such that $u(\pi') > u(\pi) $. 
The {\em violation cost} of a preference is the value of its utility. 
% [AG: CHECK!!!] - controllato su paper geffner

%\subsection{Additional Notation and Definitions}
In order to make the presentation of our compilation approach more compact, we introduce some further notation.

Given a preference clause $\phi_{i} = l_1 \lor l_2 \lor ... \lor l_m$, the set $L(\phi_{i}) = \{l_1, l_2, ..., l_m\}$ is the equivalent set-based definition of $\phi_{i}$ and $\overline{L}(\phi_{i}) = \{\lnot l_1, \lnot l_2, ..., \lnot l_m\}$ is the literal complement set of $L(\phi_{i})$.

Given an operator $o$ of a {\stripspp} problem, $Z(o)$ denotes  the set of literals
$$
Z(o) = (\textit{Pre}(o) \setminus \{p \mid \lnot p \in \textit{Eff}(o)^{-}\}) \cup \textit{Eff}(o)^{+} \cup \textit{Eff}(o)^{-}.
$$
Note that the literals in $Z(o)$ hold in any reachable state resulting from the execution of operator $o$.

The state where an operator $o$ is applied is indicated with $s$ and the state resulting from the application of $o$ with $s'$.

\begin{mydef}\label{def:trueclauses}
Given an operator $o$ and a CNF formula $\phi = \phi_{1} \land ... \land \phi_{n}$, we define the set $C_{\phi}(o)$ of clauses of $\phi$ that $o$ makes {\bf certainly true} (independently from $s$) in $s'$ as:
$$
C_{\phi}(o) =  \{\phi_{i} : |L(\phi_{i}) \cap Z(o)| > 0, \;i \in \{1 \ldots n\} \}.
$$
\end{mydef}

%[AG: CHECK!!! CAMBIARE NOTAZIONE PERCHE' ABBIAMO TOLTO $\overline{C}_{\phi}(o)$ e DICIAMO $o$ MAKES CERTAINLY TRUE] ------ {FATTO}

Given a clause $\phi_{i} = l_1 \lor ... \lor l_{m_i}$ of $\phi$, condition $|L(\phi_{i}) \cap Z(o)| > 0$ in Definition \ref{def:trueclauses} requires that there exists at least a literal $l_j$, $j \in \{1 \ldots m_i\}$ that belongs to $Z(o)$ and thus making clause $\phi_i$ true in $s'$.


%If a formula $\phi$ is guaranteed to be true in $s'$ 
\begin{mydef}\label{def:becometrue}
Given an operator $o$ and a CNF formula $\phi = \phi_{1} \land ... \land \phi_{n}$, we say that $o$ \textbf{can make $\phi$ true} (dependently from $s$) if
\begin{enumerate}
% $C_{\phi}(o) \not = \O$ and
\item $|C_{\phi}(o)|>0$;

\item for each clause $\phi_i$ of $\phi$ not in ${C}_{\phi}(o), \; \overline{L}(\phi_i) \not \subseteq Z(o)$.

\end{enumerate}
\end{mydef}

Condition 1 in Definition \ref{def:becometrue} requires that there exists at least a clause of $\phi$ that is certainly true in $s'$ (independently from $s$), while Condition 2 requires that the clauses that are not certainly true in $s'$ are not certainly false in $s'$.

\begin{mydef}\label{def:becomefalse}
Given an operator $o$ and a CNF formula $\phi = \phi_{1} \land ... \land \phi_{n}$, we say that $o$ \textbf{can make $\phi$ false} (dependently from $s$) if
\begin{enumerate}
\item $|\overline{L}(\phi_i) \cap Z(o)| > 0 \land \overline{L}(\phi_i) \subset Z(o)$
% \item $L(\phi_i) \cap Z(o) = \O $
\item $|L(\phi_i) \cap Z(o)| = 0 $
\item $\overline{L}(\phi_i) \not \subseteq \textit{Pre}(o)$.
\end{enumerate}
\end{mydef}

The conditions of Definition \ref{def:becomefalse} require that there exist at least a clause of $\phi$ that
(1) has some (but not all) literals which are falsified after the execution of $o$,
(2) has not literals which are true in the resulting state from the application of $o$ and (3)
this clause is not already false in the state where $o$ is applied.
\begin{comment}
The expression 
$\overline{L}(\phi_i) \not \subseteq \textit{Pre}(o)$ in Definition \ref{violation_formula}-\ref{threat_formula}
is necessary to avoids that an operator $o$ is considered a violator/threat when its precondition is
already violated in the state where it is applied.
\end{comment}

\section{Operator-Preference Interactions}

Operators and preferences may have different kinds of interactions, that we have to deal with in their compilation.
We say that an operator $o$ is {\em neutral} for a preference $P$ if its execution in a plan can affect the satisfaction of $P$ in the state trajectory of the plan. Otherwise, depending on the preferences type of $P$,  $o$ can behave as a {\em violator}, a \textit{threat} or a {\em potential support} of $P$. Informally, a violator falsifies the preference, a threat may falsify it (depending on $s$), and a potential support may satisfies it over the full state trajectory of the plan. In the following, more formal definitions are given for each type of preference.

%[AG: CHECK. VIOLATION MODIFICATO IN VIOLATOR]

\begin{comment}
In our compilation scheme of a {\stripspp} problem we have to distinguish, for each kind of preference,
different class of operators in order to specialize the operators compilation based on 
\setlength{\headwidth}{\textwidth}
ow they interact with the preferences of the problem.

Generally speaking, we can identify three classes of operators regardless of the type of preference considered. An
operator is a \textit{threat} for a preference $P$ if in case it is executed it may violate $P$. An
operator is a (potential) \textit{threat} for a preference $P$ if in case it is executed it could satisfy $P$. Finally,
an operator $o$ is \\textit{threat}for a preference $P$ if its execution
can not influence the current state of the preference. In the following Subsection \ref{subsection:always_operators}--\ref{subsectionAMO}
we will decline these generic definitions of classes of operators for each type 
of considered preference providing for each of them a formal definition.
\end{comment}

% [AG:CHECK!! LA FRASE SEGUENTE SI PUO' OMETTERE!?]
% In the following definitions of operators classes we will assume that $\phi$ (and $\psi$ in the case of dual preferences) are arbitrary CNF formulae $\phi = \phi_{1} \land ... \land \phi_{n}$ where each clause $\phi_{i} = l_{1} \lor ... \lor l_{m_{i}}$ for each $i \in \{1 \ldots n\}$.

\subsection{Operators Affecting Always Preferences} \label{subsection:always_operators}
An always preference $\sf{A}_{\phi}$ is violated if ${\phi}$ is false in any state on the plan state trajectory. Hence, if the state $s'$ generated by an operator $o$ makes ${\phi}$ false, then $o$ is a violator of $\sf{A}_{\phi}$. 

% ----------
% VIOLAZIONE
% ----------
\begin{mydef}\label{violation_formula}
Given an operator $o$ and an always preference $\sf{A}_{\phi}$ of a {\stripspp} problem, $o$ is a \textbf{violator} of $\sf{A}_{\phi}$ if there is a clause $\phi_i$ of $\phi$ such that:
%\begin{enumerate}
%\item 
(1) $\overline{L}(\phi_i) \subseteq Z(o)$, and
%\item 
(2) $\overline{L}(\phi_i) \not\subseteq \textit{Pre}(o)$.
%\end{enumerate}
\end{mydef}

%Definition \ref{violation_formula} require that
%there exists at least a clause of $\phi$ (1) whose literals are falsified after the execution of $o$ (2)
%which is not already false in the state where $o$ is applied.
%If an operator violates a preference, the preference is unsatisfied in any state resulting
%from the application of the operator. 
% The set of always preferences that are violated by an operator $o$ is denoted with $\violA$.

% [CHECK FRASE! RISCRITTO.]
Operator $o$ is a threat of $\sf{A}_{\phi}$ if it is not a violator, its effects make false at least a literal of a clause $\phi_i$ of $\phi$, and its preconditions don't entail $\neg {\phi}_i$ (otherwise $\sf{A}_{\phi}$ would be already false in $s$). Such clause $\phi_i$ is a \textit{threatened clause} of $\sf{A}_{\phi}$.


% --------
% MINACCIA
% --------
\begin{mydef}\label{threat_formula}
Given an operator $o$ and an always preference $\sf{A}_{\phi}$ of a {\stripspp} problem, $o$ is a \textbf{threat} of $\sf{A}_{\phi}$ if it is not a violator and it can make $\phi$ false.
\end{mydef}


\begin{comment}  
\begin{mydef}\label{threat_formula}
Given an operator $o$ and an always preference $\sf{A}_{\phi}$ of a {\stripspp} problem, $o$ is a \textbf{threat} of $\sf{A}_{\phi}$  if it is not a violator and there exists a clause $\phi_i$ of $\phi$ such that:
\begin{enumerate}
\item $|\overline{L}(\phi_i) \cap Z(o)| > 0 $
\item $|L(\phi_i) \cap Z(o)| = 0 $
\item $\overline{L}(\phi_i) \not \subseteq \textit{Pre}(o)$.
\end{enumerate}
\end{mydef}

The conditions of Definitin \ref{threat_formula} require that there exist at least a clause of $\phi$ that
(1) has some (but not all) literals which are falsified after the execution of $o$,
(2) has not literals which are true in the resulting state from the application of $o$ and (3)
this clause is not already false in the state where $o$ is applied. The expression 
$\overline{L}(\phi_i) \not \subseteq \textit{Pre}(o)$ in Definition \ref{violation_formula}-\ref{threat_formula}
is necessary to avoids that an operator $o$ is considered a violator/threat when its precondition is
already violated in the state where it is applied.
\end{comment}

%A clause $\phi_i$ of $P = \sf{A}_{\phi}$ satisfying these conditions
%is a \textit{threatened clause} of $P$. A threatened preference (clause) may be falsified by an operator depending on the state where the operator is applied. 
% The set of always preferences threatened by $o$ is denoted with $\threatA$; 
The set of clauses of a preference $\sf{A}_{\phi}$ threatened by $o$ is denoted with $TC(o, \phi)$.
% --------
% NEUTRALE
% --------

An operator is neutral for $\sf{A}_{\phi}$ if it makes $\phi$ true, does not falsify any clause of $\phi$, or it can be applied only to states where $\phi$ is false. 
\begin{mydef}
Given an operator $o$ and an always preference $\sf{A}_{\phi}$ of a {\stripspp} problem, $o$ is \textbf{neutral}  for $\sf{A}_{\phi}$  if:
\begin{enumerate}
    % \item for all clauses $\phi_i$ of $P$, $L(\phi_i) \cap Z(o) \not = \O $ or $ \overline{L}(\phi_i) \cap Z(o) = \O$ holds;
    \item for all clauses $\phi_i$ of $\phi$, $|L(\phi_i) \cap Z(o)| > 0 $ or $ |\overline{L}(\phi_i) \cap Z(o)| = 0$, or
    \item there exists a clause $\phi_i$ of $\phi$ such that $\overline{L}(\phi_i) \subseteq \textit{Pre}(o)$.
\end{enumerate}
\end{mydef}

\noindent
{\bf Example}. An operator $o = \langle \lnot a, \lnot b\rangle$ 
% such that $\textit{Pre}(o) = \{\lnot a\}$ and $\textit{Eff}(o) = \{\lnot b\}$ 
is a threat of $\mathsf{A}_{\phi_1}$, a violator of $\mathsf{A}_{\phi_2}$ and neutral for
$\mathsf{A}_{\phi_3}$ where 
$\phi_1 = c \lor b$, $\phi_2 = a \lor b$
and $\phi_3 = d$.


% ----------------------------------------------
% CLASSI DI OPERATORI PER LA PREFERENZA SOMETIME
% ----------------------------------------------
\subsection{Operators Affecting Sometime Preferences} \label{subsection:sometime_operators}
A sometime preference $\sf{ST}_{\phi}$ is violated if 
there is not at least one state in which $\phi$ is true
on the plan state trajectory. Hence,
if the state $s'$ generated by an operator $o$ makes $\phi$ true, then $o$
is a potential support of $\sf{ST}_{\phi}$.


\begin{mydef} \label{sometime_definition}
Given an operator $o$ and a sometime preference $\sf{ST}_{\phi}$ of a {\stripspp} problem,
$o$ is a \textbf{potential support} for $\sf{ST}_{\phi}$ if $o$ can make true $\phi$, otherwise
the operator is \textbf{neutral} for $\sf{ST}_{\phi}$.
\end{mydef}
% The set of sometime preferences of $\Pi$ which are potentially supported by the operator 
% $o$ are denoted with $S_{\mathcal{ST}}(o)$.

{\bf Example}. An operator $o = \langle \top, \lnot b \rangle$
% $\textit{Pre}(o) = \O$ and $\textit{Eff}(o) = \{\lnot b\}$ 
% $\textit{Pre}(o) = \{\}$ and $\textit{Eff}(o) = \{\lnot b\}$ 
is a potential support of $\mathsf{ST}_{\phi_1}$ and neutral for $\mathsf{ST}_{\phi_2}$
where ${\phi_1} = c \lor \lnot b$ and ${\phi_2} = c$.

% -----------------------------------------------------
% CLASSI DI OPERATORI PER LA PREFERENZA SOMETIME BEFORE
% -----------------------------------------------------
\subsection{Operators Affecting Sometime-before Preferences}

A sometime-before preference $\sf{SB}_{\phi, \psi}$ is violated if $\phi$ becames
true before $\psi$ has been made true on the plan state trajectory. Hence,
if the state $s'$ generated by an operator $o$ can make ${\psi}$ true, 
then $o$ is a potential support of $\sf{SB}_{\phi, \psi}$. Depending by the
state where it is applied such operator could behave as a actual support
or as a neutral operators for $\sf{SB}_{\phi, \psi}$.
%%%%%%%%%%%%%%%%%%%
% POTENTIAL SUPPORT
%%%%%%%%%%%%%%%%%%%
\begin{mydef}\label{sup_sb}
Given an operator $o$ and a sometime-before preference $P= \sf{SB}_{\phi, \psi}$ of a {\stripspp} problem,
$o$ is a \textbf{potential support} for $P$ if $o$ can make $\psi$ true.\\
\end{mydef}
% The set of sometime-before preferences of $\Pi$ which are potentially supported by the operator 
% $o$ are denoted with $S_{\mathcal{SB}}(o)$.

If the state $s'$ generate by an operator $o$ can make $\phi$ true,
then $o$ is a threat of $\sf{SB}_{\phi, \psi}$. Depending by the
state where it is applied such operator could behave as a violator
or as a neutral operators for $\sf{SB}_{\phi, \psi}$.
%%%%%%%%
% THREAT
%%%%%%%%
\begin{mydef}
Given an operator $o$ and a sometime-before preference $P=\sf{SB}_{\phi, \psi}$ 
of a {\stripspp} problem, $o$ is a \textbf{threat} for $P$ if $o$ can make true $\phi$.
\end{mydef}
The set of sometime-before preferences of $\Pi$ which are threatened by the operator 
$o$ are denoted with $T_{\mathcal{SB}}(o)$.

If the state $s'$ generated by an operator $o$ can not make neither $\phi$ nor $\psi$ true,
then $o$ is a neutral operator for $\sf{SB}_{\phi, \psi}$
regardless of the state in which it is applied.
\begin{mydef}
Given an operator $o$ and a sometime-before preference $P= \sf{SB}_{\phi, \psi}$ of a {\stripspp} problem,
$o$ is a \textbf{neutral} for $P$ if $o$ is not a potential support or a threat.
\end{mydef}

{\bf Example.} An operator $o = \langle \top ,b \rangle$ 
% such that 
% $\textit{Pre}(o) = \O$
% $\textit{Pre}(o) = \{e\}$
% and $\textit{Eff}(o) = \{ b\}$ 
is a potential support of $\mathsf{SB}_{\phi_1, \psi_1}$,
a threat of $\mathsf{SB}_{\phi_2, \psi_2}$
and neutral for $\mathsf{SB}_{\phi_3, \psi_3}$ where 
$\phi_1 = c $ and $\psi_1 = a \lor b$,
$\phi_2 = c \lor b$ and $\psi_2 = d$
and $\phi_3 = d$ and $\psi_3 = e$.

% --------------------------------------------------
% CLASSI DI OPERATORI PER LA PREFERENZA AT-MOST-ONCE
% --------------------------------------------------
\subsection{Operators Affecting At-most-once Preferences}\label{subsectionAMO}
% An at-most-once preference has the following {\pddl} syntax 
% $$(preference\text{ }P\text{ }(at\textit{-}most\textit{-}once\text{ }\phi)$$
% where the formula $\phi$ has to become true at most once in the plan state trajectory. In the following we will abbreviate it with $P = \sf{AO}_{\phi}$. 
An at-most-once preference $\sf{AO}_{\phi}$ is violated if ${\phi}$ is false if ${\phi}$ becames
true more than once on the plan state trajectory. Hence, if the state $s'$ generated 
by an operator $o$ can make ${\phi}$ true, then $o$ is a threat of $\sf{AO}_{\phi}$.
\begin{mydef}\label{threat_amo}
Given an operator $o$ and an at-most-once preference $P= \sf{AO}_{\phi}$ of a {\stripspp} problem, 
$o$ is a \textbf{threat} of $P$ if $o$ can make true $\phi$,
otherwise $o$ is \textbf{neutral} for $\sf{AO}_{\phi}$..
\end{mydef}

% The set of at-most-once preferences of $\Pi$ which are threatened by 
% the operator $o$ are denoted with $T_{\mathcal{AO}}(o)$.

{\bf Example} An operator $o = \langle \top, \lnot b \rangle$ 
% such that $\textit{Pre}(o) = \O$ and $\textit{Eff}(o) = \{\lnot b\}$ 
is a potential support of $\mathsf{AO}_{\phi_1}$ where $\phi_1 = c \lor \lnot b$.


% ----------------------------------------------------
% CLASSI DI OPERATORI PER LA PREFERENZA SOMETIME-AFTER
% ----------------------------------------------------
\subsection{Operators Affecting Sometime-After Preferences}\label{subscetionSA}
A sometime-after preference ${\sf SA_{\phi, \psi}}$ is violated if $\phi$ becomes true in a state 
without $\psi$ becoming true in a succeeding state on the plan state trajectory. Hence,
if the state $s'$ generated by an operator $o$ can make
$\phi$ true, then $o$  threats $\sf SA_{\phi, \psi}$ because $\psi$ could be false in $s'$;
$o$ threats the preference also if it can make $\psi$ false, because $\phi$ could be true in $s'$.

% OPERATORE DI MINACCIA PER SOMETIME-AFTER %
\begin{mydef}\label{threato_sa1}
Given an operator $o$ and a sometime-after preference $P= \sf{SA}_{\phi, \psi}$ of a {\stripspp} problem, $o$ is a \textbf{threat} for $P$
if $o$ can make $\phi$ true or $\psi$ false.
% in the generated state $s'$ by its application.
\end{mydef}

% The set of sometime-after preferences of $\Pi$ which are threatened by 
% the operator $o$, are denoted with $T_{\mathcal{SA}}(o)$.

If the state $s'$ generated by an operator $o$ can make $\psi$ true but certainly can not
make $\phi$ true then $o$ is a potential support of $\sf{SA}_{\phi, \psi}$ because if the
preference is temporarily violated in $s'$, $o$ could make it satisfied again in the following
state.

% OPERATORE DI SUPPORTO PER MINACCIA SOMETIME-AFTER %
\begin{mydef}\label{support_sa}
Given an operator $o$ and a sometime-after preference $\sf{SA}_{\phi, \psi}$ of a {\stripspp} problem, $o$ is a \textbf{potential support} of $\sf{SA}_{\phi, \psi}$ if $o$ is not a threat of the preference and $o$ can make true $\psi$.
\end{mydef}

The set of sometime-after preferences of $\Pi$ which are potentially supported by 
the operator $o$ are denoted with $S_{\mathcal{SA}}(o)$.

\begin{mydef}\label{neutral_sa}
Given an operator $o$ and a sometime-after preference $\sf{SA}_{\phi, \psi}$ of a {\stripspp} problem, $o$ is a \textbf{neutral} for $\sf{SA}_{\phi, \psi}$ if $o$ is neither a threat nor a potential support of preference $\sf{SA}_{\phi, \psi}$.
\end{mydef}

{\bf Example}
An operator $o = \langle \top, \lnot a, b \rangle$ 
is a potential support of $\mathsf{SA}_{\phi_1, \psi_1}$,
a threat of $\mathsf{SA}_{\phi_2, \psi_2}$
and $\mathsf{SA}_{\phi_3, \psi_3}$
and neutral for $\mathsf{SA}_{\phi_4, \psi_4}$ where
$\phi_1 = c$ and $\psi_1 = \lnot a$,
$\phi_2 = \lnot a$ and $\psi_2 = b \lor d$,
$\phi_3 = \lnot d$ and $\psi_3 = b $,
and $\phi_4 = \lnot c$ and $\psi_4 = d$.



\section{Compilation of Qualitative Preferences}

In this section we describing the general compilation scheme of a {\stripspp} $\Pi$ problem. First we compile $\Pi$ into a problem with conditional effects (and possibly also disjunctive preconditions), which can then be compiled away obtaining a {\stripsp} problem equivalent to $\Pi$.
%
%some further definitions should be provided.
%
% ------------------
% OPERATORE NEUTRALE
% ------------------
%
\begin{comment}
\begin{mydef} \label{def:neutral-operators}
Given a {\stripspp} problem $\Pi$ if an operator $o$ of $\Pi$ is neutral for every given preference of $\Pi$ over
$\mathcal{A}$, $\mathcal{SB}$, $\mathcal{ST}$, $\mathcal{AO}$ and $\mathcal{G}$
then we say that $o$ is \textbf{neutral} for $\Pi$.
The set of all the neutral operator for $\Pi$ is denoted by $O_{\textit{neutral}}$.
\end{mydef}
\end{comment}
%AG: ho condensato le due definizioni iniziali in una frase, per guadagnare spazio
We will use $O_{\textit{neutral}}$ to denote the set of the problem operators that are neutral for all preferences, and $P_{\textit{affected}(o)}$ to denote the set of all problem preferences that are affected by an operator $o$.
% ------------------
% AFFECTED OPERATORS
% ------------------
\begin{comment}
\begin{mydef}
Given an operator $o$ of a {\stripspp} $\Pi$ problem the set $P_{\textit{affected}(o)}$ of prefereces affected by $o$ is defined as:
$$
P_{\textit{affected}(o)} =
\threatA \cup \threatSB \cup \supportSB \cup \threatAMO \cup \supportST.
$$
\end{mydef}
\end{comment}

%AG: formato compattato
Since the compilation of soft goals is the same as in \cite{kn:geffner09}, we omit its description and focus on the other types of preferences.
Moreover, we use a preprocessing step to filter out all preferences of type $\mathcal{A}$ and $\mathcal{SB}$ that are falsified in the initial state and all preferences of type $\mathcal{ST}$ that are satisfied in it.

 %AG: SPOSTARE NEL RELATED WORK: {\em The scheme proposed by Keyder and Geffner is considerable simpler than ours because it does not to consider the interaction between actions and preferences such as threats, supports and violators.}

%
% In order to simplify the compilation scheme we don't consider the compilation of soft goals because it can be easily added using the same method of Keyder and Geffner.
%AG: compattato fortemente per questioni di spazio
\begin{comment}
Given a {\stripspp} problem, an equivalent {\stripsp} problem can be derived by translation which has some similarities to what proposed by Keyder and Geffner for soft goals but also significant difference. The scheme proposed by Keyder and Geffner is considerable simpler than ours because it does not to consider the interaction between actions and preferences such as threats, potential supports and violators. In order to simplify the compilation scheme we don't consider the compilation of soft goals because it can be easily added using the same method of Keyder and Geffner.

Moreover we assume that every always and sometime-before preference are not violated and every sometime preference are not satisfied in the problem initial state $I$. Before starting the compilation, we carry out the following checks
in order to exclude some preferences from the process:
\begin{itemize}

\item for each $P = {\mathsf{A}_{\phi} \in \mathscr{P}_{\mathcal{A}}}$ we check that $I \models \phi$, if the condition
does not hold
we exclude $P$ from the compilation increasing the cost of the plan by $u(P)$ because it is already violated in $I$;

\item for each $P = {\mathsf{SB}_{\phi, \psi}} \in \mathscr{P}_{\mathcal{SB}}$ we check that $I \not \models \phi$, if the condition
does not hold
we exclude $P$ from the compilation increasing the cost of the plan by $u(P)$
because it is already violated in $I$; after that we check that 
$I \not \models \phi \land I \models \psi$, if the condition hold we exclude $P$ because it is already satisfied in $I$;

\item for each $P = {\mathsf{ST}_{\phi} \in \mathscr{P}_{\mathcal{ST}}}$ we check that $I \models \phi$, if the conditiom holds
we exclude $P$ because it is already satisfied in $I$;
\end{itemize}
\end{comment}


For a {\stripspp} problem $\Pi = \langle F,I,O,G,\mathscr{P},c,u \rangle$, the compiled problem of $\Pi$ is $\Pi' = \langle F,'I',O',G',c' \rangle$ where:

\begin{itemize}
    \item {$F' = F \cup V \cup D \cup C \cup \overline{C'} \cup \{\textit{\it normal-mode}, \text{\it end-mode}\}$};

    \item $I' = I \cup \overline{C'} \cup V_{\mathcal{ST}} \cup $ $S_{\mathcal{AO}}$  $ \cup \{\text{\it normal-mode}\}$;

    \item $G' = G \cup C'$;

    \item $O' = \{\text{\it collect}(P), \text{\it forgo}(P) \mid P \in \mathscr{P}\} \cup \{end\} \cup 
    \{{comp}(o, \mathscr{P}) \;\mid\; o \in O\}$

    \item$\text{\it forgo}(P) = \langle \{\text{\it end-mode}, P\text{\it-violated}, \overline{P'}\}, \{{P}', \lnot \overline{{P}'}\} \rangle$;

    \item$\text{\it collect}(P) = \langle \{\text{\it end-mode}, \lnot P\text{\it-violated}, \overline{P'}\}, \{{P}', \lnot \overline{{P}'}\} \rangle$;

    \item$\textit{end} = \langle \{ \text{\it normal-mode} \}, \{\text{\it end-mode}, \text{\it normal-mode}\} \rangle$;

    \item $\textit{comp}(o , \mathscr{P})$ is the function translating
    operator $o$ according to Definition \ref{compiledOcondeff};


    \item 
$ c'(o') =
\left\{
    \begin{array}{ll}
        u(P)  & \mbox{if } o' = \text{\it forgo}(P) \\
        % c(o) & \mbox{if } o \in O_{{comp}} \\
        c(o') & \mbox{if } o' = \textit{comp}(o , \mathscr{P}) \\
        0 & \mbox{otherwise};

    \end{array}
\right.
$
\end{itemize}

%\noindent where:

\begin{itemize}
    % CAMBIO DELL'INDICE
    % \item$PV = \cup_{i=1}^{k} \{P_i\text{\it-violated}\}, k = |P|$; 

    \item$V = \{P\text{\it-violated} \mid P \in \mathscr{P}\}$;

    \item$V_{\mathcal{ST}} =  \{P_i\text{\it-violated} \mid  P_i \in \mathscr{P}_\mathcal{ST}|\}$;
    % $\mathscr{P}_\mathcal{ST} \subseteq \mathscr{P}$; this is the set of the violator predicates restricted to the subset of the sometime preferences of all preferences $\mathscr{P}$;

    \item
    $S_{\mathcal{AO}} = \{P_i\text{-seen} \mid P_i \in \mathscr{P}_{\mathcal{AO}}(I)\}$, with
     $\mathscr{P}_\mathcal{AO}(I) 
    = \{P_i = \mathsf{AO}_{\phi} \mid P_i \in {\mathscr{P}}_{\mathcal{AO}}, I \models \phi\} $; 
    %this is the set of \textit{seen} predicates (see Section \ref{subsect:amo_comp}) for those at-most-once preferences such that $I \models \phi$;}

    % NON PIU' NECESSARIO CON GLI EFFETTI CONDIZIONALI
    % \item$D = \cup_{i=1}^n\{P_i\text{-}done\}$ where $n = |\mathscr{P}|$;

    \item$C'_{} = \{P' \mid P \in \mathscr{P}\}$ and $\overline{C'}_{} = \{\overline{P'} \mid P \in \mathscr{P}\}$;

    % NON PIU' NECESSARIO - VERSIONE SENZA EFFETTI CONDIZIONALI
    % \item $O_{comp} = O_{{\it neutral}} \cup O_{chained} \cup O_{violation}$

    % \item $O_{comp} = O_{\mathcal{N}} \cup O_{\overline{\mathcal{N}}}$;
    % \item $O_{\mathit{comp}} = O_{\mathit{comp}}^{N} \cup O_{\mathit{comp}}^{\overline{N}}$;

    % ALTERNATIVA O_{neutral/safe}
    % \item $O_{\mathit{comp}}^{{N}} = \{\langle \textit{Pre}(o) \cup \{\text{\it normal-mode}\}, \textit{Eff}(o)$\}$\rangle \mid o \in O_{\textit{neutral}}$;

    % NON PIU' NECESSARIO - VERSIONE SENZA EFFETTI CONDIZIONALI
    % \item $O_{chained}$ and $O_{violation}$ are the operator sets generated by the operator transformation schema applied to the operators of $\Pi$ that affect at least one preference of $\Pi$. An operator $o \in O$ is compiled through the compilation schema if $|I(o)| > 0$ otherwise it is considered neutral. Such set will be defined after presenting the general idea for compiling an operator affecting a preference.

    % \item  $O_{\mathit{comp}}^{\overline{N}}$ is the set of the compiled not neutral operator; the description of how these operators are compiled is shown in Definition \ref{compiledOcondeff}.

    % \item  $c_{\overline{\mathcal{N}}}$ \textbf{TODO}

    % \item the compiled operators $o_{chained}$ of the non-neutral operators are defined as:
    % $$
    % o_{chained} =\bigcup\limits_{o \in \mathit{NN}(\Pi)} chain(o, SI(o))
    % $$
    % where $SI(o)$ an arbitrary ordering of affected preferences by $o$ and $chain(o, SI(o))$ is a set of new compiled operators defined further down in following section \ref{comp_sect};

    % \item $c_{tv}(o)$ is the cost of an operator $o \not \in N(\Pi)$, or equivalently that $o \in O_{chained} \cup O_{violation}$ that we define after describing how exactly these sets are formed.


% $\textit{comp}(o , \mathscr{P}) = \\
%     \left\{
%     \begin{array}{ll}
%         \langle \textit{Pre}(o) \cup \{\text{\it normal-mode}\}, \textit{Eff}(o) \rangle & \mbox{if } o \in O_{{\it neutral}}\\
%         \langle \textit{Pre}(o) \cup \{\text{\it normal-mode}\}, \textit{Eff}(o) \cup 
%         {\displaystyle \bigcup_{P_i \in P_{\textit{affected}}(o)}{\mathcal{W}(o, P_i)}} \rangle & \mbox{if } o \in O - O_{{\it neutral}}\\
%     \end{array}
% \right.
% $



\end{itemize}

% The specific definition of $\mathcal{W}(o, P_i)$ for an operator that is not neutral for $\mathscr{P}$
% depends on the type of prefrence $P_i$ as specified in the following by Definition \ref{when_always}--\ref{when_atmostonce}

% \text{\it forgo} & \text{\it collect}
%\paragraph{{Forgo} and {Collect} Actions}
The {\it collect} and {\it forgo} actions can only appear at the end of the plan. For each preference $P$ the compilation of $\Pi$ into $\Pi'$ adds a dummy hard goal $P'$ that is false in the initial state $I'$;  $P'$ be achieved either by action $\text{\it collect}(P)$, that has cost 0 but requires $P$ to be satisfied,
%(i.e. $\pviol$ is false in the goal state for all kinds of preferences possibly except for sometime), 
or by action $\text{\it forgo}(P)$, that has cost equal to the utility of $P$ and can be performed only if $P$ is false ($\pviol$ is true in the goal state). For each $P$, exactly one of $\text{\it collect}(P)$ and $\text{\it forgo}(P)$ appears in the plan. 

% [CHECK! NUOVO PARAGRAFO COMPATTO]
The $P_i\textit{-violated}$ literals in the compiled initial state $I'$ are used to consider any $\mathcal{ST}$ preference violated until a operator supporting it is inserted into the plan; the $P_i\textit{-seen}$ literals in $I'$ are necessary to capture the violation of any $\mathcal{AO}$ preference when an operator makes the preference formula true for the second time in the state trajectory.

Function $ \textit{comp}(o , \mathscr{P}) $ transforms an original operator $o$
into the equivalent compiled operator $o'$ with an additional preconditions forcing it to appear before the 
$\text{\it forgo}$ and $\text{\it collect}$ operators. Regarding the effects of $o'$, if $o \in O_{\textit{neutral}}$, they are the same of $o$;
otherwise, $\textit{comp}(o , \mathscr{P})$ extends the effects of $o$ in $o'$ with a set of conditional effects for each preference affected by $o$.
%
The definition of such additional effects depend on the type of the affected preference and on how $o$
interacts with it; this is detailed below.
% Subsections \ref{compilation_always}--\ref{subsect:amo_comp}
%we will detail the exact definition of for each class of preference.



\begin{comment}
\paragraph{Operator Compilation Function}
The function $ \textit{comp}(o , \mathscr{P}) $ which transforms an original operator $o$
into the equivalent compiled one is splitted in two parts. If the operator is neutral 
($o \in O_{\textit{neutral}}$) then the function just extends $\textit{Pre}(o)$
with the predicate $\text{\it normal-mode}$ in order to scorporate the execution of the domain operators and
the evaluationof $\text{\it forgo}$ and $\text{\it collect}$ actions at the end of the planning.

If the operator $o$ is not neutral ($o \in O - O_{\textit{neutral}}$) where $|P_{\textit{affected}}(o)| = n$,
then the compilation function $\textit{comp}(o , \mathscr{P})$ extends its effects,
for each affected preference $P_i$, by adding a set of conditional effects denoted with 
$ \mathcal{W}(o, P_i) $ whose definition depends by $o$,
the class of preference $P_i$ belongs to and the way how $o$
interacts with $P_i$. In Subsections \ref{compilation_always}--\ref{subsect:amo_comp}
we will detail the definition of $ \mathcal{W}(o, P_i) $ for each class of preference.

But we want a compiling problem which belongs to {\strips+} class and so
we have to compile away the conditional effects (see Section \ref{subsection:conditional-effect}).
\end{comment}

% COPIA FORMA GENERICA conditional effects capitolo introduttivo!
%     It will be explained in the next section because $n$, the number of affected preferences, can differ to $n'$ the number of additional conditional effects.
% }, where $c_i$ is a formula that, when satisfied in the state where $o$ is applied, makes the formula $e_i$ an effect of $o$. 
% If the operator is not neutral ($o \in O - O_{{\it neutral}}$), 
% which means that $o$ affects $n$ preferences $P_{\textit{affected}}(o) = \{ P_1, ..., P_n\}$, then
% it can be compiled by adding a number of conditional effects \textit{(when $c_i(o, P_i)$\, $e_i(o, P_i)$)}, for $i = 1\dots n'$\footnote{
%     It will be explained in the next section because $n$, the number of affected preferences, can differ to $n'$ the number of additional conditional effects.
% }, where $c_i$ is a formula that, when satisfied in the state where $o$ is applied, makes the formula $e_i$ an effect of $o$. 
% % The exact Definition of $c_i$ and $e_i$
% % depend on the class of preference $P_i$ belongs to which
% % are specified below.

% In detail the compilation function $\textit{comp}(o , \mathscr{P})$ extends,
% for each affected preference by the execution of $o$, 
% with a set of conditional effects $ \mathcal{W}(o, P_i) $
% whose definition depends on $o$, the type of affected preference and the way
% in which $o$ interacts with the preference. In the following subsections we will detail,
% for each type of preference, the definition of function $ \mathcal{W}(o, P_i) $. 

% \textit{comp}(o , P)

\begin{comment}
% ESTENSIONE STATO INIZIALE
\paragraph{Extension of the initial state}
Note that the original initial state $I$ is extended to $I'$ with the set of literals $V_{\mathcal{ST}}$, which contains the literal $P_i\textit{-violated}$ for each sometime preference of the problem. The literal $P_i\textit{-violated}$ states that the related preference $P_i$ is (temporarily) violated in $I$ until a support operator for $P$ is applied.

Furthermore the original initial state $I$ is extended with the set of literals $S_{\mathcal{AO}}$
which contains, for each at-most-once preference $P_i = {\sf AO}_{\phi_i}$ of the problem such that
$I \models \phi_i$, the literal $\textit{seen-}P_i$. This additional fluent
$\textit{seen-}P_i$ states that the formula $\phi_i$, involved in $P_i$, is satisfied in $I$. This precaution is necessary to correctly capture any possible violators of at-most-once preferences.
\end{comment}

% We now present the transformation of not neutral operators that affects a set of preferences i.e. the operators in
% $O - O_{{neutral}}$.

% and the will specified in the following section.
% Condition $c_i$ can be derived by regression of $P_i$ over $o$ (it is analogous to precondition $l_1 \wedge \ldots \wedge l_q$ in the $\overline{o}$-operators of Definition \ref{def:operatorcomp}).
% There are two alternatives to do this,
% one leads to an exponential blow up of the operators 
% {(in our case $O(2^n)$ for each operator affecting $n$ preferences)} \cite{gazen97:ecp} and the other generates a linear number of operators but increase
% polynomially the plan lenght \cite{compilationnebel}.\\
% \redtext{\textbf{NOTA}: non e' proprio $O(2^n)$, sarebbe piu' preciso $O(2^m)$ dove $m$ dipende dal numero e dal tipo di preferenze influenzate da $o$.}
% We use a variant of the linear compilation optimized in attempt to make it more efficient for the planner 
% Our compilation scheme is based on the use of conditional effects but since an original operator can affects many preferences (e.g., $k=20$), we used a slight different of what proposed by Nebel to transform conditional effects generating only a linear number of operators ($2k$ instead of $2^k$). We describe in detail our compilation of conditional effects in section [...].
% In definition \ref{compiledOcondeff} we describe how a not neutral operator $o$ is compiled. 

% --------------------------------------------------------
% DEFINIZIONE OPERATORE COMPILATO CON EFFETTI CONDIZIONALI
% --------------------------------------------------------
\begin{mydef} \label{compiledOcondeff}
Given an operator $o$ the corresponding compiled operator is defined using the following function:
% whose execution affects the set of preferences indicated as $\affectedd = \{P_1, P_2, ..., P_N \}$, the corresponding compiled operator is defined as

% $\textit{comp}(o , \mathscr{P}) = \\
%     \left\{
%     \begin{array}{ll}
%         \langle \textit{Pre}(o) \cup \{\text{\it normal-mode}\}, \textit{Eff}(o) \rangle & \mbox{if } o \in O_{{\textit neutral}}\\
%         \langle \textit{Pre}(o) \cup \{\text{\it normal-mode}\}, \textit{Eff}(o) \cup 
%         {\displaystyle \bigcup_{P_i \in P_{\textit{affected}}(o)}{\mathcal{W}(o, P_i)}} \rangle & \mbox{if } o \in O - O_{{\textit neutral}}\\
%     \end{array}
% \right.
% $
$ 
{\textit{Pre}}(o') = {\textit{Pre}}(o) \cup \{\text{\it normal-mode}\}
$

$
{\textit{Eff}}(o') = {\textit{Eff}}(o) \cup 
{\displaystyle 
\bigcup_{P_i \in P_{\textit{affected}}(o)}{ \mathcal{W}(o, P_i)}
}
$

\noindent where
$\mathcal{W}(o, P_i)$ is the set of conditional effects concerning the affected preference $P_i$ (if any). 
%If $o \in O_\text{neutral}$ then $\textit{Eff}(o') = {\textit{Eff}(o)}$.

\end{mydef}

% Hereafter, given an operator $o$, we will denote with $o'$ the compiled operator, with $s$ the state where $o'$ is applied and with $s'$ the state resulting from the application of $o'$.



% -------------------
% COMPILAZIONE ALWAYS
% -------------------

\noindent
In the following $\phi_i$ denotes a clause of $\phi$, $\psi_i$ denotes a clause of $\psi$, and a set of formulas is interpreted as their conjunction.

\subsection{Conditional Effects for $\mathcal{A}$  Preferences}
\label{compilation_always}

%AG:  1) NUOVO TITOLO DELLA SOTTOSESSIONE... UNIFORMARE GLI ALTRI... 2) CHECK MI PARE CHE DOPO USIAMO SOLO $\overline{AA}(o)$. HO DI CONSEGUENZA MODIFICATO E SEMPLIFICATO *MOLTO* IL TESTO SEGUENTE]
\begin{comment}
Before defining the extending effects used to compile an operator $o$ which affects an always preference, we introduce some useful notation in order to simplify the formalisation. For an operator $o$ and a preference clause $\phi_i$:
\begin{itemize}
\item  $\mathit{NA}(o)_{\phi_i}= \{l_j \in L(\phi_i) \mid \neg{\l_j} \in ({\textit{Eff}}(o)^+ \cup {\textit{Eff}}(o)^-) \}$ is the set of literals in $L(\phi_i)$ falsified by the effects of o; 
\item  $AA(o)_{\phi_i} = L(\phi_i) \setminus \mathit{NA}(o)_{\phi_i}$ is the set of literals in $L(\phi_i)$ \emph{not} falsified by the effects of o;
\item  $\overline{AA}(o)_{\phi_i}$ is the literal-complement set of $AA(o)_{\phi_i}$.
\end{itemize}
\end{comment}

%We now present the transformation of operators that threaten or violate a preference in class $\mathcal{A}$. 
%[FP: compilazione delle preferenze always con nuova notazione]

The conditional effects for a compiled operator affecting a preference $ \sf{A}_{\phi}$ are defined as follows, where $\overline{AA}(o)_{\phi_i}$
is the literal-complement set of the  subset of literals in $L(\phi_i)$ that are \emph{not} falsified by the effects of $o$.

\begin{mydef} \label{when_always}
Given a preference $P = \sf{A}_{\phi}$ and an operator $o$ affecting it,
the conditional effect set $\mathcal{W}(o, P)$ in the compiled version $o'$ of $o$
(according to Definition \ref{compiledOcondeff}) is:
$$\mathcal{W}(o, P) = 
\begin{cases}
\{\textit{when} \; (\textit{cond}(o, P)) \; (P\textit{-violated})\}\\
\;\;\;\;\;\;\text{if}\;o\;\text{is a threat of }P\\
% \{\textit{when} \; (\textsc{true}) \; (\pviol)\}\\
\{\textit{when} \; (\top) \; (P\textit{-violated})\}\\
\;\;\;\;\;\;\text{if}\;o\;\text{is a violator for }P\\
\end{cases}$$
where $cond(o, P) =
%\begin{itemize}
%\item  
%$cond(o, {\sf{A}_{\phi}}) = 
\;\;\;\bigvee_{\phi_i\,\in\,TC(o, P)}(l_1 \wedge \ldots \wedge l_q)$\\
and $\{l_1, \ldots, l_q\}=\overline{AA}(o)_{\phi_i}$.
%\end{itemize}
\end{mydef}

For each affected preference $P= \sf{A}_{\phi}$, $o'$ contains a conditional effect $\pviol$ with a condition
depending on how  $\sf{A}_{\phi}$ is affected: if $o$ is a violator, then the condition is always true;
% (i.e. it is the special literal
% {\truettt} that holds in every state).
if $o$ is a \textit{threat}, the condition
checks that there exists at least a clause of ${\phi}$ of that is certainly false
in $s'$ -- this is the case if there is at least a 
threatened clause whose literals that are not falsified in $s'$
are false in $s$.

\noindent \textbf{Example}. Consider $o =\langle \top, \{a, \lnot c\} \rangle$ 
and preference  ${\sf A}_{\phi}$ with $\phi = (a \lor b) \land (c \lor d \lor e) $. The second clause of $\phi$  is threatened by $o$, and
$cond(o, {\sf{A}_{\phi}}) = \overline{AA}(o)_{c \lor d \lor e} = \{\lnot d, \lnot e\}$.

%[CHECK! AG: ESEMPIO ABBREVIATO: TOGLIERE IL RESTO!?]
 \begin{comment}
 According
to Definition \ref{when_always}, we have to define a condition
$\textit{cond}(o, \sf{A}_{\phi}))$ which has to check that exists at least a threatened clause
will be certainly false in $s'$. In this case we have a single threatened clause
and we have to build a condition to checks if those literal of this clause which
are not falsified in $s'$, i.e. $\overline{AA}(o)_{c \lor d \lor e} = \{d, e\}$, are
already false in $s$. So we have that the condition which captures the violation of ${\sf A}_{\phi}$ 
is: 
$\textit{cond}(o, {\sf A}_{\phi}) = \{\lnot d, \lnot e\}$.
% the compiled operator $o'$ is defined as
% $o' = \langle \textit{Pre}(o), \textit{Eff}(o) \cup \mathcal{W}(o, P) \rangle$ where
% $\mathcal{W}(o, P) = (\textit{when} ())$

% ---------------------
% COMPILAZIONE SOMETIME
% ---------------------
\end{comment}

\subsection{Conditional Effects for $\mathcal{ST}$  Preferences}

The conditional effects for a compiled operator affecting a preference $\sf{ST}_{\phi}$ are defined as follows.
% We now present the transformation of an operator that is a potential supports of a preference $P$ in class $\mathsf{ST}$. 
\begin{mydef} \label{when_sometime}

Given a preference $P = \sf{ST}_{\phi}$ and an operator $o$ that potentially supports it,
the conditional effect set in the compiled version $o'$ of $o$
(according to Definition \ref{compiledOcondeff}) is: 
$$
\mathcal{W}(o, P) = \{
\text{when} (\text{cond}(o, P))\text{ } (\lnot \pviol )\}$$
where $\textit{cond}(o, P) = 
\{{\phi_i}{ \; \mid \; \phi_i \not \in {C}_{\phi}(o)}\}
$.
% \item  $\textit{eff}_{\mathcal{S}}(o, P) = $ 
\end{mydef}

As described above, for each $P = \sf{ST}_{\phi}$
$\pviol$ holds in the compiled initial state, and
% (if $\phi$ does not hold in the original problem initial state
% (otherwise the preference is considered satisfied and therefore not compiled).
%These new violation predicates could be falsified
%by the potential supports, which are operators whose effects could make $\phi$ true in $s'$.
a potential support $o$ of $P$ makes $\phi$ true when
 all clauses $\phi_i$ of $\phi$ not in ${C}_{\phi}(o)$
%$ { \phi_i \not \in {C}_{\phi}(o)} $ 
hold in $s$, where ${C}_{\phi}(o)$ is the set of clauses of $\phi$
that will be certainly true in $s'$. If this condition holds in $s$, then $o'$ falsifies it in $s'$.


% In order to satisfy a given preference $\sf{ST}_{\phi}$, which 
% is temporarily violated in $I'$,
% a potential support $o$ for $P$ has to be inserted in the plan
% and applied in a state where ${(cond}_{\mathcal{S}}(o, P))$ holds,
% making an actual support for $P$.
% Otherwise, operator $o$ 
% behaves as a neutral operator for $P$, leaving the preference 
% violated in $s'$.


% ----------------------------
% COMPILAZIONE SOMETIME-BEFORE
% ----------------------------
\subsection{Conditional Effects for $\mathcal{SB}$ Preferences} \label{subsection:sometime_before_compilation}

The conditional effects for a compiled operator affecting a preference $\sf{SB}_{\phi, \psi}$  are defined as follows.
%We now present the transformation of operators that potentially support or threat a preference in $\mathcal{SB}$. 

\begin{mydef} \label{when_sometimebefore}
Given a preference $P = \sf{SB}_{\phi, \psi}$ and an operator $o$ affecting it,
the conditional effect set $\mathcal{W}(o, P)$ in the compiled version $o'$ of $o$
(according to Definition \ref{compiledOcondeff}) is:
$$
\mathcal{W}(o, P) = 
\begin{cases}
\{\textit{when} \; (\textit{cond}_{{S}}(o, P)) \; (\textit{seen-}\psi)\} \;
\\\;\;\;\;\;\;\text{if}\;o\;\text{is a potential support of }P\\
\{\textit{when} \; (\textit{cond}_{{T}}(o, P)) \; (\pviol)\} \;
\\\;\;\;\;\;\;\text{if}\;o\;\text{is a threat of }P
%\{
%\textit{when} \; (\textit{cond}_{{S}}(o, P)) \; (\textit{seen-}\psi),\\
%\ \ \textit{when} \; (\textit{cond}_{{T}}(o, P)) \; (\pviol)
%\} 
%\\\;\;\;\;\;\;\text{if}\;o\;\text{is a both a threat and support for }P\\
\end{cases}$$
where:
\begin{itemize}
\item  $\textit{cond}_{{S}}(o, P) = 
% \{\bigcup_{\psi_i \in \overline{C}_{\psi}(o)} \psi_i\} = \{ { \psi_i \in \overline{C}_{\psi}(o)} \}
\{
\psi_i \;\mid\; \psi_i \not \in {C}_{\psi}(o)    
\}
$
% \item  $\textit{eff}_{\mathcal{S}}(o, P) = \textit{seen-}\psi$ 
% \end{itemize}
% and:
% \begin{itemize}
\item  $\textit{cond}_{{T}}(o, P) = 
\{\lnot \textit{seen-}\psi\} \cup \{
\phi_i \;\mid\; \phi_i \not \in {C}_{\phi}(o) \}    
% \{\lnot \textit{seen-}\psi\} \cup \{ { \phi_i \in \overline{C}_{\phi}(o)} \}
$.
% \item  $\textit{eff}_{\mathcal{T}}(o, P) = \pviol$ 
\end{itemize}

% -------
% RIMOSSO
% -------
% and:
% \begin{align*}
% W_{\mathcal{ST}}(o, & P) = \{  \\
% & \text{when (cond}_{\mathcal{S}}(o, P))\text{ (eff}_{\mathcal{S}}(o, P))\\
% & \text{when }\text{ (cond}_{\mathcal{T}}(o, P))) \text{ (eff}_{\mathcal{T}}(o, P))
% \}
% \end{align*}

\end{mydef}


%The definition of the effects used to extend the effect of an operator $o$ which affects a sometime-before preference $P$ in 
%Definition \ref{when_sometimebefore} depends on the class of operators which $o$ belongs. We have to distinguish 
%if $o$ is a potential support, a threat or both for $P$. 


An operator $o$ affecting a preference $P = \sf{SB}_{\phi, \psi}$ can behave as a (a) {potential support} of $P$,
(b) a threat of $P$, or (c) both. These case are captured by the three conditional effects of $o'$ in Definition \ref{when_sometimebefore}.

In case (a), if all clauses that are not certainly true in $s'$ (i.e., $\textit{cond}_{{S}}(o, P)$) hold in $s$, then $\psi$ is true in $s'$, and
$o'$ keeps track of this by making $\textit{seen-}\psi$ true.
In case (b), if $\psi$ has never been true in the state-trajectory up to $s$ and all clauses of $\phi$ that are not certainly true (i.e., $\textit{cond}_{{T}}(o, P)$) hold in $s$, then $P$ is violated by $o$ and $o'$ has effect $P${\it -violated}. In case (c), if the conditions of both conditional effects hold, $P$ is violated because $\psi$ is made true simultaneously with $\phi$.

\begin{comment}
Recalling Definition \ref{def:becometrue}, an operator $o$ could make true a formula $\phi$
when there exists some clauses of $\phi$ that will surely be true in the state resulting
from the application of $o$. So, if all $ { \psi_i \not \in {C}_{\psi}(o)} $ hold in $s$ 
% (where $\overline{C}_{\psi}(o)$ is the set of clauses of $\psi$ that
% are not surely true in $s'$ - see Definition \ref{def:trueclauses})
, i.e. $\textit{cond}_{{S}}(o, P)$ holds in $s$,
then $\psi$ will be true in $s'$ and then we have to keep track of this fact by
making the predicate $\textit{seen-}\psi$ true.
% in the effects $\textit{Eff}_{\mathcal{T}}(o, P)$ of $o'$.
\end{comment}

% RIDONDANTE
% In order to check if $\psi$ will be true in the resulting state $s'$, we have to check
% the preconditions of the compiled operator to verify that the clauses, which do not belong to
% the set of clauses $C_{\psi}(o)$ certainly true in $s'$, are already true in $s$. This
% condition is specified in $\textit{cond}_{\mathcal{S}}(o, P)$ with the expression
% $\{ { \psi_i \in \overline{C}_{\psi}(o)} \}$. If this condition holds then the effect 
% $\textit{eff}_{\mathcal{S}}(o, P) = \{\textit{seen-}\psi\}$ is made true is $s'$.

\begin{comment}
The compilation if $o$ is threat of $P$ is similar. A threat of $\mathsf{SB}_{\phi, \psi}$
behaves as violators 
in the case that the operator makes $\phi$ true in $s'$
% specifing in $\textit{cond}_{\mathcal{T}}(o, P)$ the expression $\{ { \phi_i \in \overline{C1}_{\phi}(o)} \}$)
(i.e., when all $\phi_i \not \in {C}_{\phi}(o)$ holds in $s$,
which is the condition of $\textit{cond}_{{T}(o, P)}$), and
that the $\psi$ has never been made true in the states preceeding $s$. If 
both these
conditions, specified in $\textit{cond}_{{T}}(o, P)$, hold in $s$ then predicate
$P{\it-violated}$ is included in the effects of $o'$.

We have also to consider the case in which an operator $o$ is both a threat and a potential support of $P$. In this 
case $o$ can behaves in the following ways: making $\phi$ true in $s'$, making $\psi$ true in $s'$
and making both $\phi$ and $\psi$ true in $s'$. In order to handle these situations,
the compiled operator $o'$ contains both conditional effects of $o$ as threat of $P$ and
as support of $P$. Note that this correctly captures the violators of $P$ determined by $\phi$
and $\psi$ becoming simultaneously true by execution of $o$.
\end{comment}

% DA FARE PROX
% \subsection{Compilation of a Sometime-After Preference}


% -------------------------
% COMPILAZIONE AT-MOST-ONCE
% -------------------------
\subsection{Conditional effects for $\mathcal{AO}$ Preferences} \label{subsect:amo_comp}

The conditional effects for a compiled operator threatening  preference $\sf{AO}_{\phi}$ preference are defined as follows.
 
 \begin{mydef} \label{when_atmostonce}

Given a preference $P = \sf{AO}_{\phi}$ and an operator $o$ that threats  $P$,
the conditional effect set $\mathcal{W}(o, P)$ in the compiled version $o'$ of $o$
(according to Definition \ref{compiledOcondeff}) is:
\begin{align*}
\mathcal{W}(o,&P) = \{ 
\text{when } (\text{cond}_{{N}}(o, P))\text{ }(\textit{seen-}\phi))\\
&
\;\;\;\;\;\;\;\;\;\;\;
 \text{when } (\text{cond}_{{T}}(o, P)))\text{ }(\pviol )
\}
\end{align*}
where:
\begin{itemize}
\item  $\text{cond}_{{N}}(o, P) = \{\lnot \textit{seen-}\phi\} \cup \{ { \phi_i \mid \phi_i \not \in {C}_{\phi}(o)} \}$
% \item  $\text{eff}_{\mathcal{N}}(o, P) =  $
\item  
% \begin{align*}
$
\text{cond}_{{T}}(o, P) = \{\textit{seen-}\phi\} \cup \{ { \phi_i \mid \phi_i \not \in {C}_{\phi}(o)} \}\; \cup $

$
%\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;
\;\;\;\;\;\;
\{ \bigvee_{ \phi_i \in {C}_{\phi}(o)} (\lnot l_1 \land ... \land \lnot l_q) \mid  \{l_1, ..., l_q\} = {L}(\phi_i)\}
$.
% \end{align*}

% \item  $\text{eff}_{\mathcal{V}}(o, P) = \pviol $

% \item  $\mathcal{R}_{\phi}(o, P) = $

\end{itemize}
\end{mydef}

If an operator $o$ affecting $P = \sf{AO}_{\phi}$ makes $\phi$ true for the first time in the state trajectory
(i.e., $\text{cond}_{{N}}(o, P)$ holds in $s$), then the first conditional effect of $o'$ keeps track that $\phi$ has become true.
Otherwise, if (1) $\phi$ was true in any state before $s'$, (2) the execution of $o$ in $s$ makes $\phi$ true in $s'$, and (3) $\phi$ was false before (i.e., the three condition sets in $\text{cond}_{{T}}(o, P)$), then $o$ violates $P$ and $o'$ has effect $\pviol$.

\begin{comment}
The semantic of a preference in class $\mathcal{AO}$ (Figure \ref{fig:semantics}) requires that a preference
$P = \sf{AO}_{\phi}$ is satisfied by the state-trajectory generated by a 
plan $\pi$ if $\phi$ \textit{becomes} true in a state $s'$ at most once during the execution of the plan. A
formula $\phi$ becomes true in a state $s'$ due to the execution of an operator $o$ applied in a state $s$ iif 
$s \models \lnot \phi$ and $s' \models \phi$.

If $\phi$ holds in the problem initial state $I$, then it is required that
either it stay true until the end of the plan. or it becomes false at some
successor state of $I$ in the state-trajectory generated by the plan and it then stays
always false.

% If $I \models \lnot \phi$ then an operator 
% $o$ can make $\phi$ true in the resulting state for the first time in the state-trajectory,
% leaving the preference satisfied. In this case $o$ behaves as a neutral operator for $P$
% but if in a succeeding state another operator $o'$ make $\phi$ false then $o$ could behave as
% a violation for $P$ if it applied again. So, an operator that can make true $\phi$ is
% generally considered as a threat to $P$ because,
% as we have described, under certain conditions it could act as a violation.
If an operator $o$ can make $\phi$ true for the first time in the plan  trajectory
of a plan, then it behaves as a neutral operator for $P$. On the other hand, if $o$
can make $\phi$ true after having been true and become false in past states of the trajectory,
then $o$ behaves as a threat of $P$.

In order to correctly capture the possible violators of $P$,
the set of effects extending $o$ $\mathcal{W}(o,P)$ has two conditional effects. The
first one is a neutral effect that is used to catch the behavior
of $o$ as neutral operator for $P$. Condition 
$\text{cond}_{{N}}(o, P)$ requires that $\phi$ has been never changed truth value
from true to false in preceeding state using the negated predicate
$\lnot \textit{seen-}\phi$ and that $\phi$
will be true in $s'$ using the condition $\{ { \phi_i \mid \phi_i \not \in {C}_{\phi}(o)} \}$
similarly to what done in the compilation schemes previously presented. If the conditions 
specified in $\text{cond}_{{N}}(o, P)$ hold in 
the state $s$ where $o$ is applied,
then we take into account that $\phi$ becomes true in $s'$ for the first time stating in the effects of 
the neutral conditional effect predicate $\textit{seen-}\phi$.

% \bluetext{
    The second conditional effect is a violating effect that 
    is used to catch the the behavior of of $o$ as a threat of $P$. This
    happens when the following conditions, specified in $\text{cond}_{{T}(o, P)}$ hold:
    (specified in $\text{cond}_{\mathcal{V}(o, P)}$):
    \begin{itemize}
        \item  $\phi$ has alreay been made true in a state preceding s; this is
        expressed by the predicate $\textit{seen-}\phi$ when $o$ is applied;
        \item  $\phi$ is made true in the resulting state $s'$; this is guaranteed by the conditions
        $\{ { \phi_i \mid \phi_i \not \in {C}_{\phi}(o)} \}$;
        \item  $\phi$ is false in the state $s$ where $o$ is applied; this is specified by requiring that at least
        a clause in $C_{\phi}(o)$ is false in $s$. 
    \end{itemize}
% }


% \bluetext{
    If all these conditions hold in the state where $o$ is applied then $P$ will be violated in the state resulting from
    the application of $o$.
% }

\end{comment}


\subsection{Conditional effects for $\mathcal{SA}$ Preferences} \label{subsect:sa_comp}

The conditional effects for a compiled operator affecting  preference $\sf{SA}_{\phi}$ preference are defined as follows.
 
\begin{mydef} \label{when_sometimeafter}

Given a preference $P = \sf{SA}_{\phi}$ and an operator $o$ that affects  $P$,
the conditional effect set $\mathcal{W}(o, P)$ in the compiled version $o'$ of $o$
(according to Definition \ref{compiledOcondeff}) is:
$$
\mathcal{W}(o, P) = 
\begin{cases}
\{\textit{when} \; (\textit{cond}_{T}(o, P)) \; (\pviol)\} \;
\\\;\;\;\;\;\;\text{if}\;o\;\text{is a threat on }\phi \text{ of }P\\
\{\textit{when} \; (\textit{cond}_{{S_{}}}(o, P)) \; (\lnot \pviol)\} \;
\\\;\;\;\;\;\;\text{if}\;o\;\text{is a support of }P\\
%\{
%\textit{when} \; (\textit{cond}_{{S}}(o, P)) \; (\textit{seen-}\psi),\\
%\ \ \textit{when} \; (\textit{cond}_{{T}}(o, P)) \; (\pviol)
%\} 
%\\\;\;\;\;\;\;\text{if}\;o\;\text{is a both a threat and support for }P\\
\end{cases}$$
where:
\begin{itemize}
% \item  $\textit{cond}_{{T_{\phi}}}(o, P) = 
% % \{\bigcup_{\psi_i \in \overline{C}_{\psi}(o)} \psi_i\} = \{ { \psi_i \in \overline{C}_{\psi}(o)} \}
% \{
% \phi \;\mid\; \phi_i \not \in {C}_{\phi}(o)
% \} \cup \{\bigvee_{\phi_i\,\in\,TC(o, P)}
% $
% $
% (l_1 \wedge \ldots \wedge l_q)\}$ and $\{l_1, \ldots, l_q\}=\overline{AA}(o)_{\phi_i}$;

\item  $\textit{cond}_{{T_{}}}(o, P) = 
\{ R(o, \phi),  R(o, \lnot \psi)\}$

% \item  $\textit{cond}_{{T_{\psi}}}(o, P) = 
% \{ R(o, \psi), \lnot R(o, \phi)\}$
% \item  $\textit{cond}_{{T_{\psi}}}(o, P) = 
% \{\bigvee_{\psi_i\,\in\,TC(o, P)}
% $
% $
% (l_1 \wedge \ldots \wedge l_q)\}$ and $\{l_1, \ldots, l_q\}=\overline{AA}(o)_{\psi_i}$;


% \item  $\textit{cond}_{S}(o, P) = 
% \{ \psi_i \;\mid\; \psi_i \not \in {C}_{\psi}(o)\}$


\item $\textit{cond}_{S}(o, P) = \{R(o, \psi)\}
$
% \item $
% R(o, \phi) = 
% \begin{cases}
% \top \text{ if } \;\forall\; \text{ clause } \phi_i \text{ of } \phi, \; |Z(o) \cap {L}(\phi_i) | > 0 \text{ }  \\
% \bigwedge_{\phi_i \in TC(o, \phi)} (l_1 \lor ... \lor l_q) \text{ where } \{l_1, ..., l_q\}\\
% \;\;\;\text{is the set of literals of}\,\phi_i\,\text{not falsified by o}.
% %\;\;\;\;\;\;\;\;(l_1, ..., l_q) = L(\phi_i) \setminus \{\overline{L}(o) \cap Z(o)\}
% \end{cases}
% $
\hl{CHECK: versione corretta di R(o, $\phi$?)}
\item $
R(o, \phi) = 
\begin{cases}
\top \text{ if } \;\forall\; \text{ clause } \phi_i \text{ of } \phi, \; |Z(o) \cap {L}(\phi_i) | > 0 \text{ }  \\
\bigwedge_{\phi_i \not \in C_{\phi}(o)} (l_1 \lor ... \lor l_q) \text{ where } \{l_1, ..., l_q\}\\
\;\;\;\text{is the set of literals of}\,\phi_i\,\text{not falsified by o}.
%\;\;\;\;\;\;\;\;(l_1, ..., l_q) = L(\phi_i) \setminus \{\overline{L}(o) \cap Z(o)\}
\end{cases}
$
% \item  $\textit{eff}_{\mathcal{T}}(o, P) = \pviol$ 
\end{itemize}
\end{mydef}

An operator $o$ affecting a preference $P = {\sf SA}_{\phi, \psi}$ can behave as
(i) a threat of $P$ or (ii) a potential support of $P$. In
case (i) the effect condition captures the fact that $o$ generates a state $s'$ where $\phi$ is true and $\psi$ is false, (temporarily) violating $P$. In case
(b) the condition captures the fact that $o$ generates $s'$ in which $\psi$ is true, and so $s'$ cannot violate $P$.
%without requiring any condition on $\phi$ because in state where $\psi$ is true $P$ is achieved.
% ----------------------------
% SEZIONE EFFETTI CONDIZIONALI
% ----------------------------



\subsection{Compilation equivalence}

It can be proved that the original planning problem with preferences has a solution plan with a certain cost (sum of its action costs and of the violated preference costs) if only if the compiled plan has a solution with the same cost.

\begin{prop}
Let $\Pi'$ be the compiled problem with conditional effects of a {\stripsp} problem $\Pi$. From any plan $\pi$ solving $\Pi$ we can derive a plan $\pi'$ solving $\Pi'$, and viceversa, such that the costs of $\pi$ and $\pi'$ are the same.
\end{prop}

\begin{proof}(Sketch).
The proof has the same structure of the plan-correspondence proof for Keyder and Geffner's compilation of soft goals \cite{kn:geffner09}, with $\pi' = \langle \pi'', end, \pi''' \rangle$ in which $\pi''$ is obtained from $\pi$ by replacing the original operators with the compiled ones involving conditional effects, and the rest of $\pi'$ defined as in Keyder and Geffner's proof ($\pi'''$ involves only {\it collect} and {\it forgo} actions). Since the conditional effects in $\pi''$ affect only the additional fluents of the compiled problem, all original operator preconditions remain satisfied in the state trajectory of $\pi''$. Moreover, by construction of the conditional effects for the compiled operators, it can be proved that in the state where $\textit{end}$ is applied, for each preference $P$, $P\textit{-violated}$ holds if and only if $P$ is violated in $\pi$. Viceversa, from a valid plan $\pi'$ we can obtain a plan for $\pi$ by replacing the compiled operators with their original version, and removing $\textit{end}$  and all {\it collect}/{\it forgo} actions.
\end{proof}



\subsection{Compilation of Conditional Effects} \label{subsection:conditional-effect}


According to the described compilation schema, each compiled operator $o'$ of an original operator $o$ affecting $n$ preferences
has $m \leq 2n$ conditional effects. 

In the literature, there are two main general methods for compiling away conditional effects.  In the first method by Gazen and Knoblock's \cite{gazen1997combining}, each plan of the compiled problem preserves the length of the corresponding plan for the original problem, but an exponential number of compiled operators are generated (in our  context $O(2^m)$ operators for each $o'$).
In the second method by Nebel \cite[see proof of Theorem 20]{nebel2000compilability}, a polynomial number of new operators are generated, but each plans for the compiled problem increases polynomially the length. 

In our context, we use Nebel's method because, depending on the operators' structure and the input preferences,
many conditional effects can be generated, making the other approach impractical. Moreover, Nebel's method can be optimised for our conditional effects because of their particular structure. In particular \hl{conflicts can not arise between the effects of conditional effects referring to different preferences, while those referring to the same preference can be resolved by imposing an order of evaluation.}. 
% DARE SPIEGAZIONE DI QUALI SONO LE PROPRIETA' STRUTTURALI DEI COND EFFECTS CHE SFRUTTIAMO.

This allows us to simplify the compilation by omitting the so called ``copy-operators" of Nebel's method. Another optimization concerns the ordering of the set of operator pairs ``activating" the conditional effects, which in the original version of unordered, while in our context they can be ordered as a sort of macro operators. For lack of space, in this paper we don't give a detailed description of these optimisations, which leads to a revised method similar to the technique described in \cite{ceriani2015planning} for always constraints, that we extended to deal with every class of {\pddlIII} qualitative preferences.


% SEZIONE COMMENTATA
\begin{comment}
\subsection{Compilation Example} \

[NOTA FP: ho usato $\psi$ e $\psi$ per le formule riferito alla preferenza always ed at-most-once dell'esempio
per evitare i doppi indici nei pedici delle clausole e renderle piu' facilmente distinguibili]

Consider the operator $o = \langle \textit{Pre}(o), \textit{Eff}(o) \rangle =\langle \top, \{a, \lnot c\} \rangle$
with cost $\kappa$, which affects two preferences ${\sf A}_{\phi}$ and ${\sf AO}_{\psi}$, where
% $P1 \in \mathcal{A}$ and $P2 \in \mathcal{AO}$, where
% $P1 = (\textit{always}\text{ }\phi^{P1})$ and $P2 = ({\textit{at-most-once}}\text{ }\phi^{P2})$
% $P1 \in \mathcal{A}$ and $P2 \in \mathcal{AO}$, where
% $P1 = (\textit{always}\text{ }\phi^{P1})$ and $P2 = ({\textit{at-most-once}}\text{ }\phi^{P2})$
% ${\sf A}_{\phi_1}$
$\phi = \phi_{1} \land \phi_{2} = (a \lor b) \land (c \lor d) $
and $\psi = \psi_{1} \land \psi_{2} =(\lnot c \lor e) \land (d \lor f)$.

\paragraph{Compilation of ${\sf A}_{\phi}$} 
Using the conditions specified in Definition \ref{threat_formula}, we can classify
$o$ as a \textbf{threat} of ${\sf A}_{\phi}$ because there exists a clause 
$\phi_{2} = (c \lor d$) of $\phi$ such that:

\begin{enumerate}

\item it contains at least a literal, i.e. $c$,
that is negated by the effects of $o$:
$|\overline{L}(\psi_{1}) \cap Z(o)| = 
|\overline{L}(c \lor d) \cap Z(o)| = 
|\{\lnot c, \lnot d\} \cap \{a, \lnot c\}| = |\{\lnot c\}| > 0$;

\item the other literal which does not satisfy the previous point, i.e. $d$, 
will not be certainly true in the resulting state from the application of $o$:
$|{L}(\psi_{1}) \cap Z(o)| = |L(c \lor d) \cap Z(o)| = |\{c, d\} \cap \{a, \lnot c\}| = 0$;

\item (the clause) is not false in the state where $o$ is applied because
the negation of its literals are not implied by the precondition of $o$:
$\overline{L}(c \lor d) = \{\lnot c, \lnot d\} \not \subseteq \textit{Pre}(o)$.

\end{enumerate}

Operator $o$ threatens only one clause of ${\sf A}_{\phi}$, 
i.e. $TC_{\mathcal{A}}(o, {\sf A}_{\phi}) = \{ \phi_2\} = \{ c \lor d\}$
(remember that $TC_{\mathcal{A}}(o, {\sf A}_{\phi})$ is the set of threatened clauses of ${\sf A}_{\phi}$ by $o$),
which could be falsified if $o$ is applied.

Using the preliminary definitions provided in Section \ref{compilation_always},
we define the set $\overline{AA}(o)_{\phi_{2}} = \{\lnot d\}$
which contains those literal of the threatened clause $\phi_2$
which are not falsified by the $o$ execution.
% that we have to check when $o$ is applied in order to capture 
% the possibile violation of ${\sf A}_{\phi}$ in the resulting state.

% In this case $o$, denying the literal $c$, threatens a single clause 
% of ${\sf A}_{\phi}$, i.e. $\phi_2 = c \lor d$, and
% therefore we have to check 
% the literal $d$ is true in the precondition
% to capture the possible violation.

Starting from these considerations and using Definition
\ref{when_always} we define the conditional effect $\mathcal{W}(o, P1)$
to add to $\textit{Eff}(o)$:
\begin{gather*}
\mathcal{W}(o, P1) = \\
\{\textit{when} \; ((\lnot d), \; \{\lnot d\}=\overline{AA}(o)_{\phi_2}) \; ({\sf A}_{\phi}\textit{-violated})\} = \\
\{\textit{when} \; (\lnot d) \; ({\sf A}_{\phi}\textit{-violated})\}.
\end{gather*}

% \begin{gather*}
% \mathcal{W}(o, P1) = \{\textit{when} \; (\lnot d) \; (P1\textit{-violated})\}.
% \end{gather*}

\paragraph{Compilation of ${\sf AO}_{\psi}$} 
According to Definition \ref{threat_amo}, we can classify $o$ as a \textit{threat} of ${\sf AO}_{\psi}$
because it could make true $\psi$ in the state resulting from the application of $o$;
indeed:

\begin{itemize}

    \item there exists a clause of $\psi$, i.e. $\psi_{1} = c \lor e$, which contains at least a 
        literal, i.e. $\lnot c$, that will be surely true in $s'$:
        $|L(\phi_{1}) \cap Z(o)| = |L(\lnot c \lor e) \cap Z(o)| = |\{\lnot c, e\} \cap \{a, \lnot c\}| = |\{\lnot c\}| > 0 $;\\

    \item while the other clause, i.e. $\psi_{2} = d \lor f$, will not be certainly false in the state resulting
    the application of $o$:
	$|\overline{L}(\psi_{2}) \cap Z(o)| = |\overline{L}(d \lor f) \cap Z(o)| = |\{\lnot d, \lnot f\} \cap \{a, \lnot c\}| = 0 $

\end{itemize}

We denote with $C_{\psi}(o) = \{\psi_{1}\} = \{\lnot c \lor e\}$
the set of clauses of $\psi$ that will be surely true in the resulting state $s'$. Starting 
from these considerations and using Definition
\ref{subsect:amo_comp} we define the conditional effect $\mathcal{W}(o, {\sf AO}_{\psi})$
to add to $\textit{Eff}(o)$:

% \begin{gather*}
% \mathcal{W}(o,&P_2) = \{  \\
% \textit{when } (\textit{cond}_{{N}}(o, P_2))\text{ }(\textit{seen-}\phi^{P2}))\\
% \textit{when } (\textit{cond}_{{T}}(o, P_2)))\text{ }(\pviol )
% \}
% \end{gather*}
\begin{gather*}
\mathcal{W}(o, {\sf AO}_{\psi}) = \{  \\
\textit{when } (\textit{cond}_{{N}}(o, {\sf AO}_{\psi}))\text{ }(\textit{seen-}\psi)),\\
\textit{when } (\textit{cond}_{{T}}(o, {\sf AO}_{\psi})))\text{ }({\sf AO}_{\psi}\textit{-violated} )
\}
\end{gather*}
\noindent where:

% \begin{itemize}
% \item 
\begin{gather*}
\textit{cond}_{{N}}(o, {\sf AO}_{\psi}) = \\\{\lnot \textit{seen-}\psi\} \cup \{ { \psi_i \mid \psi_i \not \in {C}_{\psi}(o)} \} = \\
= \{\lnot \textit{seen-}\psi\} \cup \{ \psi_2 \} = \{\lnot \textit{seen-}\psi, d \lor f\}
\end{gather*}

% \item 
\begin{gather*}
\textit{cond}_{T}(o, {\sf AO}_{\psi}) = \{\textit{seen-}\psi\} \cup \{ { \psi_i \mid \psi_i \not \in {C}_{\psi}(o)} \} \; \cup \\
\{ \bigvee_{ \psi_i \in {C}_{\psi}(o)} (\lnot l_1 \land ... \land \lnot l_q) \mid  \{l_1, ..., l_q\} = {L}(\psi_i)\} = \\
= \{ \textit{seen-}\psi \} \cup \{ \psi_2 \} \cup \{ \{ \lnot \lnot c \land \lnot e \} \mid \{\lnot c, e\} = L(\psi_1) \}  = \\
= \{\textit{seen-}\psi, d \lor f, c \land \lnot e\}
\end{gather*}

The condition $\textit{cond}_{{N}}(o, {\sf AO}_{\psi})$ to activate the predicate $\textit{seen-}\psi$
requires that the following condition hold in the state where $o$ is applied:

\begin{itemize}

\item the predicate $\textit{seen-}\psi$ has to be false which means that $\psi^{P2}$
never became true until then;

\item those clauses of ${\sf AO}_{\psi}$ which are not certainly true in the resulting state
from the application of $o$,
i.e. $\psi_2 = d \lor f$, have to be true
making sure that $\phi$ becomes true. 
\end{itemize}

The condition $\textit{cond}_{{T}}(o, {\sf AO}_{\psi}$))
to activate the violation of ${\sf AO}_{\psi}$ requires that:

\begin{itemize}

\item the fluent $\textit{seen-}\psi$ has to be true and so $\psi$
has already been made true;
\item those clauses of ${\sf AO}_{\psi}$ which are not affected by $o$,
i.e. $\psi_2 = d \lor f$, have to be true and those clauses of ${\sf AO}_{\psi}$
which are affected by $o$, i.e. $\psi_1 = \lnot c \lor e$, have to be false
ensuring that  $\psi$ passes from false to true.

\end{itemize}
\end{comment}







\begin{comment}
%
\begin{prop}[Correspondence between plans]
For an applicable action sequence $\pi$ for a $\Pi$ problem, let $\pi'$ a compiled plan such that
each action $o'$ in $\pi$ it is built by adding a set of
conditional effects to the original operator effects according to Definition \ref{compiledOcondeff},
% which definition depends by the operator itself and the class of affected preferences, 
then:
$$\pi \text{ is a plan for P } \Longleftrightarrow  \pi' \text{ is a plan for P'}.$$


\begin{proof} ($\Rightarrow$) \\
(a) The original initial state $I$ is extendend with some additional predicates
such that $I \subseteq I'$. 
Excluding from the demonstration
the $\textit{forgo}$ and $\textit{collect}$ operators for soft goals and sometime preferences,
whose correctness has already been demonstrated in  
we can observe that {(b)} the compiled goal $G'$
is equal to the original goal $G$. 

Using Definition \ref{compiledOcondeff}, 
if $o \in O_{\text{neutral}}$ then
no conditional effects are added by the compilation to the operator since it does not 
affect any preferences of $\Pi'$, while if $o \not \in O_{\text{neutral}}$ then
its effects are extended with a set of conditional effects which only affect the compiled 
additional fluents, e.g. $P\text{-violated}$ (see Definitions 
\ref{compilation_always}-\ref{when_sometimeafter}). So {(c)} 
the compiled operators have the property of not deleting any predicate belonging to $F$.

The first action of $\pi'$ is applicable in $I'$ cause (a), the following ones
are applicable cause (b) and so $\pi'$ is still executable and $\pi' \models G$ cause (b) and (c).
\end{proof}
\end{comment}


\begin{table}[]
\tiny
\setlength\tabcolsep{2pt}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
% \multicolumn{7}{|c|}{$\mathscr{P}_{\mathcal{ALL}}$}\\ \hline 
Planner & Rovers & TPP & Trucks & Openstacks & Storage &TOTAL \\ \hline 
\lamap & 16.98 & 8.34 & 15.43 & 19.28 & \bf 18.48 & \bf 78.51\\ \hline 
\fdremix & 17.89 & 7.1 & \bf 17.8 & 18.99 & 16.21 & 78.0\\ \hline 
\fdssshort & 17.6 & 7.03 & 17.21 & 18.7 & 17.12 & 77.66\\ \hline 
LAMA(2011) & 17.01 & 7.53 & 13.16 & 18.42 & 17.83 & 73.94\\ \hline 
IBaCoP2 & \bf 19.62 & 9.68 & 10.0 & 17.85 & 15.73 & 72.88\\ \hline 
LPRPG-P & 11.36 & \bf 18.74 & 7.1 & \bf 19.71 & 12.88 & 69.78\\ \hline 
MIPlan & 17.65 & 8.8 & 9.23 & 17.35 & 14.42 & 67.46\\ \hline 
Mercury & 16.07 & 6.57 & 7.84 & 18.06 & 14.51 & 63.04\\ \hline 
\end{tabular}

\caption{IPC comparison calculated using all kinds of preferences together. {\lprpgp} is the planning system which natively support preferences, while the others are all classical planners. The considered planning system are sorted by the total IPC score. The best performance are indicated in bold.}
\label{tab:ipc_score}
\end{table}
%\newpage


\begin{table}[t]
\tiny
\setlength\tabcolsep{2pt}
\centering

\begin{tabular}{|c|c|c|c|c|c|c|}

\hline 
\multicolumn{7}{|c|}{$\mathscr{P}_{\mathcal{A}}$}\\ \hline 
Planner & Rovers & TPP & Trucks & Openstacks & Storage &TOTAL \\ \hline 
\lamap & 14.91 & \bf 20.0 & 15.0 & \bf 20.0 & 19.0 & \bf 88.91\\ \hline 
\fdssshort & 14.75 & 17.0 & \bf 18.0 & 17.83 & \bf 20.0 & 87.59\\ \hline 
MIPlan & \bf 15.27 & \bf 20.0 & 12.0 & 19.0 & \bf 20.0 & 86.27\\ \hline 
LPRPG-P & 15.02 & 7.0 & 0.0 & 19.5 & 11.0 & 52.52\\ \hline \hline

\multicolumn{7}{|c|}{$\mathscr{P}_{\mathcal{G}}$}\\ \hline 
Planner & Rovers & TPP & Trucks & Openstacks & Storage &TOTAL \\ \hline 
LPRPG-P & --- & \bf 19.45 & 16.48 & \bf 19.57 & 14.96 & \bf 70.47\\ \hline 
\fdssshort & --- & 14.66 & \bf 16.49 & 18.55 & 18.46 & 68.16\\ \hline 
\lamap & --- & 14.82 & 14.36 & 18.85 & 18.92 & 66.94\\ \hline 
MIPlan & --- & 15.08 & 9.85 & 16.84 & \bf 19.32 & 61.09\\ \hline \hline

\multicolumn{7}{|c|}{$\mathscr{P}_{\mathcal{AO}}$}\\ \hline 
Planner & Rovers & TPP & Trucks & Openstacks & Storage &TOTAL \\ \hline 
\fdssshort & \bf 17.22 & 18.0 & \bf 20.0 & --- & 19.0 & \bf 74.22\\ \hline 
\lamap & 15.33 & \bf 19.0 & \bf 20.0 & --- & 19.0 & 73.33\\ \hline 
MIPlan & 14.76 & 17.0 & 15.0 & --- & \bf 20.0 & 66.76\\ \hline 
LPRPG-P & 14.11 & 2.0 & 19.0 & --- & 12.0 & 47.11\\ \hline \hline

\multicolumn{7}{|c|}{$\mathscr{P}_{\mathcal{SB}}$}\\ \hline 
Planner & Rovers & TPP & Trucks & Openstacks & Storage &TOTAL \\ \hline 
\lamap & 18.6 & \bf 20.0 & \bf 18.0 & --- & 19.0 & \bf 75.6\\ \hline 
MIPlan & \bf 18.66 & \bf 20.0 & 12.0 & --- & \bf 20.0 & 70.66\\ \hline 
\fdssshort & 17.92 & 17.0 & 16.5 & --- & 19.0 & 70.42\\ \hline 
LPRPG-P & 8.63 & 14.0 & 15.5 & --- & 7.0 & 45.13\\ \hline \hline

\multicolumn{7}{|c|}{$\mathscr{P}_{\mathcal{ST}}$}\\ \hline 
Planner & Rovers & TPP & Trucks & Openstacks & Storage &TOTAL \\ \hline 
\lamap & 15.3 & 10.0 & --- & --- & \bf 19.0 & \bf 44.3\\ \hline 
\fdssshort & \bf 17.2 & 8.0 & --- & --- & \bf 19.0 & 44.2\\ \hline 
LPRPG-P & 10.42 & \bf 17.0 & --- & --- & 14.0 & 41.42\\ \hline 
MIPlan & 15.86 & 9.0 & --- & --- & 14.0 & 38.86\\ \hline

\end{tabular}

\caption{
IPC comparison calculated considering all kinds of preferences separately. Each subtable
concerne a single class of preferences which is indicated in the first row.
{\lprpgp} is the planning system which natively support preferences,
while the others are all classical planners. The considered planning system are sorted in
each subtable by the total IPC score. The best performance are indicated in bold.
}
\label{tab:ipc_score-class}
\end{table}
%\newpage


% EXPERIMENTAL RESULTS
\section{Experimental Evaluation}

%\subsection{Experiment Settings}

We implemented the proposed compilation scheme, and we have compared the performance (plan quality) 
of propositional planners using it and of  {\lprpgp}, a the state-state-of-the art system for satisficing planning
with {\pddlIII} preferences \cite{coles2011lprpg}. We chose {\lprpgp} because we observed it has 
the best overall performance for the considered benchmarks, over all available systems supporting {\pddlthree} preferences.

Moreover, we have evaluated the effectiveness of using our compilation for optimal planning with preferences against
an alternative recent  compilation based on automata.

We used a selection of the best performing planners at IPC-8 and IPC-9 \cite{vallati20152014,ipc9website}:
{\lama} \cite{RicLAMA}, {\mercury} \cite{MercuryIPC8},
 {\miplan} \cite{nunez2014miplan}, {\ibacop} \cite{cenamor2014ibacop}, 
\fdss \cite{fdss2018} (abbreviated FDSS), \fdremixcomplete \cite{fdremix} (abbreviated \fdremix).
In addition, we used a recent version of {\lama}, called {\lamapruning}, exploiting admissibile
heuristic $h_\text{R}$ for testing reachability of soft goals during search
\cite{percassi2017improving}.

% [CHECK!!! SONO 100??] -> si
As benchmarks we used all (100) original problems
of the qualitative preference track of IPC5 \cite{GHLS+09}, which has five domains,
% which involve always, sometime, sometime-before, at-most-once and soft goal preferences,
 i.e.
Rovers, TPP, Trucks, Openstacks and Storage, of which the problems in Storage and TPP have no 
hard goal. In these benchmarks there is no preference of type sometime-after.

%For each original problem all preferences and each original utility were kept.
The propositional planners were run on the compiled problems, while {\lprpgp}
was run on the original problems. All the experiments
were conducted on a 2.00GHz Core Intel(R) Xeon(R) CPU E5-2620 machine with CPU-time
% (compilation time included [CHECK!!! VERO?]) # in questo round di esperimenti no
and memory limits of 30 minutes and 8GB, respectively. 

The compared planners are evaluated using the IPC quality score (introduced in IPC-6).
Given a planner $p$ and a planning instance $i$, if $p$ solves $i$, the
following score is assigned to $p$:
%$
$
\textit{score}(p, i) = \frac{\textit{cost}_{\textit{best}} (i)}{\textit{cost}(p, i)},
$
%$
where $\textit{cost}_{\textit{best}}(i)$ is the cost of the best known
solution for $i$ 
found by any planner, and $\textit{cost}(p, i)$
is the cost of the best solution found by $p$, using at most 30 CPU minutes.
% In our 
%case our reference for $\textit{cost}_{\textit{best}} (i)$ is equal to the
%cost of the best solution among the tested planners within 30 minutes. 
If $p$
does not find a solution, then $\textit{score}(p, i) = 0$.
%is equal to $0$ in order to reward both quality and coverage. 

We also consider another metric of plan quality evaluation, that we denote
$\alpha_{\text{cost}}$:
%which is useful for understanding what class of preferences have been achieved and
%how important they are to achieve a good quality plan.
%
% A description follows. 
if planner $p$ solves instance $i$, we assign the following score to $p$
$$
\alpha_{\textit{cost}}(p, i) = \frac{\textit{cost}(p, i)}{\textit{cost}_{\textit{total}}(i)} = \frac{
\sum_{P \in \mathscr{P}(i) \;:\; \pi \not \models P}{c(P)}}{\sum_{P \in \mathscr{P}(i)}{c(P)}}
$$
where 
%$\textit{cost}(p, i)$ is defined as the cost of the solution found by planner $p$
%$for the task $i$ within 30 minutes and $\textit{cost}_{\textit{total}}(i)$ is
%the sum of the costs of all the preferences involved in the task $i$ (note that 
$\mathscr{P}(i)$ is the set of the preferences in $i$.
Note that $\alpha_{\textit{cost}}(p, i)$ can vary between $0$ 
and $1$; if $\alpha_{\textit{cost}}(p, i) = 0$, then $\textit{cost}(p, i)= 0$ and
$p$ has found an optimal plan for $i$ preference-wise. On opposite side,
 if $\alpha_{\textit{cost}}(p, i) = 1$,  $p$ has found the worst plan 
for $i$  (all preferences are violated). If for two planners $p$ and $p'$ we have
$\alpha_{\textit{cost}}(p, i) < \alpha_{\textit{cost}}(p', i)$, then $p$ performs better than $p'$ for $i$, and
the difference between these metric values quantifies the performance discrepancy.

Concerning the optimal planning, similarly to what 
done in \cite{nebel2018}, we have tested our scheme
using LAMA with three admissible heuristics: $h^\text{blind}$, assigning zero to goal states and 1 to every other state,
 $h^\text{max}$ \cite{bonet2001planning}, $h^{\text{cpdb}}$ \cite{{haslum2007domain}}.
%These heuristics used with A* algorithm guarantee the optimality of the solution found. 

For this analysis we generated a set of benchmarks using the same methodology as in \cite{nebel2018}.
Starting from the IPC5 problems,  we created
simpler instances by randomly sampling subsets of the soft trajectory constraints in original instances:
from each original instance, five new instances are generate, each of which has with 1\%, 5\%, 10\%, 20\% and 40\% of the original
(grounded) soft trajectory constraints, while the hard goals are all unchanged, if they exist.
Since the authors of the aforementioned paper did not make their sampled instances available,
we generated, for each sampling percentage (except for 100 \%), 3 sampled
instances and considered the average perforce on them.
\begin{figure*}[t]
\centering
\includegraphics[width=7cm]{histogrammi/rovers.png}
%\label{hrovers}
\qquad
\includegraphics[width=7cm]{histogrammi/tpp.png}
%\label{htpp}

\caption{$\alpha_{\text{cost}}$ analysis for Rovers and TPP domain.\label{hrovers-htpp}
}

\end{figure*}

\subsection{Experimental Results}

Tables \ref{tab:ipc_score} and \ref{tab:ipc_score-class} give the performances of the compared planners
in term of IPC quality score aggregated by domain. The results in Table \ref{tab:ipc_score} concern plan quality
considering all preferences, while those in Table \ref{tab:ipc_score-class} concern plan quality when only the specified preferences of a particular class are used for the plan evaluation.

% [CHECK!! METTIAMO TUTTI I PLANNER NELLA TABELLA 2?][FP: Se avanza spazio si]
The analysis in Table \ref{tab:ipc_score-class} is restricted to a subset of those considered in Table \ref{tab:ipc_score}.
Overall the compilation approach performs better than {\lprpgp}, with six planners obtaining better total IPC scores.  The comparison considering each preference class separately shows good performance for every preference class except for soft goals.
 
In Rovers, Trucks and Storage each considered planner performs better than, or at least similarly to, {\lprpgp}
(except for {\mercury} in Trucks); {\ibacop} performs particularly well in Rovers, {\fdremix} in Trucks
and {\lamapruning} in Storage. Also {\miplan} works well in Trucks but it
is penalized due to coverage (it solves only 15 instances out of 20).
The tested planners from IPC9, {\fdremix} and {\fdss}, perform overall better than those from IPC8, but {\lamapruning} is better than everyone else (it improves the performance of LAMA in all the considered domains except
Rovers, where there is no soft goal and $h_r$ is not exploited for pruning).


On the other hand, {\lprpgp} performs comparably with {\lamapruning} in Openstacks and much better than the other planners in TPP. 
The bad performances of the compilation approach in TPP is mainly due to presence of many soft goals and, as shown in \cite{percassi2017improving}, compiling soft goals through Keyder and Geffner's method
can sometimes be problematic.
%
Indeed Table \ref{tab:ipc_score-class} shows that for soft goals {\lprpgp} has higher IPC score
 than all others planners. We can also observe that, compared to {\lprpgp}, the classical planners 
achieve better results for preferences of classes always, sometime-before and at-most-once, 
but this has not a crucial impact of the overall plan quality,  because in these problems violating the
 soft-goals is more expensive than the other preferences (or equivalently they are more useful to satisfy than the other preferences).


%COMMENTO ROVERS
The comparison of the planners' performance using the $\alpha_{\text{cost}}$ helps to further understand the behaviour of the planners.
For lack of space, we will focus this analysis on two selected domains: Rovers, one of the domains where the compilation approach works better, and TPP, the only domain where we observed poor performance compare to {\lprpgp}.
Figure \ref{hrovers-htpp} shows, for each preferences class, the planners' $\alpha_{\text{cost}}$ values obtained by adding the relative $\alpha_{\text{cost}}$ for every instance in the of these two benchmark domains. Each level of the stacked histogram
represents the aggregated $\alpha_{\text{cost}}$ restricted to a specific class of preferences, which indicates how much each class of the violated preferences contributes to the total cost of the plans.

For Rovers, the IPC-score gap between the classical planners and {\lprpgp} 
is due to {\lprpgp}'s violation of the sometime-before preferences. Regarding the other
classes preferences, the violation costs in the generated plans are similar
except for {\ibacop}, that satisfies more sometime-before and sometime
preferences than the others planning, and violates more at-most-once preferences,
generally obtaining better quality plans.

For TPP,  indeed looking at Figure \ref{hrovers-htpp} we note that the most important preferences in this domain are 
the soft goals, which are better satisfied by {\lprpgp}. The search pruning technique in  {\lamapruning} slightly helps
{\lama} to achieve more soft goals, but not enough to reach the performance of {\lprpgp}.



%AG: NB, IL RESTO E' SOLO PARTE DEL TESTO ORIGINALE DI FRANCESCO
\begin{comment}
%%%%%%%%%%%%%%%%%%%
% COMMENTO ROVERS %
%%%%%%%%%%%%%%%%%%%
Looking at Figure \ref{hrovers} we note that
the IPC gap between the classical planners and {\lprpgp} in Rovers
is due to the {\lprpgp} violation of sometime-before preferences. As regards the remaining
classes preferences, the violation cost settles down on similar level
except for {\ibacop}, which achieves more sometime-before and sometime
preferences than the others planning violating more at-most-once preferences
but succeeding to obtaining better quality plans.
% Figures \ref{lst:file1}--\ref{lst:file5} show the $\alpha_{\text{cost}}$ comparison
% for each domain comparing the best perfoming classical planner 
% in term of IPC score in such domain with {\lprpgp}.

% Looking at Figure \ref{lst:file1},
% where we have reported the $\alpha_{\text{cost}}$ comparison for 
% the best performing planners in Rovers in term of IPC score,
% % {\ibacop} and {\lprpgp},
% % the best performing competitor planning system in Rovers according to Table \ref{tab:ipc_score},
% we can say that the compilative approach, combined with the use of {\ibacop},
% achieves to satisfy a better subset of preferences in all the compared instances
% compared to {\lprpgp}.



%%%%%%%%%%%%%%%%
% COMMENTO TPP %
%%%%%%%%%%%%%%%%
In TPP the compilative 
approach seems to be very ineffective, indeed each classical planner
achieves an extremely lower quality performance compared to {\lprpgp}. The bad
performances in this domain are due to the many soft goals and sometime
preferences because, as shown in \cite{percassi2017improving}, the compilation
of soft goals can be sometime problematic and neither the use of the reachability
heuristic $h_{\text{R}}$ in {\lamapruning} can compensate this weakness. Indeed looking
at Table \ref{tab:ipc_score-class} we note that {\lprpgp} gets an high IPC score
for these two preference classes than all the classical planners. Looking 
at Table \ref{tab:ipc_score} we can also observe that the classical planners 
achieves a better result
of always, sometime-before and at-most-once preferences compared to {\lprpgp} in
term of IPC score, but this is not very relevant for the plan quality because
apparently it happens at the expense of soft-goal and sometime preferences which
are clearly more expensive to violate (or equivalently more useful to satisfy), indeed
looking to Figure \ref{htpp} we note that the decisive preferences in this domain are 
mainly soft goals which are more achieved by {\lprpgp}. Note that {\lamap} helps
a bit {\lama} to achieves more soft goals.


% %%%%%%%%%%%%%%%%%
% COMMENTO TRUCKS %
% %%%%%%%%%%%%%%%%%
Looking at Figure {\ref{htrucks}} we observe that the classical planners,
except {\miplan}, achieves a comparable performance with {\lprpgp} and
in particular {\lamap} and {\fdss} get higher quality plan because
they manage to achieve almost all the always preferences
and more sometime-before preferences than their competitor.

% In Trucks {\ibacop} gets a poorer performance while {\fdremix} gets the best result
% in term of IPC score 

% Looking at Figure \ref{lst:file3},
% where we have reported the $\alpha_{\text{cost}}$ comparison for the
% % best performing competitor planning system in Trucks according to Table \ref{tab:ipc_score},
% best performing planners in Trucks in term of IPC score,
% we can say that the compilative approach, combined with the use of {\fdremix}, 
% achieves to satisfy a better, worst and equal subset of preferences in all instances compared to
% {\lprpgp} in 15, 4 and 1 respectively.

% Looking at Figure \ref{lst:file2},
% where we have reported the $\alpha_{\text{cost}}$ for the
% best performing competitor planning system, according to Table \ref{tab:ipc_score} in TPP,
% we can say that the compilative approach combined with the use of {\ibacop} 
% achieves to satisfy a better, worst and equal subset of preferenes in 12, 7 and 1 instances
% respectively compared to {\lprpgp}.

% Looking at Figure \ref{lst:file2},
% where we have reported the $\alpha_{\text{cost}}$ comparison between {\ibacop}
% and {\lprpgp} calculated considering all kind of preferences, we can say that our classical planner

% Looking at Figure \ref{lst:file2},
% where we have reported the $\alpha_{\text{cost}}$ for the
% best performing competitor planning system, according to Table \ref{tab:ipc_score} in Openstacks,
% we can say that our approach combined with the use of {\ibacop} 
% achieves to satisfy a better subset of preferences in all instances.


% Looking at Figure \ref{lst:file3},
% where we have reported the $\alpha_{\text{cost}}$ comparison between {\lamapruning}
% and {\lprpgp} calculated considering all kind of preferences, we can say that our classical planner
% achieves to satisfy a better, worst and equal subset of preferenes in 12, 7 and 1 instances respectively.

%%%%%%%%%%%%%%%%%%%%%%%
% COMMENTO OPENSTACKS %
%%%%%%%%%%%%%%%%%%%%%%%
Regarding Opentacks and looking at Table \ref{tab:ipc_score} all the tested planners 
achieve a comparable performance in term of IPC score even if the classical planners
are slightly penalized compared to \lprpgp. Looking at Figure \ref{hopenstacks} we can assert
that the only relevant classes of preferences in this domain are soft goals and always preferences 
and every planner performs a similar performance (except for {\ibacop} amd {\miplan}
that do worse).

Looking at Figure {\ref{hstorage}} we observe that all the classical planners,
achieves a better performance than {\lprpgp} and
in particular {\lamap} and {\miplan} get higher quality plan because
they manage to achieve almost the sometime-before preferences
than their competitor.

% {\lama} and {\\fdremix} compute lower quality plans that {\lprpgp}
% for more than half of the instances. Both classical system work better than
% LPRPGP in smaller instances but they get worse as the size increases. Looking
% at Figure \ref{lst:file3} we can say that {\lama} and {\\fdremix} performs better
% for more than half of the instances, in particular they find better plan in 13 and 16
% instances out of 20. Note that the classical planners get the optimal solution for some
% of the first seven instances. Looking at Figure \ref{lst:file4} we can say 
% that both approaches achieve a comparable performance even if {\lprpgp} generally finds 
% slightly better solutions than both classical competitors in 13 instances out of 20.
\end{comment}



\begin{table}[t]
\tiny
\centering
\begin{tabular}{|c||c|c||c|c||c|c||}
\hline 
\multirow{2}{*}{Domain} & \multicolumn{2}{c||}{$h^{\text{blind}}$} & \multicolumn{2}{c||}{$h^{\text{max}}$} & \multicolumn{2}{c||}{$h^{\text{cpdb}}$} \\ \cline{2-7}
& WMB & Our & WMB & Our & WMB & Our \\ \hline
Storage & 24.78 & {\bf 57.0} & 29.2 & {\bf45.0} & 23.10 & \bf{57.0} \\ \hline
Rovers & 17.4 & {\bf24.0} & 21.43 & {\bf 25.0} & 15.17 & \bf{23.0} \\ \hline
Trucks & 18.84 & {\bf24.0} & 23.19 & \bf{25.0} & n/a & {25} \\ \hline
TPP & --- & 47.0 & --- & {45.0} & --- & 40.0 \\ \hline
\end{tabular}
\caption{Coverage of our and Wright, Mattmueller and Nebel (WMB)'s compilation schemes 
on the IPC5 benchmarks set with
additional sampled soft-trajectory constraints. ``n/a''  means that the considered heuristic was not applicable; ``---'' means that no data
are reported in \cite{nebel2018}.}
\label{coverage_admissible_heuristics}
\end{table}


Table \ref{coverage_admissible_heuristics} gives results about using our compilation scheme for optimal planning.
For each of the three considered versions of LAMA using admissible heuristics, the table indicates the percentage of 
solved problems. The results are compared with those of reported  in \cite{nebel2018}  for WMN's compilation.\footnote{
A more detailed comparison with WMN's approach is very difficult because, at the time of writing, WMN's compiler, compiled files and solution plans are unavailable.}
%
Domain Openstacks here is not considered because no one of the considered planner solved any instance. 
Also note that results for TPP are missing because they were not reported in \cite{nebel2018}
they were not reported in their work.

According to these results,  the compilation approach without automata seems quite
preferable, because  a higher coverage is obtained in all three considered domains. This is the case even if the machine that we used for our experiments if less powerful (CPU and memory wise) than the one used in \cite{nebel2018}, and moreover our CPU-time limit was half of that used in \cite{nebel2018}. 
 

%AG SPOSTATO QUI...
%We observe that all the experiments about the optimality
%were conducted on the same setting used for the satisficing evaluation, while the experiments 
%reported in \cite{nebel2018} are conducted on a
%Intel(R) Xeon(R) E5-2650v2 2.60GHz processors with 64GiB with one hour of CPU-time
%for the search and therefore our approach is penalized.

%Despite the unfavorable experimental setting, the compilation approach without automata seems 
%preferable because we get an higher coverage in all three considered domains.

% \begin{figure}[]
% \centering
% % \includegraphics[width=8.5cm]{histogrammi/rovers.png}
% \includegraphics[width=8.5cm]{histogrammi/trucks.png}
% \caption{$\alpha_{\text{cost}}$ comparison for Trucks domain.}
% \label{htrucks}
% \end{figure}

% \begin{figure}[]
% \centering
% % \includegraphics[width=8.5cm]{histogrammi/rovers.png}
% \includegraphics[width=8.5cm]{histogrammi/openstacks.png}
% \caption{$\alpha_{\text{cost}}$ comparison for Openstacks domain.}
% \label{hopenstacks}
% \end{figure}

% \begin{figure}[]
% \centering
% % \includegraphics[width=8.5cm]{histogrammi/rovers.png}
% \includegraphics[width=8.5cm]{histogrammi/preprocessed-storage.png}
% \caption{$\alpha_{\text{cost}}$ comparison for Storage domain.}
% \label{hstorage}
% \end{figure}
% STESSA TABELLA MA CON HEURISTICA M&S
% \begin{table}[]
% \scriptsize
% \centering
% \begin{tabular}{|c||c|c||c|c||c|c||c|c||}
% \hline 
% \multirow{2}{*}{DOMINO} & \multicolumn{2}{c||}{$h^{\text{blind}}$} & \multicolumn{2}{c||}{$h^{\text{max}}$} & \multicolumn{2}{c||}{$h^{\text{m\&s}}$} & \multicolumn{2}{c||}{$h^{\text{cpdb}}$} \\ \cline{2-9}
% & WRB & Our & WRB & Our& WRB & Our& WRB & Our \\ \hline
% Storage & 24.78 & 57.0 & 29.2 & 45.0 & 32.50 & 24.0 & 23.10 & 57.0 \\ \hline
% Rovers & 17.4 & 24.0 & 21.43 & 25.0 & 16.67 & 26.0 & 15.17 & 23.0 \\ \hline
% Trucks & 18.84 & 24.0 & 23.19 & 25.0 & n/a & 25.0 & n/a & 25 \\ \hline
% TPP & --- & 47.0 & --- & 45.0 & --- & 47.0 & -- & 40.0 \\ \hline
% \end{tabular}
% \caption{Coverage of our and Nebel compilation scheme on the IPC5 benchmarks set with
% additional instances with random sampled soft-trajectory constraints, A*
% search for optimal solution. Our results concerning the sampled instances are averaged. The best performance are indicated in bold.}
% \label{coverage_admissible_heuristics}
% \end{table}


\section{Conclusions}

We have proposed a new compilation schema for solving propositional planning with the full class of {\pddlIII} soft state-trajectory constraints called qualitative preferences.
Our work significantly extends the approach of Keyder and Geffner, that is restricted to only softgoals. Experimental results show that, despite the compilation of only soft goals may be less effective than other approaches, such LPRPG-P, for soft state-trajectory constraints our compilation is quite competitive with state-of-the-art systems supporting them. Since the planning language of the compiled problem is very simple, many available planners can use it.

 
\newpage
\bibliographystyle{plain}
\bibliography{biblio.bib}
\end{document}
